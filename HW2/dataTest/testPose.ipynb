{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization utilies.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def show_points(points, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim3d([-2, 2])\n",
    "    ax.set_ylim3d([-2, 2])\n",
    "    ax.set_zlim3d([0, 4])\n",
    "    ax.scatter(points[:, 0], points[:, 2], points[:, 1])\n",
    "\n",
    "def compare_points(points1, points2):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_xlim3d([-2, 2])\n",
    "    ax.set_ylim3d([-2, 2])\n",
    "    ax.set_zlim3d([0, 4])\n",
    "    ax.scatter(points1[:, 0], points1[:, 2], points1[:, 1])\n",
    "    ax.scatter(points2[:, 0], points2[:, 2], points2[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# Load the canonical-space object point clouds\n",
    "import trimesh\n",
    "import collada\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fps_downsample(points, number_of_points_to_sample):\n",
    "  selected_points = np.zeros((number_of_points_to_sample, 3))\n",
    "  dist = np.ones(points.shape[0]) * np.inf # distance to the selected set\n",
    "  for i in tqdm(range(number_of_points_to_sample)):\n",
    "      # pick the point with max dist\n",
    "      idx = np.argmax(dist)\n",
    "      selected_points[i] = points[idx]\n",
    "      dist_ = ((points - selected_points[i]) ** 2).sum(-1)\n",
    "      dist = np.minimum(dist, dist_)\n",
    "  return selected_points\n",
    "\n",
    "object_models_file = \"models/objects_v1.csv\"\n",
    "object_models_info = pd.read_csv(object_models_file)\n",
    "object_models_name = object_models_info[\"object\"].to_list()\n",
    "object_models_location = object_models_info[\"location\"].to_list()\n",
    "object_models_num = len(object_models_name)\n",
    "\n",
    "\n",
    "object_models = {}\n",
    "for object_name, object_mesh_dir in zip(object_models_name, object_models_location):\n",
    "  object_mesh_path = os.path.join(object_mesh_dir, \"visual_meshes\", \"visual.dae\")\n",
    "  # Collada-based loading\n",
    "  object_mesh = trimesh.exchange.dae.load_collada(object_mesh_path)\n",
    "  mesh_name = object_mesh['graph'][0]['geometry']\n",
    "  object_models[object_name] = object_mesh['geometry'][mesh_name]['vertices']\n",
    "  # Trimesh-based loading\n",
    "  # object_mesh = trimesh.load(object_mesh_path, force='mesh')\n",
    "  # object_models[object_name] = object_mesh.vertices\n",
    "  # print(object_mesh)\n",
    "\n",
    "# object_models with key=objectName, value=objectPointCloud\n",
    "# print(object_models_info)\n",
    "print(object_models[\"jenga\"])\n",
    "print(len(object_models[\"jenga\"]))\n",
    "# print(np.unique(object_models[\"jenga\"]))\n",
    "# print(len(object_models))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data loading helpers\"\"\"\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import utils\n",
    "import open3d as o3d\n",
    "# # Helper function for the reconstruction of the target point cloud\n",
    "# rgb = np.array(Image.open(rgb_files[0])) / 255   # convert 0-255 to 0-1\n",
    "# depth = np.array(Image.open(depth_files[0])) / 1000   # convert from mm to m\n",
    "# label = np.array(Image.open(label_files[0]))\n",
    "# meta = load_pickle(meta_files[0])\n",
    "# intrinsic = meta['intrinsic']\n",
    "# z = depth\n",
    "# v, u = np.indices(z.shape)\n",
    "# uv1 = np.stack([u + 0.5, v + 0.5, np.ones_like(z)], axis=-1)\n",
    "# points_viewer = uv1 @ np.linalg.inv(intrinsic).T * z[..., None]  # [H, W, 3]\n",
    "# # print(points_viewer.shape)\n",
    "# # print(points_viewer.shape[0]*points_viewer.shape[1])\n",
    "# # print(points_viewer)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def dump_json(sample, path):\n",
    "  with open(path, 'w') as fp:\n",
    "    json.dump(sample, fp)\n",
    "  return 0\n",
    "\n",
    "def load_json(path):\n",
    "  f = open(path)\n",
    "  data = json.load(f)\n",
    "  return data\n",
    "\n",
    "def get_point_cloud(depth, intrinsic):\n",
    "  z = depth\n",
    "  v, u = np.indices(z.shape)\n",
    "  uv1 = np.stack([u + 0.5, v + 0.5, np.ones_like(z)], axis=-1)\n",
    "  points_viewer = uv1 @ np.linalg.inv(intrinsic).T * z[..., None]  # [H, W, 3]\n",
    "  return points_viewer\n",
    "\n",
    "def pts_to_o3d_pcd(pts):\n",
    "  \"\"\"Transform to o3d pcd\"\"\"\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "  return pcd\n",
    "\n",
    "def o3dvis(pts):\n",
    "    o3d.visualization.draw_geometries([pts_to_o3d_pcd(pt) for pt in pts])\n",
    "    return 0\n",
    "\n",
    "def get_object_point_cloud(test_image_label, object_id, test_depth, intrinsic):\n",
    "  # print(np.where(test_image_label==object_id))\n",
    "  # print(test_image_label[327][654])\n",
    "  test_image_label[np.where(test_image_label==object_id)] = 255\n",
    "  # print(test_image_label[327][654])\n",
    "  # print(np.where(test_image_label==255))\n",
    "  test_image_label[np.where(test_image_label!=255)] = 0\n",
    "  test_image_label[np.where(test_image_label==255)] = 1\n",
    "  test_object_depth = test_depth * test_image_label\n",
    "  test_pcd_target = get_point_cloud(test_object_depth, intrinsic)\n",
    "  # (H, W, dim) = test_pcd_target.shape\n",
    "  # filter out target object point cloud\n",
    "  # print(test_pcd_target.shape)\n",
    "  # print(test_pcd_target)\n",
    "  # print((test_pcd_target[:,0]!=0)|(test_pcd_target[:,1]!=0)|(test_pcd_target[:,2]!=0))\n",
    "  test_pcd_target = test_pcd_target.reshape(-1, test_pcd_target.shape[-1]) # reshape to (H*W, 3)\n",
    "  test_pcd_target = test_pcd_target[(test_pcd_target[:,0]!=0)|(test_pcd_target[:,1]!=0)|(test_pcd_target[:,2]!=0)]\n",
    "  return test_pcd_target\n",
    "\n",
    "\n",
    "def get_meta(meta_path):\n",
    "  return load_pickle(meta_path)\n",
    "\n",
    "def get_depth(depth_path):\n",
    "  return (np.array(Image.open(depth_path))/1000)\n",
    "\n",
    "def get_label(label_path):\n",
    "  return np.array(Image.open(label_path))\n",
    "\n",
    "\n",
    "def dump_json(sample, path):\n",
    "  with open(path, 'w') as fp:\n",
    "    json.dump(sample, fp)\n",
    "  return 0\n",
    "\n",
    "def load_json(path):\n",
    "  f = open(path)\n",
    "  data = json.load(f)\n",
    "  return data\n",
    "\n",
    "def get_point_cloud(depth, intrinsic):\n",
    "  z = depth\n",
    "  v, u = np.indices(z.shape)\n",
    "  uv1 = np.stack([u + 0.5, v + 0.5, np.ones_like(z)], axis=-1)\n",
    "  points_viewer = uv1 @ np.linalg.inv(intrinsic).T * z[..., None]  # [H, W, 3]\n",
    "  return points_viewer\n",
    "\n",
    "\n",
    "def get_meta(meta_path):\n",
    "  return load_pickle(meta_path)\n",
    "\n",
    "def get_depth(depth_path):\n",
    "  return (np.array(Image.open(depth_path))/1000)\n",
    "\n",
    "def get_label(label_path):\n",
    "  return np.array(Image.open(label_path))\n",
    "\n",
    "\n",
    "\"\"\"Metric and visualization.\"\"\"\n",
    "\n",
    "def compute_rre(R_est: np.ndarray, R_gt: np.ndarray):\n",
    "    \"\"\"Compute the relative rotation error (geodesic distance of rotation).\"\"\"\n",
    "    assert R_est.shape == (3, 3), 'R_est: expected shape (3, 3), received shape {}.'.format(R_est.shape)\n",
    "    assert R_gt.shape == (3, 3), 'R_gt: expected shape (3, 3), received shape {}.'.format(R_gt.shape)\n",
    "    # relative rotation error (RRE)\n",
    "    # Rotational degree loss (not objective of optimization)\n",
    "    rre = np.arccos(np.clip(0.5 * (np.trace(R_est.T @ R_gt) - 1), -1.0, 1.0))\n",
    "    return rre\n",
    "\n",
    "\n",
    "def compute_rte(t_est: np.ndarray, t_gt: np.ndarray):\n",
    "    assert t_est.shape == (3,), 't_est: expected shape (3,), received shape {}.'.format(t_est.shape)\n",
    "    assert t_gt.shape == (3,), 't_gt: expected shape (3,), received shape {}.'.format(t_gt.shape)\n",
    "    # relative translation error (RTE)\n",
    "    rte = np.linalg.norm(t_est - t_gt) # Resembling MSE loss\n",
    "    return rte\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['poses_world', 'extents', 'scales', 'object_ids', 'object_names', 'extrinsic', 'intrinsic'])\n",
      "pudding_box\n",
      "src and tg len:\n",
      "4135 49152\n",
      ":: Load two point clouds and disturb initial pose.\n",
      ":: Downsample with a voxel size 5.000.\n",
      ":: Estimate normal with search radius 10.000.\n",
      ":: Compute FPFH feature with search radius 25.000.\n",
      ":: Downsample with a voxel size 5.000.\n",
      ":: Estimate normal with search radius 10.000.\n",
      ":: Compute FPFH feature with search radius 25.000.\n",
      "PointCloud with 49152 points. PointCloud with 4135 points.\n",
      "PointCloud with 49152 points. PointCloud with 4135 points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:28<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse list:\n",
      "[0.01176805705359216, 0.010612464069852225, 0.010887726200242871, 0.010612464667375062, 0.011477113912545744, 0.011768057053592108, 0.01144558727610238, 0.0105925372291997, 0.010735323891631022, 0.011434162143288778]\n",
      "49152 49152\n",
      "(49152, 3)\n",
      "(4135, 3)\n",
      "GT T:\n",
      "[[ 0.00145066  0.99998546  0.00521667  0.27790332]\n",
      " [-0.9999827   0.00148046 -0.00571105 -0.06234347]\n",
      " [-0.00571869 -0.00520829  0.9999701   0.02262924]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "GT T world:\n",
      "[[ 0.00145066  0.99998546  0.00521667  0.27790332]\n",
      " [-0.9999827   0.00148046 -0.00571105 -0.06234347]\n",
      " [-0.00571869 -0.00520829  0.9999701   0.02262924]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "pred T:\n",
      "[[ 0.99574795 -0.08321756  0.03950768 -0.10587012]\n",
      " [ 0.07676105  0.98665715  0.14358069  0.12900343]\n",
      " [-0.05092897 -0.13993753  0.9888497   0.62512751]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "pred T world:\n",
      "[[-0.03562217  0.86214034 -0.5054157   0.28400288]\n",
      " [ 0.99530219 -0.01495223 -0.0956553  -0.06012973]\n",
      " [-0.09002532 -0.50644884 -0.85755762  0.03213286]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "rre=174.18280923908495, rte=0.011507561387264649\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Visualization on comparing between train pose-transformed point clouds.\"\"\"\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp],\n",
    "                                      zoom=0.4559,\n",
    "                                      front=[0.6452, -0.3036, -0.7011],\n",
    "                                      lookat=[1.9892, 2.0208, 1.8945],\n",
    "                                      up=[-0.2779, -0.9482, 0.1556])\n",
    "\n",
    "def get_o3d_icp_tensor(source_pcd, target_pcd):\n",
    "  # T = icp(source_pcd, target_pcd, 150)\n",
    "  source_pcd_o3d, target_pcd_o3d = o3d.t.geometry.PointCloud(), o3d.t.geometry.PointCloud()\n",
    "  # print(source_pcd)\n",
    "  source_pcd_o3d.point.positions = o3d.core.Tensor(source_pcd, dtype=o3d.core.Dtype.Float64)\n",
    "  # source_pcd_o3d.estimate_normals(o3d.geometry.KDTreeSearchParamKNN(knn=50))\n",
    "  target_pcd_o3d.point.positions = o3d.core.Tensor(target_pcd, dtype=o3d.core.Dtype.Float64)\n",
    "  # Search distance for Nearest Neighbour Search [Hybrid-Search is used].\n",
    "  max_correspondence_distance = 1\n",
    "  init = o3d.core.Tensor.eye(4, o3d.core.Dtype.Float64)\n",
    "  # Select the `Estimation Method`, and `Robust Kernel` (for outlier-rejection).\n",
    "  treg = o3d.cpu.pybind.t.pipelines.registration\n",
    "  estimation = treg.TransformationEstimationPointToPoint()\n",
    "  callback_after_iteration = lambda updated_result_dict : print(\"Iteration Index: {}, Fitness: {}, Inlier RMSE: {},\".format(\n",
    "    updated_result_dict[\"iteration_index\"].item(), \\\n",
    "    updated_result_dict[\"fitness\"].item(), \\\n",
    "    updated_result_dict[\"inlier_rmse\"].item())) \\\n",
    "\n",
    "  # Convergence-Criteria for Vanilla ICP\n",
    "  criteria = treg.ICPConvergenceCriteria(relative_fitness=0.0000000000001,\n",
    "                      relative_rmse=0.0000000000001,\n",
    "                      max_iteration=100)\n",
    "\n",
    "  # Down-sampling voxel-size.\n",
    "  # voxel_size = 0.0016625\n",
    "  # voxel_size = 0.003125\n",
    "  voxel_size = 0.00625\n",
    "  # voxel_size = 0.0125\n",
    "\n",
    "  # Save iteration wise `fitness`, `inlier_rmse`, etc. to analyse and tune result.\n",
    "  save_loss_log = True\n",
    "  s = time.time()\n",
    "\n",
    "  # voxel_size = 0.05  # means 5cm for this dataset\n",
    "  # source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(voxel_size, source_pcd_o3d, target_pcd_o3d)\n",
    "  # result_ransac = execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size)\n",
    "  # init = o3d.core.Tensor(result_ransac.transformation, dtype=o3d.core.Dtype.Float64)\n",
    "\n",
    "  registration_icp = treg.icp(source_pcd_o3d, target_pcd_o3d, max_correspondence_distance, init, estimation, criteria, voxel_size, callback_after_iteration)\n",
    "  icp_time = time.time() - s\n",
    "  T = registration_icp.transformation\n",
    "  T = T.numpy()\n",
    "  return T\n",
    "\n",
    "\n",
    "def get_o3d_icp(source_pcd, target_pcd, init=np.eye(4)):\n",
    "  # T = icp(source_pcd, target_pcd, 150)\n",
    "  source_pcd_o3d, target_pcd_o3d = o3d.geometry.PointCloud(), o3d.geometry.PointCloud()\n",
    "  # print(source_pcd)\n",
    "  source_pcd_o3d.points = o3d.utility.Vector3dVector(source_pcd)\n",
    "  # source_pcd_o3d.estimate_normals(o3d.geometry.KDTreeSearchParamKNN(knn=50))\n",
    "  target_pcd_o3d.points = o3d.utility.Vector3dVector(target_pcd)\n",
    "  # print(source_pcd_o3d)\n",
    "  # print(target_pcd_o3d)\n",
    "  # target_pcd_o3d.estimate_normals(o3d.geometry.KDTreeSearchParamKNN(knn=50))\n",
    "  # init = np.random.rand(4,4)\n",
    "  # init = -1 + init * 2\n",
    "  # init[-1] = np.zeros(4)\n",
    "  # print(init)\n",
    "  # init[0,0] = 2\n",
    "  T = o3d.pipelines.registration.registration_icp( \\\n",
    "        source_pcd_o3d, target_pcd_o3d, 10, init, \\\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(), \\\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=0.00000001, \\\n",
    "                                 relative_rmse=0.00000001, \\\n",
    "                                 max_iteration=5000))\n",
    "  # T = icp(source_pcd, target_pcd)\n",
    "  # print(T.inlier_rmse)\n",
    "  # print(T.fitness)\n",
    "  # print(T.correspondence_set)\n",
    "  return T\n",
    "\n",
    "def preprocess_point_cloud(pcd, voxel_size):\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    # pts = pcd.points\n",
    "    # size = len(pts)\n",
    "    # if (size >= 1000):\n",
    "    #   pcd_down = pcd.farthest_point_down_sample(size//2)\n",
    "    # else: \n",
    "    #   pcd_down = pcd\n",
    "    pcd_down = pcd\n",
    "\n",
    "    radius_normal = voxel_size * 2\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return pcd_down, pcd_fpfh\n",
    "\n",
    "def prepare_dataset(voxel_size, source, target):\n",
    "    print(\":: Load two point clouds and disturb initial pose.\")\n",
    "\n",
    "    # trans_init = np.asarray([[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n",
    "    #                          [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n",
    "    # source.transform(trans_init)\n",
    "\n",
    "    source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
    "    target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
    "    return source, target, source_down, target_down, source_fpfh, target_fpfh\n",
    "\n",
    "def execute_global_registration(source_down, target_down, source_fpfh,\n",
    "                                target_fpfh, voxel_size):\n",
    "  distance_threshold = voxel_size * 1.5\n",
    "  # print(\":: RANSAC registration on downsampled point clouds.\")\n",
    "  # print(\"   Since the downsampling voxel size is %.3f,\" % voxel_size)\n",
    "  # print(\"   we use a liberal distance threshold %.3f.\" % distance_threshold)\n",
    "  result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "      source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "      distance_threshold,\n",
    "      o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "      3, [\n",
    "          o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "              0.9),\n",
    "          o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "              distance_threshold)\n",
    "      ], o3d.pipelines.registration.RANSACConvergenceCriteria(100000, 0.999))\n",
    "  return result\n",
    "\n",
    "def execute_fast_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size):\n",
    "  distance_threshold = voxel_size * 0.5\n",
    "  print(\":: Apply fast global registration with distance threshold %.3f\" \\\n",
    "          % distance_threshold)\n",
    "  result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "      source_down, target_down, source_fpfh, target_fpfh,\n",
    "      o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "          maximum_correspondence_distance=distance_threshold))\n",
    "  return result\n",
    "\n",
    "def refine_registration(source, target, source_fpfh, target_fpfh, voxel_size, init):\n",
    "    distance_threshold = voxel_size * 0.4\n",
    "    print(\":: Point-to-plane ICP registration is applied on original point\")\n",
    "    print(\"   clouds to refine the alignment. This time we use a strict\")\n",
    "    print(\"   distance threshold %.3f.\" % distance_threshold)\n",
    "    result = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, distance_threshold, init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "    return result\n",
    "\n",
    "def get_o3d_icp_with_global_registration(source_pcd, target_pcd, voxel_size=10, init_random_time=10):\n",
    "  source_pcd_o3d, target_pcd_o3d = o3d.geometry.PointCloud(), o3d.geometry.PointCloud()\n",
    "  source_pcd_o3d.points = o3d.utility.Vector3dVector(source_pcd)\n",
    "  target_pcd_o3d.points = o3d.utility.Vector3dVector(target_pcd)\n",
    "  source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(voxel_size, source_pcd_o3d, target_pcd_o3d)\n",
    "  print(source, target)\n",
    "  print(source_down, target_down)\n",
    "\n",
    "  # perform multiple RANSAC inits, and choose the best amongest them\n",
    "  Trans, rmse = [], []\n",
    "  for i in tqdm(range(init_random_time)):\n",
    "    result_ransac = execute_global_registration(source_down, target_down, \\\n",
    "                      source_fpfh, target_fpfh, \\\n",
    "                      voxel_size)\n",
    "    # print(\"result_ransac: \")\n",
    "    # print(result_ransac)\n",
    "    # print(result_ransac.transformation.shape)\n",
    "    # o3dvis([getTransPcd(source_down.points, result_ransac.transformation), target_down.points])\n",
    "    # result_icp = refine_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size, result_ransac.transformation)\n",
    "    # TODO: Fetch the best initialization as the result\n",
    "    result_icp = get_o3d_icp(source_down.points, target_down.points, result_ransac.transformation)\n",
    "    # print(\"result_icp:\")\n",
    "    # print(result_icp)\n",
    "    Trans.append(result_icp.transformation)\n",
    "    rmse.append(result_icp.inlier_rmse)\n",
    "    # draw_registration_result(source_down, target_down, result_icp.transformation)\n",
    "  print(\"rmse list:\")\n",
    "  print(rmse)\n",
    "  return Trans[np.argmin(rmse)]\n",
    "\n",
    "def pts_to_o3d_pcd(pts):\n",
    "  \"\"\"Transform to o3d pcd\"\"\"\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "  return pcd\n",
    "\n",
    "def o3dvis(pts):\n",
    "  o3d.visualization.draw_geometries([pts_to_o3d_pcd(pt) for pt in pts])\n",
    "  return 0\n",
    "\n",
    "def getTransPcd(source_pcd, T):\n",
    "  return source_pcd @ T[:3, :3].T + T[:3, 3]\n",
    "\n",
    "\n",
    "# Define the data point to visualize\n",
    "train_vis_varianct, train_vis_index, object_ids, vis_id = \"1-1-4\", 0, [35, 39, 48, 51, 58], 58\n",
    "rgb_file = \"datas/1-1-4_color_kinect.png\"\n",
    "depth_file = \"datas/1-1-4_depth_kinect.png\"\n",
    "label_file = \"datas/1-1-4_label_kinect.png\"\n",
    "meta_file = \"datas/1-1-4_meta.pkl\"\n",
    "\n",
    "\n",
    "# Load scene meta info\n",
    "meta_vis = get_meta(meta_file)\n",
    "scales = meta_vis[\"scales\"]\n",
    "print(meta_vis.keys())\n",
    "intrinsic = meta_vis[\"intrinsic\"]\n",
    "extrinsic = meta_vis[\"extrinsic\"]\n",
    "inv_extrinsic = np.linalg.inv(extrinsic)\n",
    "gt_T_world = meta_vis[\"poses_world\"][vis_id]\n",
    "gt_T = extrinsic @ gt_T_world\n",
    "# print(gt_T.shape)\n",
    "gt_T = meta_vis[\"poses_world\"][vis_id]\n",
    "# Fetch source point cloud from model dictionary\n",
    "# print(object_models)\n",
    "\n",
    "\n",
    "# Reconstruct source and target point clouds (in camera frame)\n",
    "print(object_name)\n",
    "object_name = object_models_name[vis_id]\n",
    "vis_pcd_source = object_models[object_name]\n",
    "vis_pcd_source = vis_pcd_source * scales[vis_id]\n",
    "vis_pcd_target = get_object_point_cloud(get_label(label_file), \\\n",
    "                    vis_id, \\\n",
    "                    get_depth(depth_file), \\\n",
    "                    intrinsic)\n",
    "\n",
    "# o3dvis([vis_pcd_source])\n",
    "\n",
    "print(\"src and tg len:\")\n",
    "print(len(vis_pcd_target), len(vis_pcd_source))\n",
    "\n",
    "\n",
    "# Perform ICP\n",
    "# source_pcd, target_pcd = vis_pcd_target, vis_pcd_source\n",
    "source_pcd, target_pcd = vis_pcd_source, vis_pcd_target\n",
    "# source_pcd = np.hstack((source_pcd, np.ones((len(source_pcd), 1))))\n",
    "# target_pcd = np.hstack((target_pcd, np.ones((len(target_pcd), 1))))\n",
    "# source_pcd, target_pcd = (source_pcd @ np.linalg.inv(extrinsic).T), (target_pcd @ np.linalg.inv(extrinsic).T)\n",
    "# source_pcd = np.array([x[:-1]/x[-1] for x in source_pcd])\n",
    "# target_pcd = np.array([x[:-1]/x[-1] for x in target_pcd])\n",
    "# print(source_pcd.shape, target_pcd.shape)\n",
    "# o3dvis([source_pcd, target_pcd])\n",
    "T = get_o3d_icp_with_global_registration(source_pcd, target_pcd, voxel_size=5)\n",
    "# T = get_o3d_icp_tensor(source_pcd, target_pcd)\n",
    "# T = get_o3d_icp(source_pcd, target_pcd) # should use T.transformation to get transformation matrix!\n",
    "# T = icp(source_pcd, target_pcd, 100)\n",
    "T_world = inv_extrinsic @ T\n",
    "TrEstiPcd = source_pcd @ T[:3, :3].T + T[:3, 3]\n",
    "TrGtPcd = source_pcd @ gt_T[:3, :3].T + gt_T[:3, 3]\n",
    "\n",
    "o3dvis([TrEstiPcd, target_pcd])\n",
    "\n",
    "\n",
    "print(len(TrEstiPcd), len(TrGtPcd))\n",
    "# print(TrEstiPcd)\n",
    "# print(TrGtPcd)\n",
    "# o3d.visualization.draw_geometries([pts_to_o3d_pcd(TrEstiPcd), pts_to_o3d_pcd(TrGtPcd)])\n",
    "\n",
    "# T = np.eye(4)\n",
    "print(source_pcd.shape)\n",
    "print(target_pcd.shape)\n",
    "print(\"GT T:\")\n",
    "print(gt_T)\n",
    "print(\"GT T world:\")\n",
    "print(gt_T_world)\n",
    "print(\"pred T:\")\n",
    "print(T)\n",
    "print(\"pred T world:\")\n",
    "print(T_world)\n",
    "# rre = np.rad2deg(compute_rre(T[:3, :3], gt_T[:3, :3]))\n",
    "# rte = compute_rte(T[:3, 3], gt_T[:3, 3])\n",
    "rre = np.rad2deg(compute_rre(T_world[:3, :3], gt_T_world[:3, :3]))\n",
    "rte = compute_rte(T_world[:3, 3], gt_T_world[:3, 3])\n",
    "print(f\"rre={rre}, rte={rte}\")\n",
    "# show_points(target_pcd, \"Target pcd\")\n",
    "# show_points(source_pcd, \"Source pcd\")\n",
    "# show_points(TrEstiPcd, \"Transformed source pcd\")\n",
    "# show_points(TrGtPcd, \"GT Transformed source pcd\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize given pcd\"\"\"\n",
    "train_vis_varianct, train_vis_index, object_ids, vis_id = \"1-1-1\", 0, [35, 39, 48, 51, 58], 58\n",
    "object_name = object_models_name[vis_id]\n",
    "meta_vis = get_meta(meta_file)\n",
    "scales = meta_vis[\"scales\"]\n",
    "vis_pcd_source = object_models[object_name]\n",
    "vis_pcd_source = vis_pcd_source * scales[vis_id]\n",
    "o3d.visualization.draw_geometries([pts_to_o3d_pcd(vis_pcd_source)])\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
