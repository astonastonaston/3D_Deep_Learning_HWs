{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualization utilies.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def show_points(points, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim3d([-2, 2])\n",
    "    ax.set_ylim3d([-2, 2])\n",
    "    ax.set_zlim3d([0, 4])\n",
    "    ax.scatter(points[:, 0], points[:, 2], points[:, 1])\n",
    "\n",
    "def compare_points(points1, points2):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(projection='3d')\n",
    "    ax.set_xlim3d([-2, 2])\n",
    "    ax.set_ylim3d([-2, 2])\n",
    "    ax.set_zlim3d([0, 4])\n",
    "    ax.scatter(points1[:, 0], points1[:, 2], points1[:, 1])\n",
    "    ax.scatter(points2[:, 0], points2[:, 2], points2[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997 -0.01499998]\n",
      " [ 0.07499998 -0.02499997  0.01499998]\n",
      " [-0.07499998 -0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [-0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [-0.07499998  0.02499997 -0.01499998]\n",
      " [ 0.07499998  0.02499997  0.01499998]\n",
      " [ 0.07499998  0.02499997 -0.01499998]]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# Load the canonical-space object point clouds\n",
    "import trimesh\n",
    "import collada\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fps_downsample(points, number_of_points_to_sample):\n",
    "  selected_points = np.zeros((number_of_points_to_sample, 3))\n",
    "  dist = np.ones(points.shape[0]) * np.inf # distance to the selected set\n",
    "  for i in tqdm(range(number_of_points_to_sample)):\n",
    "      # pick the point with max dist\n",
    "      idx = np.argmax(dist)\n",
    "      selected_points[i] = points[idx]\n",
    "      dist_ = ((points - selected_points[i]) ** 2).sum(-1)\n",
    "      dist = np.minimum(dist, dist_)\n",
    "  return selected_points\n",
    "\n",
    "object_models_file = \"models/objects_v1.csv\"\n",
    "object_models_info = pd.read_csv(object_models_file)\n",
    "object_models_name = object_models_info[\"object\"].to_list()\n",
    "object_models_location = object_models_info[\"location\"].to_list()\n",
    "object_models_num = len(object_models_name)\n",
    "\n",
    "\n",
    "object_models = {}\n",
    "for object_name, object_mesh_dir in zip(object_models_name, object_models_location):\n",
    "  object_mesh_path = os.path.join(object_mesh_dir, \"visual_meshes\", \"visual.dae\")\n",
    "  # Collada-based loading\n",
    "  object_mesh = trimesh.exchange.dae.load_collada(object_mesh_path)\n",
    "  mesh_name = object_mesh['graph'][0]['geometry']\n",
    "  object_models[object_name] = object_mesh['geometry'][mesh_name]['vertices']\n",
    "  # Trimesh-based loading\n",
    "  # object_mesh = trimesh.load(object_mesh_path, force='mesh')\n",
    "  # object_models[object_name] = object_mesh.vertices\n",
    "  # print(object_mesh)\n",
    "\n",
    "# object_models with key=objectName, value=objectPointCloud\n",
    "# print(object_models_info)\n",
    "print(object_models[\"jenga\"])\n",
    "print(len(object_models[\"jenga\"]))\n",
    "# print(np.unique(object_models[\"jenga\"]))\n",
    "# print(len(object_models))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data loading helpers\"\"\"\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import utils\n",
    "import open3d as o3d\n",
    "# # Helper function for the reconstruction of the target point cloud\n",
    "# rgb = np.array(Image.open(rgb_files[0])) / 255   # convert 0-255 to 0-1\n",
    "# depth = np.array(Image.open(depth_files[0])) / 1000   # convert from mm to m\n",
    "# label = np.array(Image.open(label_files[0]))\n",
    "# meta = load_pickle(meta_files[0])\n",
    "# intrinsic = meta['intrinsic']\n",
    "# z = depth\n",
    "# v, u = np.indices(z.shape)\n",
    "# uv1 = np.stack([u + 0.5, v + 0.5, np.ones_like(z)], axis=-1)\n",
    "# points_viewer = uv1 @ np.linalg.inv(intrinsic).T * z[..., None]  # [H, W, 3]\n",
    "# # print(points_viewer.shape)\n",
    "# # print(points_viewer.shape[0]*points_viewer.shape[1])\n",
    "# # print(points_viewer)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def dump_json(sample, path):\n",
    "  with open(path, 'w') as fp:\n",
    "    json.dump(sample, fp)\n",
    "  return 0\n",
    "\n",
    "def load_json(path):\n",
    "  f = open(path)\n",
    "  data = json.load(f)\n",
    "  return data\n",
    "\n",
    "def get_point_cloud(depth, intrinsic):\n",
    "  z = depth\n",
    "  v, u = np.indices(z.shape)\n",
    "  uv1 = np.stack([u + 0.5, v + 0.5, np.ones_like(z)], axis=-1)\n",
    "  points_viewer = uv1 @ np.linalg.inv(intrinsic).T * z[..., None]  # [H, W, 3]\n",
    "  return points_viewer\n",
    "\n",
    "def pts_to_o3d_pcd(pts):\n",
    "  \"\"\"Transform to o3d pcd\"\"\"\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "  return pcd\n",
    "\n",
    "def o3dvis(pts):\n",
    "    o3d.visualization.draw_geometries([pts_to_o3d_pcd(pt) for pt in pts])\n",
    "    return 0\n",
    "\n",
    "def get_object_point_cloud(test_image_label, object_id, test_depth, intrinsic):\n",
    "  # print(np.where(test_image_label==object_id))\n",
    "  # print(test_image_label[327][654])\n",
    "  test_image_label[np.where(test_image_label==object_id)] = 255\n",
    "  # print(test_image_label[327][654])\n",
    "  # print(np.where(test_image_label==255))\n",
    "  test_image_label[np.where(test_image_label!=255)] = 0\n",
    "  test_image_label[np.where(test_image_label==255)] = 1\n",
    "  test_object_depth = test_depth * test_image_label\n",
    "  test_pcd_target = get_point_cloud(test_object_depth, intrinsic)\n",
    "  # (H, W, dim) = test_pcd_target.shape\n",
    "  # filter out target object point cloud\n",
    "  # print(test_pcd_target.shape)\n",
    "  # print(test_pcd_target)\n",
    "  # print((test_pcd_target[:,0]!=0)|(test_pcd_target[:,1]!=0)|(test_pcd_target[:,2]!=0))\n",
    "  test_pcd_target = test_pcd_target.reshape(-1, test_pcd_target.shape[-1]) # reshape to (H*W, 3)\n",
    "  test_pcd_target = test_pcd_target[(test_pcd_target[:,0]!=0)|(test_pcd_target[:,1]!=0)|(test_pcd_target[:,2]!=0)]\n",
    "  return test_pcd_target\n",
    "\n",
    "\n",
    "def get_meta(meta_path):\n",
    "  return load_pickle(meta_path)\n",
    "\n",
    "def get_depth(depth_path):\n",
    "  return (np.array(Image.open(depth_path))/1000)\n",
    "\n",
    "def get_label(label_path):\n",
    "  return np.array(Image.open(label_path))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"Metric and visualization.\"\"\"\n",
    "\n",
    "def compute_rre(R_est: np.ndarray, R_gt: np.ndarray):\n",
    "    \"\"\"Compute the relative rotation error (geodesic distance of rotation).\"\"\"\n",
    "    assert R_est.shape == (3, 3), 'R_est: expected shape (3, 3), received shape {}.'.format(R_est.shape)\n",
    "    assert R_gt.shape == (3, 3), 'R_gt: expected shape (3, 3), received shape {}.'.format(R_gt.shape)\n",
    "    # relative rotation error (RRE)\n",
    "    # Rotational degree loss (not objective of optimization)\n",
    "    rre = np.arccos(np.clip(0.5 * (np.trace(R_est.T @ R_gt) - 1), -1.0, 1.0))\n",
    "    return rre\n",
    "\n",
    "\n",
    "def compute_rte(t_est: np.ndarray, t_gt: np.ndarray):\n",
    "    assert t_est.shape == (3,), 't_est: expected shape (3,), received shape {}.'.format(t_est.shape)\n",
    "    assert t_gt.shape == (3,), 't_gt: expected shape (3,), received shape {}.'.format(t_gt.shape)\n",
    "    # relative translation error (RTE)\n",
    "    rte = np.linalg.norm(t_est - t_gt) # Resembling MSE loss\n",
    "    return rte\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['poses_world', 'extents', 'scales', 'object_ids', 'object_names', 'extrinsic', 'intrinsic'])\n",
      "[35 39 48 51 58]\n",
      "['jenga', 'master_chef_can', 'potted_meat_can', 'pudding_box', 'wood_block']\n",
      "wooden_puzzle3\n",
      "src and tg len:\n",
      "6117 49152\n",
      ":: Load two point clouds and disturb initial pose.\n",
      ":: Downsample with a voxel size 0.030.\n",
      ":: Estimate normal with search radius 0.060.\n",
      ":: Compute FPFH feature with search radius 0.150.\n",
      ":: Estimate normal with search radius 0.060.\n",
      ":: Compute FPFH feature with search radius 0.150.\n",
      "PointCloud with 49152 points. PointCloud with 6117 points.\n",
      "PointCloud with 4416 points. PointCloud with 6117 points.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:18<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdist list:\n",
      "[40.838890354689205, 28.13779620600238, 28.1410672672902, 40.52657242806276, 49.075959354355504, 49.17863542012413, 28.11946394776579, 53.99750767596552, 28.13560607965636, 48.186485626604636, 28.144658589455467, 49.64867463202808, 63.40137178316486, 40.59856568572702, 160.90379682172673, 148.70079902114287, 49.466064553216, 40.259667553730225, 48.6633517931776, 47.49001225887349, 48.745680107671895, 48.668086819895414, 48.65037252036005, 40.34916896948722, 39.98957468733967, 49.16036105598759, 40.619438996469995, 28.147027263738327, 131.93002722276526, 48.16546161509025, 28.068803025148128, 134.42394053734805, 49.11858488047333, 28.095218643013652, 62.512014640103715, 28.1390633534278, 47.936921031314284, 92.93833393835583, 39.49803528021863, 130.74728837042534, 48.04588928502494, 39.4223303015782, 28.155369204200394, 49.178879000813836, 48.022630126342094, 28.112953929549704, 28.1073455857653, 49.507629014310524, 40.57035201932845, 48.47363946166535, 48.08039104840375, 28.12178261491213, 48.47262965730101, 39.455066434543184, 49.53412379258465, 49.10233011231237, 27.86884431841769, 166.71262712904502, 28.084402787457133, 28.120801035219184, 63.029788997841806, 163.0227904586356, 28.109375616392885, 49.147394717429535, 48.72033725717043, 62.68952400968418, 40.134713410445755, 63.05937659427171, 48.949951097145274, 27.5893066027, 27.040861993747292, 28.158051390595574, 48.1852602911504, 164.93484090771733, 40.22963946658559, 48.028172466044296, 28.088332185238517, 48.01336457778825, 47.97948232880022, 82.02499289788426, 49.223162710923134, 47.952613135210385, 28.13701691977749, 28.1520439973137, 62.924528960781764, 28.127688708840402, 40.35060660027884, 48.137601064346214, 158.99147802746418, 48.14557137201071, 49.68644769954903, 63.121602783100826, 48.07804914560518, 76.13051855667842, 47.769040700947706, 48.49827699609949, 49.01734000513115, 49.554259447464396, 47.79432856275048, 48.20368877223008]\n",
      "27.040861993747292\n",
      "70\n",
      "The following gives rre and rte max indices and values:\n",
      "179.66947671977346 20\n",
      "0.005422011197594052 70\n",
      "Good list under current parameter sets (one and two):\n",
      "[175.52139852 175.21015704 175.13666648 175.50936214 175.48635079\n",
      " 176.72997467 176.52473086 177.99979034 177.91581972 178.50355663\n",
      " 178.40256017 178.5871746  175.67183613 178.57964869 175.99945952\n",
      " 176.33094841 176.31374474 178.94620701 175.36160247 178.02893701\n",
      " 175.12493266 176.0971814  177.48922087 178.54144277 178.60105546\n",
      " 176.26545062 178.81999081 177.76754999 179.34414935 175.82919648\n",
      " 175.73524512 177.05140129 178.49151071 177.84687012 179.50654489\n",
      " 176.44428422 175.28762971 178.69063073 178.58336584 176.05422351\n",
      " 178.50513586 178.93224145 175.82872788 175.47329182 175.53989514\n",
      " 178.58405114 178.59650431 177.04600812 175.95500062 178.60051569\n",
      " 178.48432518 177.85938436 177.5322566  178.40141973 178.83859894] 55\n",
      "[] 0\n",
      "Good list rmse and cdist (one and two):\n",
      "[-1.06120025 -1.0639831  -1.05968143 -1.06250983 -1.06026201 -0.41750562\n",
      "  0.93698079 -0.40322269 -0.42457359 -0.41961293 -0.40430321 -0.40744497\n",
      " -1.06257888 -0.42406124 -1.06357427  0.86848397 -1.06475964 -0.42270363\n",
      " -1.06335936 -0.40526154 -1.05933277 -1.05989033 -0.4019676  -0.42493625\n",
      " -0.42356318 -1.05912842 -0.41651928 -0.40388724 -1.04902032 -1.05953222\n",
      " -1.06081254  0.95368407  0.90185003 -0.4013783  -1.04751844 -1.06540976\n",
      " -1.06338274 -0.41425529 -0.42323854 -1.0629789  -0.42295531 -0.42281991\n",
      " -1.06232407 -1.0669005  -1.0614528  -0.42395326 -0.42390248 -0.40741551\n",
      "  0.9108697  -0.42172655 -0.42420618 -0.40000139 -0.40396676 -0.42172252\n",
      " -0.42431699]\n",
      "[-0.77429174 -0.77419319 -0.77484405 -0.77435772 -0.77408499 -0.12621529\n",
      "  0.28812386 -0.13171694 -0.15590094 -0.19125114 -0.15575829 -0.15629198\n",
      " -0.77401363 -0.1709013  -0.77557451  0.26132945 -0.77425356 -0.17450376\n",
      " -0.7737623  -0.14036922 -0.77504018 -0.77520915 -0.13046469 -0.16161657\n",
      " -0.1734643  -0.77477419 -0.16164699 -0.12966646 -0.78239468 -0.77480377\n",
      " -0.77514799  0.26667742  0.27782028 -0.14726632 -0.79081655 -0.80734\n",
      " -0.77368149 -0.17030481 -0.17503753 -0.77578198 -0.17650446 -0.17731397\n",
      " -0.77431522 -0.77386248 -0.77459626 -0.17174068 -0.17150055 -0.12507727\n",
      "  0.27969503 -0.18284461 -0.16087429 -0.14523604 -0.12905982 -0.18208274\n",
      " -0.1697496 ]\n",
      "[]\n",
      "[]\n",
      "Fractioned optimal rmse and cdist:\n",
      "-1.0654097572912915\n",
      "-0.8073400039892543\n",
      "Global optimal rmse and cdist, with indices:\n",
      "-1.0669005046602533 83\n",
      "-0.8073400039892543 70\n",
      "49152 49152\n",
      "(49152, 3)\n",
      "(6117, 3)\n",
      "GT T:\n",
      "[[ 0.8254157   0.55862933 -0.0813773   0.28677785]\n",
      " [ 0.39185923 -0.67073524 -0.6297306   0.11328191]\n",
      " [-0.40636858  0.48790103 -0.7725394   0.58364433]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "GT T world:\n",
      "[[ 4.5022696e-01 -8.9291412e-01 -4.5032657e-08  2.4753702e-01]\n",
      " [ 8.9291412e-01  4.5022696e-01 -3.5382783e-08  3.3320317e-01]\n",
      " [ 5.1868703e-08 -2.4280020e-08  1.0000000e+00  4.2127986e-02]\n",
      " [ 0.0000000e+00  0.0000000e+00  0.0000000e+00  1.0000000e+00]]\n",
      "pred T:\n",
      "[[-0.8621336  -0.50469152 -0.04485672  0.28436739]\n",
      " [-0.38472226  0.70965711 -0.59023349  0.11058534]\n",
      " [ 0.32971873 -0.49160275 -0.8059853   0.57960496]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "pred T world:\n",
      "[[-0.39260699  0.91852149  0.04666858  0.24825163]\n",
      " [-0.91789398 -0.39451259  0.04278327  0.33126953]\n",
      " [ 0.05770863 -0.02603982  0.99799384  0.04714282]\n",
      " [ 0.          0.          0.          1.        ]]\n",
      "rre=176.44428421899744, rte=0.005422011197594052\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Visualization on comparing between train pose-transformed point clouds.\"\"\"\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "\n",
    "# def draw_registration_result(source, target, transformation):\n",
    "#     source_temp = copy.deepcopy(source)\n",
    "#     target_temp = copy.deepcopy(target)\n",
    "#     source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "#     target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "#     source_temp.transform(transformation)\n",
    "#     o3d.visualization.draw_geometries([source_temp, target_temp],\n",
    "#                                       zoom=0.4559,\n",
    "#                                       front=[0.6452, -0.3036, -0.7011],\n",
    "#                                       lookat=[1.9892, 2.0208, 1.8945],\n",
    "#                                       up=[-0.2779, -0.9482, 0.1556])\n",
    "\n",
    "# def get_o3d_icp_tensor(source_pcd, target_pcd):\n",
    "#   # T = icp(source_pcd, target_pcd, 150)\n",
    "#   source_pcd_o3d, target_pcd_o3d = o3d.t.geometry.PointCloud(), o3d.t.geometry.PointCloud()\n",
    "#   # print(source_pcd)\n",
    "#   source_pcd_o3d.point.positions = o3d.core.Tensor(source_pcd, dtype=o3d.core.Dtype.Float64)\n",
    "#   # source_pcd_o3d.estimate_normals(o3d.geometry.KDTreeSearchParamKNN(knn=50))\n",
    "#   target_pcd_o3d.point.positions = o3d.core.Tensor(target_pcd, dtype=o3d.core.Dtype.Float64)\n",
    "#   # Search distance for Nearest Neighbour Search [Hybrid-Search is used].\n",
    "#   max_correspondence_distance = 1\n",
    "#   init = o3d.core.Tensor.eye(4, o3d.core.Dtype.Float64)\n",
    "#   # Select the `Estimation Method`, and `Robust Kernel` (for outlier-rejection).\n",
    "#   treg = o3d.cpu.pybind.t.pipelines.registration\n",
    "#   estimation = treg.TransformationEstimationPointToPoint()\n",
    "#   callback_after_iteration = lambda updated_result_dict : print(\"Iteration Index: {}, Fitness: {}, Inlier RMSE: {},\".format(\n",
    "#     updated_result_dict[\"iteration_index\"].item(), \\\n",
    "#     updated_result_dict[\"fitness\"].item(), \\\n",
    "#     updated_result_dict[\"inlier_rmse\"].item())) \\\n",
    "\n",
    "#   # Convergence-Criteria for Vanilla ICP\n",
    "#   criteria = treg.ICPConvergenceCriteria(relative_fitness=0.0000000000001,\n",
    "#                       relative_rmse=0.0000000000001,\n",
    "#                       max_iteration=100)\n",
    "\n",
    "#   # Down-sampling voxel-size.\n",
    "#   # voxel_size = 0.0016625\n",
    "#   # voxel_size = 0.003125\n",
    "#   voxel_size = 0.00625\n",
    "#   # voxel_size = 0.0125\n",
    "\n",
    "#   # Save iteration wise `fitness`, `inlier_rmse`, etc. to analyse and tune result.\n",
    "#   save_loss_log = True\n",
    "#   s = time.time()\n",
    "\n",
    "#   # voxel_size = 0.05  # means 5cm for this dataset\n",
    "#   # source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(voxel_size, source_pcd_o3d, target_pcd_o3d)\n",
    "#   # result_ransac = execute_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size)\n",
    "#   # init = o3d.core.Tensor(result_ransac.transformation, dtype=o3d.core.Dtype.Float64)\n",
    "\n",
    "#   registration_icp = treg.icp(source_pcd_o3d, target_pcd_o3d, max_correspondence_distance, init, estimation, criteria, voxel_size, callback_after_iteration)\n",
    "#   icp_time = time.time() - s\n",
    "#   T = registration_icp.transformation\n",
    "#   T = T.numpy()\n",
    "#   return T\n",
    "\n",
    "# def execute_fast_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size):\n",
    "#   distance_threshold = voxel_size * 0.5\n",
    "#   print(\":: Apply fast global registration with distance threshold %.3f\" \\\n",
    "#           % distance_threshold)\n",
    "#   result = o3d.pipelines.registration.registration_fgr_based_on_feature_matching(\n",
    "#       source_down, target_down, source_fpfh, target_fpfh,\n",
    "#       o3d.pipelines.registration.FastGlobalRegistrationOption(\n",
    "#           maximum_correspondence_distance=distance_threshold))\n",
    "#   return result\n",
    "\n",
    "# def refine_registration(source, target, source_fpfh, target_fpfh, voxel_size, init):\n",
    "#     distance_threshold = voxel_size * 0.4\n",
    "#     print(\":: Point-to-plane ICP registration is applied on original point\")\n",
    "#     print(\"   clouds to refine the alignment. This time we use a strict\")\n",
    "#     print(\"   distance threshold %.3f.\" % distance_threshold)\n",
    "#     result = o3d.pipelines.registration.registration_icp(\n",
    "#         source, target, distance_threshold, init,\n",
    "#         o3d.pipelines.registration.TransformationEstimationPointToPlane())\n",
    "#     return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "icp_global_voxel_size = 0.03\n",
    "icp_dist_threshold = 5\n",
    "init_random_time = 100\n",
    "max_preproc_nn_normal = 50\n",
    "max_preproc_nn_fpfh = 100\n",
    "tgSrcFactor = 1.3\n",
    "dist_voxel_factor = 3\n",
    "fps_dnsamp_factor = 2000\n",
    "# cdistFrac = 0\n",
    "cdistFrac = 0.1\n",
    "# heavy_dnsamp_frac = 9/14\n",
    "heavy_dnsamp_frac = 1/2\n",
    "# heavy_dnsamp_frac = 5/14\n",
    "icp_relative_fitness = 0.00001\n",
    "icp_relative_rmse = 0.00001\n",
    "icp_max_iteration = 500\n",
    "# light_heavy_dnsamp_vxsize_ratio = 0\n",
    "light_heavy_dnsamp_vxsize_ratio = 0.001\n",
    "ransac_max_iteration = 10000 \n",
    "ransac_max_validation = 0.9999\n",
    "# heavy_dnsamp_frac = 5/7\n",
    "# heavy_dnsamp_frac = 4/7\n",
    "\n",
    "# Define the data point to visualize\n",
    "train_vis_varianct, train_vis_index, object_ids, vis_id = \"1-1-15\", 0, [14, 35, 48, 56, 58], 48\n",
    "rgb_file = \"datas/{}_color_kinect.png\".format(train_vis_varianct)\n",
    "depth_file = \"datas/{}_depth_kinect.png\".format(train_vis_varianct)\n",
    "label_file = \"datas/{}_label_kinect.png\".format(train_vis_varianct)\n",
    "meta_file = \"datas/{}_meta.pkl\".format(train_vis_varianct)\n",
    "\n",
    "# object-based parameter adjustment\n",
    "# if vis_id==48:\n",
    "#   # icp_global_voxel_size = 0.05\n",
    "#   tgSrcFactor = 1.5\n",
    "#   # light_heavy_dnsamp_vxsize_ratio = 0.1\n",
    "#   # heavy_dnsamp_frac = 4.5/7\n",
    "\n",
    "\n",
    "def get_o3d_icp(source_pcd, target_pcd, init=np.eye(4), icp_dist_threshold=icp_dist_threshold,\n",
    "                icp_relative_fitness=icp_relative_fitness,\n",
    "                icp_relative_rmse=icp_relative_rmse,\n",
    "                icp_max_iteration=icp_max_iteration\n",
    "                ):\n",
    "  # T = icp(source_pcd, target_pcd, 150)\n",
    "  source_pcd_o3d, target_pcd_o3d = o3d.geometry.PointCloud(), o3d.geometry.PointCloud()\n",
    "  # print(source_pcd)\n",
    "  source_pcd_o3d.points = o3d.utility.Vector3dVector(source_pcd)\n",
    "  # source_pcd_o3d.estimate_normals(o3d.geometry.KDTreeSearchParamKNN(knn=50))\n",
    "  target_pcd_o3d.points = o3d.utility.Vector3dVector(target_pcd)\n",
    "  # target_pcd_o3d.estimate_normals(o3d.geometry.KDTreeSearchParamKNN(knn=50))\n",
    "  # print(source_pcd_o3d)\n",
    "  # print(target_pcd_o3d)\n",
    "  # init = np.random.rand(4,4)\n",
    "  # init = -1 + init * 2\n",
    "  # init[-1] = np.zeros(4)\n",
    "  # print(init)\n",
    "  # init[0,0] = 2\n",
    "  T = o3d.pipelines.registration.registration_icp( \\\n",
    "        source_pcd_o3d, target_pcd_o3d, icp_dist_threshold, init, \\\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint(), \\\n",
    "        # o3d.pipelines.registration.TransformationEstimationPointToPlane(), \\\n",
    "        o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=icp_relative_fitness, \\\n",
    "                                 relative_rmse=icp_relative_rmse, \\\n",
    "                                 max_iteration=icp_max_iteration))\n",
    "  # T = icp(source_pcd, target_pcd)\n",
    "  # print(T.inlier_rmse)\n",
    "  # print(T.fitness)\n",
    "  # print(T.correspondence_set)\n",
    "  return T\n",
    "\n",
    "def half_voxel_dnsample(source, voxel_size, heavy_dnsamp_frac=heavy_dnsamp_frac, \n",
    "                        light_heavy_dnsamp_vxsize_ratio=light_heavy_dnsamp_vxsize_ratio):\n",
    "  pts = source.points\n",
    "  size = len(pts)\n",
    "  # print(pts.shape)\n",
    "  first_half = o3d_Vec_to_o3d_pcd(pts[:int(size*heavy_dnsamp_frac)])\n",
    "  first_half = first_half.voxel_down_sample(voxel_size=voxel_size) # heavily down-sampled side\n",
    "  secn_half = o3d_Vec_to_o3d_pcd(pts[int(size*heavy_dnsamp_frac):])\n",
    "  if (light_heavy_dnsamp_vxsize_ratio != 0):\n",
    "    secn_half = secn_half.voxel_down_sample(voxel_size=voxel_size*light_heavy_dnsamp_vxsize_ratio) # weakly down-sampled side\n",
    "\n",
    "  first_half_pts = np.asarray(first_half.points) \n",
    "  secn_half_pts = np.asarray(secn_half.points)\n",
    "  full_pcd_pts = np.vstack((first_half_pts, secn_half_pts))\n",
    "  full_pcd_pts = pts_to_o3d_pcd(full_pcd_pts)\n",
    "  return full_pcd_pts\n",
    "\n",
    "def half_farthest_dnsample(source, fps_rates=[1/64, 1/32]):\n",
    "  pts = source.points\n",
    "  size = len(pts)\n",
    "  # print(pts.shape)\n",
    "  first_half = o3d_Vec_to_o3d_pcd(pts[:int(size*heavy_dnsamp_frac)])\n",
    "  first_half = first_half.farthest_point_down_sample(int(size * fps_rates[0])) # heavily down-sampled side\n",
    "  secn_half = o3d_Vec_to_o3d_pcd(pts[int(size*heavy_dnsamp_frac):])\n",
    "  if (fps_rates[1] != 1):\n",
    "    secn_half = secn_half.farthest_point_down_sample(int(size * fps_rates[1])) # weakly down-sampled side\n",
    "\n",
    "  first_half_pts = np.asarray(first_half.points) \n",
    "  secn_half_pts = np.asarray(secn_half.points)\n",
    "  full_pcd_pts = np.vstack((first_half_pts, secn_half_pts))\n",
    "  full_pcd_pts = pts_to_o3d_pcd(full_pcd_pts)\n",
    "  return full_pcd_pts\n",
    "\n",
    "def preprocess_point_cloud(source, target, \n",
    "                           voxel_size, max_nn_normal=max_preproc_nn_normal, \n",
    "                           max_nn_fpfh=max_preproc_nn_fpfh, tgSrcFactor=tgSrcFactor, fps_dnsamp_factor=fps_dnsamp_factor):\n",
    "    print(\":: Downsample with a voxel size %.3f.\" % voxel_size)\n",
    "    # pts = pcd.points\n",
    "    # size = len(source.points)\n",
    "    # if (len(target.points) > 1000):\n",
    "    #   target = target.farthest_point_down_sample(len(target.points))\n",
    "    size_factor = len(source.points) / len(target.points)\n",
    "    if (size_factor >= 1):\n",
    "      # print(len(target.points), fps_dnsamp_factor, len(target.points)//fps_dnsamp_factor)\n",
    "      # src_pcd_down = source.farthest_point_down_sample(len(source.points)//fps_dnsamp_factor)\n",
    "      # src_pcd_down = source.voxel_down_sample(voxel_size=0.019)\n",
    "      # src_pcd_down = source.voxel_down_sample(voxel_size=voxel_size)\n",
    "      src_pcd_down = half_voxel_dnsample(source, voxel_size=voxel_size)\n",
    "      # src_pcd_down = half_farthest_dnsample(source)\n",
    "    else: \n",
    "      src_pcd_down = source\n",
    "    # src_pcd_down = source\n",
    "    tg_pcd_down = target\n",
    "    # tg_pcd_down = target.farthest_point_down_sample(int(tgSrcFactor*len(src_pcd_down.points)))\n",
    "    # src_pcd_down = source.farthest_point_down_sample(int(size/5))\n",
    "    # src_pcd_down = source\n",
    "\n",
    "\n",
    "    radius_normal = voxel_size * 2\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    src_pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=max_nn_normal))\n",
    "    radius_feature = voxel_size * 5\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    src_pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        src_pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=max_nn_fpfh))\n",
    "\n",
    "\n",
    "    print(\":: Estimate normal with search radius %.3f.\" % radius_normal)\n",
    "    tg_pcd_down.estimate_normals(\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=max_nn_normal))\n",
    "    print(\":: Compute FPFH feature with search radius %.3f.\" % radius_feature)\n",
    "    tg_pcd_fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        tg_pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=max_nn_fpfh))\n",
    "\n",
    "    return src_pcd_down, src_pcd_fpfh, tg_pcd_down, tg_pcd_fpfh\n",
    "\n",
    "def prepare_dataset(voxel_size, source, target):\n",
    "    print(\":: Load two point clouds and disturb initial pose.\")\n",
    "\n",
    "    # trans_init = np.asarray([[0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0],\n",
    "    #                          [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0]])\n",
    "    # source.transform(trans_init)\n",
    "\n",
    "    # source_down, source_fpfh = preprocess_point_cloud(source, voxel_size)\n",
    "    # target_down, target_fpfh = preprocess_point_cloud(target, voxel_size)\n",
    "    source_down, source_fpfh, target_down, target_fpfh = preprocess_point_cloud(source, target, voxel_size)\n",
    "    return source, target, source_down, target_down, source_fpfh, target_fpfh\n",
    "\n",
    "def execute_global_registration(source_down, target_down, source_fpfh,\n",
    "                                target_fpfh, voxel_size, dist_voxel_factor=dist_voxel_factor,\n",
    "                                ransac_max_iteration=ransac_max_iteration,\n",
    "                                ransac_max_validation=ransac_max_validation):\n",
    "  distance_threshold = voxel_size * dist_voxel_factor\n",
    "  # print(\":: RANSAC registration on downsampled point clouds.\")\n",
    "  # print(\"   Since the downsampling voxel size is %.3f,\" % voxel_size)\n",
    "  # print(\"   we use a liberal distance threshold %.3f.\" % distance_threshold)\n",
    "  result = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "      source_down, target_down, source_fpfh, target_fpfh, True,\n",
    "      distance_threshold,\n",
    "      o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "      3, [\n",
    "          o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(\n",
    "              0.9),\n",
    "          o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(\n",
    "              distance_threshold)\n",
    "      ], o3d.pipelines.registration.RANSACConvergenceCriteria(ransac_max_iteration, ransac_max_validation))\n",
    "  return result\n",
    "\n",
    "\n",
    "def normalization(data):\n",
    "    _range = np.max(data) - np.min(data)\n",
    "    return (data - np.min(data)) / _range\n",
    "\n",
    "def standardization(data):\n",
    "    mu = np.mean(data, axis=0)\n",
    "    sigma = np.std(data, axis=0)\n",
    "    return (data - mu) / sigma\n",
    "\n",
    "def visualize_rre(Trans, rmse, cdists, optim_index):\n",
    "  rmseList = np.array(rmse)\n",
    "  cdistsList = np.array(cdists)\n",
    "  rreList = [np.rad2deg(compute_rre((inv_extrinsic @ t)[:3,:3], gt_T_world[:3,:3])) for t in Trans]\n",
    "  rteList = [compute_rte((inv_extrinsic @ t)[:3,3], gt_T_world[:3,3]) for t in Trans]\n",
    "  print(\"The following gives rre and rte max indices and values:\")\n",
    "  print(np.max(rreList), np.argmax(rreList))\n",
    "  print(np.min(rteList), np.argmin(rteList))\n",
    "  # print(rteList[np.argmax(rreList)], np.argmax(rreList))\n",
    "  rreList, rteList = np.array(rreList), np.array(rteList) \n",
    "  # print((rreList>=175))\n",
    "  # print((rteList<=0.01))\n",
    "  # np.intersect1d(np.where(rreList>=175), np.where(rteList<=0.01)) \n",
    "  goodList = rreList[np.intersect1d(np.where(rreList>=175), np.where(rteList<=0.01))]\n",
    "  goodListTwo = rreList[np.intersect1d(np.where(rreList<=5), np.where(rteList<=0.01))]\n",
    "  print(\"Good list under current parameter sets (one and two):\")\n",
    "  print(goodList, len(goodList))\n",
    "  print(goodListTwo, len(goodListTwo))\n",
    "  print(\"Good list rmse and cdist (one and two):\")\n",
    "  print(rmseList[np.intersect1d(np.where(rreList>=175), np.where(rteList<=0.01))])\n",
    "  print(cdistsList[np.intersect1d(np.where(rreList>=175), np.where(rteList<=0.01))])\n",
    "  print(rmseList[np.intersect1d(np.where(rreList<=5), np.where(rteList<=0.01))])\n",
    "  print(cdistsList[np.intersect1d(np.where(rreList<=5), np.where(rteList<=0.01))])\n",
    "  print(\"Fractioned optimal rmse and cdist:\")\n",
    "  print(rmseList[optim_index])\n",
    "  print(cdistsList[optim_index])\n",
    "  print(\"Global optimal rmse and cdist, with indices:\")\n",
    "  print(np.min(rmseList), np.argmin(rmseList))\n",
    "  print(np.min(cdistsList), np.argmin(cdistsList))\n",
    "  return 0\n",
    "\n",
    "def get_o3d_icp_with_global_registration(source_pcd, target_pcd, \n",
    "                                         voxel_size=icp_global_voxel_size, \n",
    "                                         init_random_time=init_random_time, \n",
    "                                         cdistFrac = cdistFrac):\n",
    "  source_pcd_o3d, target_pcd_o3d = o3d.geometry.PointCloud(), o3d.geometry.PointCloud()\n",
    "  source_pcd_o3d.points = o3d.utility.Vector3dVector(source_pcd)\n",
    "  target_pcd_o3d.points = o3d.utility.Vector3dVector(target_pcd)\n",
    "  source, target, source_down, target_down, source_fpfh, target_fpfh = prepare_dataset(voxel_size, source_pcd_o3d, target_pcd_o3d)\n",
    "  print(source, target)\n",
    "  print(source_down, target_down)\n",
    "\n",
    "  # perform multiple RANSAC inits, and choose the best amongest them\n",
    "  Trans, rmse, cdists = [], [], []\n",
    "  for i in tqdm(range(init_random_time)):\n",
    "    result_ransac = execute_global_registration(source_down, target_down, \\\n",
    "                      source_fpfh, target_fpfh, \\\n",
    "                      voxel_size)\n",
    "    # result_ransac = execute_fast_global_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size)\n",
    "    # print(\"result_ransac: \")\n",
    "    # print(result_ransac)\n",
    "    # print(result_ransac.transformation.shape)\n",
    "    # o3dvis([getTransPcd(source_down.points, result_ransac.transformation), target_down.points])\n",
    "    # result_icp = refine_registration(source_down, target_down, source_fpfh, target_fpfh, voxel_size, result_ransac.transformation)\n",
    "    # TODO: Fetch the best initialization as the result\n",
    "    result_icp = get_o3d_icp(source_down.points, target_down.points, init=result_ransac.transformation)\n",
    "    # print(\"result_icp:\")\n",
    "    # print(result_icp)\n",
    "    tr = result_icp.transformation\n",
    "    trans_source_down = copy.deepcopy(source_down)\n",
    "    trans_source_down = trans_source_down.transform(tr)\n",
    "    dists = target_down.compute_point_cloud_distance(trans_source_down)\n",
    "    dists = np.asarray(dists)\n",
    "    # print(\"chamfer dist:\")\n",
    "    # print(dists)\n",
    "    # print(dists)\n",
    "    Trans.append(tr)\n",
    "    rmse.append(result_icp.inlier_rmse)\n",
    "    cdists.append(np.sum(dists))\n",
    "    # draw_registration_result(source_down, target_down, result_icp.transformation)\n",
    "  print(\"cdist list:\")\n",
    "  print(cdists)\n",
    "  print(np.min(cdists))\n",
    "  print(np.argmin(cdists))\n",
    "  o3dvis([getTransPcd(source_down.points, Trans[np.argmin(np.array(cdists)+np.array(rmse))]), target_down.points])\n",
    "  # return Trans[np.argmin(rmse)]\n",
    "  cdistsNorm = standardization(np.array(cdists))\n",
    "  rmseNorm = standardization(np.array(rmse))\n",
    "  optim_index = np.argmin((1-cdistFrac)*rmseNorm+cdistFrac*cdistsNorm)\n",
    "  # visualize_rre(Trans, rmse, cdists, optim_index)\n",
    "  visualize_rre(Trans, rmseNorm, cdistsNorm, optim_index)\n",
    "  return Trans[np.argmin((1-cdistFrac)*rmseNorm+cdistFrac*cdistsNorm)]\n",
    "\n",
    "def pts_to_o3d_pcd(pts):\n",
    "  \"\"\"Transform to o3d pcd\"\"\"\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "  return pcd\n",
    "\n",
    "def o3d_Vec_to_o3d_pcd(pts):\n",
    "  \"\"\"Transform to o3d pcd\"\"\"\n",
    "  pcd = o3d.geometry.PointCloud()\n",
    "  pcd.points = pts\n",
    "  return pcd\n",
    "\n",
    "def o3dvis(pts):\n",
    "  o3d.visualization.draw_geometries([pts_to_o3d_pcd(pt) for pt in pts])\n",
    "  return 0\n",
    "\n",
    "def getTransPcd(source_pcd, T):\n",
    "  return source_pcd @ T[:3, :3].T + T[:3, 3]\n",
    "\n",
    "def get_transform(rotation, translation):\n",
    "    transformation = np.eye(4)\n",
    "    transformation[:3, :3] = rotation\n",
    "    transformation[:3, 3] = translation\n",
    "    return transformation\n",
    "\n",
    "\n",
    "\n",
    "# Load scene meta info\n",
    "meta_vis = get_meta(meta_file)\n",
    "scales = meta_vis[\"scales\"]\n",
    "print(meta_vis.keys())\n",
    "print(meta_vis[\"object_ids\"])\n",
    "print(meta_vis[\"object_names\"])\n",
    "intrinsic = meta_vis[\"intrinsic\"]\n",
    "extrinsic = meta_vis[\"extrinsic\"]\n",
    "inv_extrinsic = np.linalg.inv(extrinsic)\n",
    "gt_T_world = meta_vis[\"poses_world\"][vis_id]\n",
    "gt_T = extrinsic @ gt_T_world\n",
    "# print(gt_T.shape)\n",
    "# gt_T = meta_vis[\"poses_world\"][vis_id]\n",
    "# Fetch source point cloud from model dictionary\n",
    "# print(object_models)\n",
    "\n",
    "\n",
    "# Reconstruct source and target point clouds (in camera frame)\n",
    "print(object_name)\n",
    "object_name = object_models_name[vis_id]\n",
    "vis_pcd_source = object_models[object_name]\n",
    "vis_pcd_source = vis_pcd_source * scales[vis_id]\n",
    "vis_pcd_target = get_object_point_cloud(get_label(label_file), \\\n",
    "                    vis_id, \\\n",
    "                    get_depth(depth_file), \\\n",
    "                    intrinsic)\n",
    "\n",
    "# o3dvis([vis_pcd_source])\n",
    "\n",
    "print(\"src and tg len:\")\n",
    "print(len(vis_pcd_target), len(vis_pcd_source))\n",
    "\n",
    "\n",
    "# Perform ICP\n",
    "# source_pcd, target_pcd = vis_pcd_target, vis_pcd_source\n",
    "source_pcd, target_pcd = vis_pcd_source, vis_pcd_target\n",
    "# source_pcd = np.hstack((source_pcd, np.ones((len(source_pcd), 1))))\n",
    "# target_pcd = np.hstack((target_pcd, np.ones((len(target_pcd), 1))))\n",
    "# source_pcd, target_pcd = (source_pcd @ np.linalg.inv(extrinsic).T), (target_pcd @ np.linalg.inv(extrinsic).T)\n",
    "# source_pcd = np.array([x[:-1]/x[-1] for x in source_pcd])\n",
    "# target_pcd = np.array([x[:-1]/x[-1] for x in target_pcd])\n",
    "# print(source_pcd.shape, target_pcd.shape)\n",
    "# o3dvis([source_pcd, target_pcd])\n",
    "T = get_o3d_icp_with_global_registration(source_pcd, target_pcd)\n",
    "# T = get_o3d_icp_tensor(source_pcd, target_pcd)\n",
    "# T = get_o3d_icp(source_pcd, target_pcd) # should use T.transformation to get transformation matrix!\n",
    "# T = icp(source_pcd, target_pcd, 100)\n",
    "T_world = inv_extrinsic @ T\n",
    "TrEstiPcd = source_pcd @ T[:3, :3].T + T[:3, 3]\n",
    "TrGtPcd = source_pcd @ gt_T[:3, :3].T + gt_T[:3, 3]\n",
    "# TrGtPcd = source_pcd @ gt_T[:3, :3].T + gt_T[:3, 3]\n",
    "\n",
    "\n",
    "o3dvis([TrEstiPcd, target_pcd])\n",
    "\n",
    "# o3dvis([TrGtPcd, target_pcd])\n",
    "\n",
    "\n",
    "print(len(TrEstiPcd), len(TrGtPcd))\n",
    "# print(TrEstiPcd)\n",
    "# print(TrGtPcd)\n",
    "# o3d.visualization.draw_geometries([pts_to_o3d_pcd(TrEstiPcd), pts_to_o3d_pcd(TrGtPcd)])\n",
    "\n",
    "# T = np.eye(4)\n",
    "print(source_pcd.shape)\n",
    "print(target_pcd.shape)\n",
    "print(\"GT T:\")\n",
    "print(gt_T)\n",
    "print(\"GT T world:\")\n",
    "print(gt_T_world)\n",
    "print(\"pred T:\")\n",
    "print(T)\n",
    "print(\"pred T world:\")\n",
    "print(T_world)\n",
    "# rre = np.rad2deg(compute_rre(T[:3, :3], gt_T[:3, :3]))\n",
    "# rte = compute_rte(T[:3, 3], gt_T[:3, 3])\n",
    "rre = np.rad2deg(compute_rre(T_world[:3, :3], gt_T_world[:3, :3]))\n",
    "rte = compute_rte(T_world[:3, 3], gt_T_world[:3, 3])\n",
    "print(f\"rre={rre}, rte={rte}\")\n",
    "# show_points(target_pcd, \"Target pcd\")\n",
    "# show_points(source_pcd, \"Source pcd\")\n",
    "# show_points(TrEstiPcd, \"Transformed source pcd\")\n",
    "# show_points(TrGtPcd, \"GT Transformed source pcd\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "source_points = vis_pcd_target # RGBD pcd, 6117\n",
    "target_points = vis_pcd_source # Model pcd, 49000\n",
    "# print(source_points.shape, target_points.shape)\n",
    "target_points_o3d = pts_to_o3d_pcd(target_points)\n",
    "\n",
    "target_points_o3d = target_points_o3d.farthest_point_down_sample(len(source_points)) # heavily down-sampled side\n",
    "target_points_pts = np.asarray(target_points_o3d.points) \n",
    "\n",
    "def transformation_generate(rotation, translation):\n",
    "    '''\n",
    "    input:\n",
    "        rotation 3*3\n",
    "        translation 3*1\n",
    "    output:\n",
    "        transformation 4*4\n",
    "    '''\n",
    "    transformation = np.eye(4)\n",
    "    \n",
    "    transformation[:3, :3] = rotation\n",
    "    transformation[:3, 3] = translation\n",
    "    \n",
    "    return transformation\n",
    "\n",
    "def nearest_neighbor(src, dst):\n",
    "    '''\n",
    "    Find the nearest (Euclidean) neighbor in dst for each point in src\n",
    "    Input:\n",
    "        src: Nxm array of points\n",
    "        dst: Nxm array of points\n",
    "    Output:\n",
    "        distances: Euclidean distances of the nearest neighbor\n",
    "        indices: dst indices of the nearest neighbor\n",
    "    '''\n",
    "    # print(src.shape,dst.shape)\n",
    "    assert src.shape == dst.shape\n",
    "\n",
    "    neigh = NearestNeighbors(n_neighbors=1)\n",
    "    neigh.fit(dst)\n",
    "    distances, indices = neigh.kneighbors(src, return_distance=True)\n",
    "    return distances.ravel(), indices.ravel()\n",
    "\n",
    "def umeyama_alignment(x, y, with_scale = True):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        x: 3*n\n",
    "        y: 3*n\n",
    "        with_scale: calculate scale or not\n",
    "    output:\n",
    "        r: rotation\n",
    "        t: translation\n",
    "        c: scale\n",
    "    \"\"\"\n",
    "\n",
    "    if x.shape != y.shape:\n",
    "        raise GeometryException(\"data matrices must have the same shape\")\n",
    "\n",
    "    # m = dimension, n = nr. of data points\n",
    "    m, n = x.shape\n",
    "\n",
    "    # means, eq. 34 and 35\n",
    "    mean_x = x.mean(axis=1)\n",
    "    mean_y = y.mean(axis=1)\n",
    "\n",
    "    # variance, eq. 36\n",
    "    # \"transpose\" for column subtraction\n",
    "    sigma_x = 1.0 / n * (np.linalg.norm(x - mean_x[:, np.newaxis])**2)\n",
    "\n",
    "    # covariance matrix, eq. 38\n",
    "    outer_sum = np.zeros((m, m))\n",
    "    for i in range(n):\n",
    "        outer_sum += np.outer((y[:, i] - mean_y), (x[:, i] - mean_x))\n",
    "    cov_xy = np.multiply(1.0 / n, outer_sum)\n",
    "\n",
    "    # SVD (text betw. eq. 38 and 39)\n",
    "    u, d, v = np.linalg.svd(cov_xy)  \n",
    "\n",
    "    # S matrix, eq. 43\n",
    "    s = np.eye(m)\n",
    "    if np.linalg.det(u) * np.linalg.det(v) < 0.0:\n",
    "        # Ensure a RHS coordinate system (Kabsch algorithm).\n",
    "        s[m - 1, m - 1] = -1\n",
    "\n",
    "    # rotation, eq. 40\n",
    "    r = u.dot(s).dot(v)\n",
    "\n",
    "    # scale & translation, eq. 42 and 41\n",
    "    c = 1 / sigma_x * np.trace(np.diag(d).dot(s)) if with_scale else 1.0\n",
    "\n",
    "    t = mean_y - np.multiply(c, r.dot(mean_x))\n",
    "\n",
    "    return r, t, c\n",
    "\n",
    "def icp(source, target, max_iterations=100, tolerance=1e-6, init_scale = 1, init_transformation = None):\n",
    "    \"\"\"\n",
    "    iterative get rigid transformation\n",
    "    input:\n",
    "        source: N*3\n",
    "        target: N*3\n",
    "        init_transformation = 4*4\n",
    "    output:\n",
    "        src: transformed point clouds\n",
    "    \"\"\"\n",
    "    src = np.copy(source.T) # 3*N\n",
    "    dst = np.copy(target.T) # 3*N\n",
    "    \n",
    "    if init_transformation is not None:\n",
    "        r = init_transformation[:3, :3]\n",
    "        t = init_transformation[:3, 3][:, np.newaxis]\n",
    "        src = np.multiply(init_scale, np.dot(r, src)) + t\n",
    "    prev_error = 0\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        distances, indices = nearest_neighbor(src.T,dst.T)\n",
    "        rotation, translation, scale = umeyama_alignment(src, dst[:,indices], with_scale=False)\n",
    "        # TODO:write into homogeneous coordinates\n",
    "        src = np.multiply(scale, rotation.dot(src)) + translation[:,np.newaxis]\n",
    "        mean_error = np.mean(distances)\n",
    "        \n",
    "        if np.abs(prev_error-mean_error) < tolerance:\n",
    "            # last iterative calculate the scale\n",
    "            rotation, translation, scale = umeyama_alignment(src, dst[:,indices], with_scale=True)\n",
    "            src = np.multiply(scale, rotation.dot(src)) + translation[:,np.newaxis]\n",
    "            break\n",
    "        prev_error = mean_error\n",
    "    # calc source point clouds Transformation\n",
    "    rotation, translation, scale = umeyama_alignment(source.T, src, with_scale=True)\n",
    "    return rotation, translation, scale, src\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "source_points: from RGBD\n",
    "target_points: from canonical frame\n",
    "\"\"\"\n",
    "\n",
    "target_points_pts, source_points = source_points, target_points_pts\n",
    "# init_trans: TODO can be improved to get better result\n",
    "t_wlh = np.max(target_points_pts,axis=0)-np.min(target_points_pts,axis=0)\n",
    "# t_wlh = np.max(target_points,axis=0)-np.min(target_points,axis=0)\n",
    "s_wlh = np.max(source_points,axis=0)-np.min(source_points,axis=0)\n",
    "init_scale = np.mean(t_wlh/s_wlh)\n",
    "\n",
    "# iterative closest points: get rigid transformation result\n",
    "rotation, translation, scale, Tsrc = icp(\n",
    "    source_points, target_points_pts, init_scale = init_scale, init_transformation=np.eye(4)\n",
    "    # source_points, target_points, init_scale = init_scale, init_transformation=np.eye(4)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rre=174.56518010558622, rte=0.016216558186695342\n"
     ]
    }
   ],
   "source": [
    "rotation, translation\n",
    "tr = transformation_generate(rotation, translation)\n",
    "tr_world = inv_extrinsic @ tr\n",
    "rre = np.rad2deg(compute_rre(tr_world[:3, :3], gt_T_world[:3, :3]))\n",
    "rte = compute_rte(tr_world[:3, 3], gt_T_world[:3, 3])\n",
    "print(f\"rre={rre}, rte={rte}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize given pcd\"\"\"\n",
    "train_vis_varianct, train_vis_index, object_ids, vis_id = \"1-1-1\", 0, [35, 39, 48, 51, 58], 58\n",
    "object_name = object_models_name[vis_id]\n",
    "meta_vis = get_meta(meta_file)\n",
    "scales = meta_vis[\"scales\"]\n",
    "vis_pcd_source = object_models[object_name]\n",
    "vis_pcd_source = vis_pcd_source * scales[vis_id]\n",
    "o3d.visualization.draw_geometries([pts_to_o3d_pcd(vis_pcd_source)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 126] 找不到指定的模块。 Error loading \"c:\\Users\\NamShoo\\miniconda3\\lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PointNetDense network (saved)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\NamShoo\\miniconda3\\lib\\site-packages\\torch\\__init__.py:137\u001b[0m\n\u001b[0;32m    135\u001b[0m                 err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(ctypes\u001b[38;5;241m.\u001b[39mget_last_error())\n\u001b[0;32m    136\u001b[0m                 err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 137\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    139\u001b[0m     kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# See Note [Global dependencies]\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 126] 找不到指定的模块。 Error loading \"c:\\Users\\NamShoo\\miniconda3\\lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "# PointNetDense network (saved)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "\n",
    "def rot_6d(x, y):\n",
    "    # Gram-schmidt process to get the rotation matrix\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = y - x * (x * y).sum(-1, keepdims=True)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    z = torch.cross(x, y, -1)\n",
    "    return torch.stack([x, y, z], dim=-1)\n",
    "\n",
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k_input=32, k_output=32*32, n_points=256, spread_points=False):\n",
    "        self.n_points = n_points\n",
    "        self.spread_points = spread_points\n",
    "        self.k_input = k_input\n",
    "        self.k_output = k_output\n",
    "\n",
    "        super(STNkd, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k_input, 64, 1) # feature extractors\n",
    "        self.conv2 = torch.nn.Conv1d(64, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 512, 1)\n",
    "        self.bnConv1 = nn.BatchNorm1d(64)\n",
    "        self.bnConv2 = nn.BatchNorm1d(256)\n",
    "        self.bnConv3 = nn.BatchNorm1d(512)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        if self.spread_points:\n",
    "          self.fc4 = nn.Linear(128, k_output*n_points)\n",
    "        else:\n",
    "          self.fc4 = nn.Linear(128, k_output)\n",
    "        self.bnDense2 = nn.BatchNorm1d(256)\n",
    "        self.bnDense3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        # weight initializations\n",
    "        for net in [\n",
    "            self.conv1,\n",
    "            self.conv2,\n",
    "            self.conv3,\n",
    "            self.fc2,\n",
    "            self.fc3,\n",
    "            self.fc4\n",
    "        ]:\n",
    "            torch.nn.init.xavier_uniform_(net.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bnConv1(self.conv1(x)))\n",
    "        x = F.relu(self.bnConv2(self.conv2(x)))\n",
    "        x = F.relu(self.bnConv3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 512)\n",
    "\n",
    "        x = F.relu(self.bnDense2(self.fc2(x)))\n",
    "        x = F.relu(self.bnDense3(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        if self.spread_points:\n",
    "          x = x.view(-1, self.k_output, self.n_points)\n",
    "        return x\n",
    "\n",
    "class PointNet(nn.Module):\n",
    "    def __init__(self, global_feature_size=81, n_points=256): # global feature = object id (size=79) + scene id (size=2)\n",
    "        super(PointNet, self).__init__()\n",
    "        self.n_points = n_points\n",
    "        self.global_feature_size = global_feature_size\n",
    "        self.stn3d_input_feat = STNkd(k_input=3, k_output=3*3, n_points=self.n_points, spread_points=True)\n",
    "        self.conv1 = torch.nn.Conv1d(3*3, 64, 1) # local feature extraction layers\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 256, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(256, 512, 1)\n",
    "        self.conv5 = torch.nn.Conv1d(512, 1024, 1)\n",
    "        self.bnConv1 = nn.BatchNorm1d(64)\n",
    "        self.bnConv2 = nn.BatchNorm1d(128)\n",
    "        self.bnConv3 = nn.BatchNorm1d(256)\n",
    "        self.bnConv4 = nn.BatchNorm1d(512)\n",
    "        self.bnConv5 = nn.BatchNorm1d(1024)\n",
    "\n",
    "        self.branch1 = torch.nn.Linear(global_feature_size, 256) # global feature extraction layers\n",
    "        self.bnBranch1 = nn.BatchNorm1d(256)\n",
    "        self.branch2 = torch.nn.Linear(256, 1024) # global feature extraction layers\n",
    "        self.bnBranch2 = nn.BatchNorm1d(1024)\n",
    "        self.dense2_r = torch.nn.Linear(2048, 1024) # inference layers\n",
    "        self.dense2_t = torch.nn.Linear(2048, 1024)\n",
    "        self.bnDense2_r = nn.BatchNorm1d(1024)\n",
    "        self.bnDense2_t = nn.BatchNorm1d(1024)\n",
    "        self.dense3_r = torch.nn.Linear(1024, 512)\n",
    "        self.dense3_t = torch.nn.Linear(1024, 512)\n",
    "        self.bnDense3_r = nn.BatchNorm1d(512)\n",
    "        self.bnDense3_t = nn.BatchNorm1d(512)\n",
    "        self.dense4_r = torch.nn.Linear(512, 256)\n",
    "        self.dense4_t = torch.nn.Linear(512, 256)\n",
    "        self.bnDense4_r = nn.BatchNorm1d(256)\n",
    "        self.bnDense4_t = nn.BatchNorm1d(256)\n",
    "        self.dense5_r = torch.nn.Linear(256, 128)\n",
    "        self.dense5_t = torch.nn.Linear(256, 128)\n",
    "        self.bnDense5_r = nn.BatchNorm1d(128)\n",
    "        self.bnDense5_t = nn.BatchNorm1d(128)\n",
    "        self.dense6_r = torch.nn.Linear(128, 6) # output layer for rotation\n",
    "        self.dense6_t = torch.nn.Linear(128, 3) # output layer for translation\n",
    "\n",
    "        # weight initializations\n",
    "        for net in [\n",
    "            # self.conv0,\n",
    "            self.conv1,\n",
    "            self.conv2,\n",
    "            self.conv3,\n",
    "            self.conv4,\n",
    "            self.conv5,\n",
    "            self.branch1,\n",
    "            self.branch2,\n",
    "            self.dense2_r,\n",
    "            self.dense2_t,\n",
    "            self.dense3_r,\n",
    "            self.dense3_t,\n",
    "            self.dense4_r,\n",
    "            self.dense4_t,\n",
    "            self.dense5_r,\n",
    "            self.dense5_t,\n",
    "            self.dense6_r,\n",
    "            self.dense6_t\n",
    "        ]:\n",
    "            torch.nn.init.xavier_uniform_(net.weight)\n",
    "\n",
    "    def forward(self, x, label, scene):\n",
    "        points = x[:, :3]  # batch_size, 3, n_points\n",
    "        # colors = x[:, 3:]\n",
    "\n",
    "        # normalize to [-1,1] centered at (0,0,0)\n",
    "        mins = points.min(dim=2, keepdim=True).values\n",
    "        maxs = points.max(dim=2, keepdim=True).values\n",
    "        center = (mins + maxs) / 2\n",
    "        half_extents = (maxs - mins) / 2\n",
    "        longest = half_extents.max(dim=1, keepdim=True).values.clamp(\n",
    "            min=1e-3\n",
    "        )\n",
    "        points = (points - center) / longest\n",
    "\n",
    "        # pcd feature extractions\n",
    "        x = points\n",
    "\n",
    "        # initial transform with spatial transformer\n",
    "        x = self.stn3d_input_feat(x)\n",
    "        x = F.relu(self.bnConv1(self.conv1(x)))\n",
    "        x = F.relu(self.bnConv2(self.conv2(x)))\n",
    "        x = F.relu(self.bnConv3(self.conv3(x)))\n",
    "        x = F.relu(self.bnConv4(self.conv4(x)))\n",
    "        x = F.relu(self.bnConv5(self.conv5(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        # global feature extractions\n",
    "        globalFeature = torch.cat((label, scene), dim=1)\n",
    "        globalFeature = self.bnBranch1(self.branch1(globalFeature))\n",
    "        globalFeature = self.bnBranch2(self.branch2(globalFeature))\n",
    "\n",
    "        # concatenate local and global features, and transform using a transformer\n",
    "        x = torch.cat((x, globalFeature), dim=1)\n",
    "        x_rot = x\n",
    "        x_tran = x_rot.clone()\n",
    "\n",
    "        # infer rotation\n",
    "        x_rot = F.relu(self.bnDense2_r(self.dense2_r(x_rot)))\n",
    "        x_rot = F.relu(self.bnDense3_r(self.dense3_r(x_rot)))\n",
    "        x_rot = F.relu(self.bnDense4_r(self.dense4_r(x_rot)))\n",
    "        x_rot = F.relu(self.bnDense5_r(self.dense5_r(x_rot)))\n",
    "        x_rot = self.dense6_r(x_rot)\n",
    "        x_rot = rot_6d(x_rot[..., 0:3], x_rot[..., 3:6]) # fetch output rotation matrix\n",
    "\n",
    "        x_tran = F.relu(self.bnDense2_t(self.dense2_t(x_tran)))\n",
    "        x_tran = F.relu(self.bnDense3_t(self.dense3_t(x_tran)))\n",
    "        x_tran = F.relu(self.bnDense4_t(self.dense4_t(x_tran)))\n",
    "        x_tran = F.relu(self.bnDense5_t(self.dense5_t(x_tran)))\n",
    "        x_tran = self.dense6_t(x_tran)\n",
    "        x_tran = x_tran * longest.view(-1, 1) + center.view_as(x_tran) # scale back and un-center (batch_size, 3), to get translation vector\n",
    "        return x_tran, x_rot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start ICP estimation for all test data\n",
    "!mkdir test_outputs_pointnet_icp_1\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Flag for continuing from previous steps\n",
    "ctu = False\n",
    "ctu_variant, ctu_index = 0, 0\n",
    "if ctu:\n",
    "  ctu_variant, ctu_index = \"2-50-1\", 149\n",
    "\n",
    "# Output resulting dictionary\n",
    "dataset_type = \"test\"\n",
    "output_dir = \"./{}_outputs_pointnet_icp_1\".format(dataset_type)\n",
    "test_size = 200\n",
    "\n",
    "# pointnet initializer loading\n",
    "init_dict = load_json(\"/content/result_test_pointnet.json\")\n",
    "\n",
    "# For each object at each testing data: Get scene object's point cloud (as target pcd)\n",
    "result_dict = {}\n",
    "if ctu:\n",
    "  json_path = os.path.join(output_dir, \"result_till_{}_index_{}.json\".format(ctu_variant, ctu_index))\n",
    "  result_dict = load_json(json_path)\n",
    "print(test_depth_files)\n",
    "print(test_meta_files)\n",
    "print(test_label_files)\n",
    "print(test_prefix_ids)\n",
    "\n",
    "# for test_depth_path, test_meta_path, test_label_path, test_prefix_id in tqdm(zip(test_depth_files, test_meta_files, test_label_files, test_prefix_ids)):\n",
    "for i in range(ctu_index, test_size):\n",
    "  if (i % 10 == 0):\n",
    "    print(\"Auto-uploading test outputs to cloud at epoch {}\".format(i))\n",
    "    !cp -r /content/test_outputs_pointnet_icp_1 /content/drive/MyDrive/\n",
    "  test_depth_path, test_meta_path, test_label_path, test_prefix_id = test_depth_files[i], test_meta_files[i], test_label_files[i], test_prefix_ids[i]\n",
    "  print(\"Test data id {}\".format(test_prefix_id))\n",
    "  test_meta, test_depth, test_seg_label = get_meta(test_meta_path), get_depth(test_depth_path), get_label(test_label_path)   # convert from mm to m\n",
    "  intrinsic = test_meta[\"intrinsic\"]\n",
    "  extrinsic = test_meta[\"extrinsic\"]\n",
    "  inv_extrinsic = np.linalg.inv(extrinsic)\n",
    "  scales = test_meta[\"scales\"]\n",
    "  result_dict[test_prefix_id] = {}\n",
    "  result_dict[test_prefix_id][\"poses_world\"] = [None] * object_models_num\n",
    "  for object_id, object_name in zip(test_meta[\"object_ids\"], test_meta[\"object_names\"]):\n",
    "    # print(test_meta[\"object_ids\"], test_meta[\"object_names\"])\n",
    "    print(\"Object id {}, name {}\".format(object_id, object_name))\n",
    "    # if (object_id != 56):\n",
    "      # continue\n",
    "    # mask the object's depth map to get test_object_depth\n",
    "    # print(object_id, type(object_id))\n",
    "    # print(np.sum(test_seg_label==object_id))\n",
    "    if (np.sum(test_seg_label==object_id) == 0):\n",
    "      print(\"Object not found at scene.\")\n",
    "      continue\n",
    "    test_image_label = test_seg_label.copy()\n",
    "\n",
    "    # mask the object's depth map to get test_object_depth\n",
    "    # print(test_image_label[test_image_label==30])\n",
    "    # print(test_image_label[test_image_label==51])\n",
    "    test_pcd_target = get_object_point_cloud(test_image_label, object_id, test_depth, intrinsic)\n",
    "\n",
    "    # Fetch source point cloud from model dictionary\n",
    "    test_pcd_source = object_models[object_name]\n",
    "    test_pcd_source *= scales[object_id] # scale the model pcd using factors\n",
    "    if (not test_pcd_source.any()):\n",
    "      print(\"Model pcd are all origins. Weird...\")\n",
    "      continue\n",
    "    # print(test_pcd_source.shape)\n",
    "    # print(test_pcd_target.shape)\n",
    "\n",
    "    # ICP to poinget posture\n",
    "    init = init_dict[test_prefix_id][\"poses_world\"][object_id]\n",
    "    init_cam = extrinsic @ init # map to camera frame\n",
    "\n",
    "    T = get_o3d_icp_with_pointnet_init(test_pcd_source, test_pcd_target, init=init_cam)\n",
    "    T_world = inv_extrinsic @ T # Transform to world coordinate\n",
    "    # TODO: may simply take PointNet's rotation and ICP's translation as results\n",
    "    # T_world[:3, :3] = init[:3, :3]\n",
    "\n",
    "    # update resulting dictionary with object id and pose list\n",
    "    result_dict[test_prefix_id][\"poses_world\"][object_id] = T_world.tolist()\n",
    "  if (((i+1)%10)==0):\n",
    "    output_name = \"result_till_{}_index_{}.json\".format(test_prefix_id, i)\n",
    "    dump_json(result_dict, os.path.join(output_dir, output_name))\n",
    "\n",
    "print(\"Test result:\")\n",
    "print(result_dict)\n",
    "print(len(result_dict))\n",
    "\n",
    "\n",
    "\n",
    "# Output resulting dictionary\n",
    "output_name = \"result_pointnet_icp_1.json\"\n",
    "dump_json(result_dict, os.path.join(output_dir, output_name))\n",
    "!cp /content/test_outputs_pointnet_icp_1/result_pointnet_icp_1.json /content/drive/MyDrive/outputs\n",
    "!cp -r /content/test_outputs_pointnet_icp_1 /content/drive/MyDrive/\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
