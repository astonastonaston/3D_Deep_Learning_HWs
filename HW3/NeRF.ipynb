{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeElnIzXySkk",
        "outputId": "13ef1782-738a-46e2-9d43-28de046b9208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2mxlzte17k8"
      },
      "source": [
        "# Github repo cloning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxyWSLjpt9qX",
        "outputId": "ef7e9119-96c6-4708-b06b-6584df0a4af0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'nerf-pytorch'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (45/45), done.\u001b[K\n",
            "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
            "remote: Total 187 (delta 26), reused 33 (delta 15), pack-reused 142\u001b[K\n",
            "Receiving objects: 100% (187/187), 400.65 KiB | 13.82 MiB/s, done.\n",
            "Resolving deltas: 100% (93/93), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/astonastonaston/nerf-pytorch.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrtvVcGZ2PF9"
      },
      "source": [
        "# Dataset downloading and re-formulations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2aCDmCZ0Mhr"
      },
      "outputs": [],
      "source": [
        "# Prepare sample datasets\n",
        "# !cd nerf-pytorch && bash download_example_data.sh\n",
        "!cd nerf-pytorch && rm -rf data\n",
        "!cd nerf-pytorch && mkdir data && cd data && mkdir nerf_synthetic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-G20uTyyIij"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/bottles/bottles /content/nerf-pytorch/data/nerf_synthetic/bottles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxPJoYBb2KzU"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcQXobY-2ZBN"
      },
      "source": [
        "# Environmental preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcXdHeq-yRvC",
        "outputId": "52f13b8d-a4b4-4462-f1d5-b25a8dca6a03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu113\n",
            "Collecting torch==1.11.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torch-1.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (1637.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 GB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.12.0+cu113\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.12.0%2Bcu113-cp310-cp310-linux_x86_64.whl (22.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchaudio==0.11.0\n",
            "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.11.0%2Bcu113-cp310-cp310-linux_x86_64.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0+cu113) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.12.0+cu113) (9.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.12.0+cu113) (2023.11.17)\n",
            "Installing collected packages: torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.16.0+cu121\n",
            "    Uninstalling torchvision-0.16.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.16.0+cu121\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 2.1.0+cu121\n",
            "    Uninstalling torchaudio-2.1.0+cu121:\n",
            "      Successfully uninstalled torchaudio-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 1.11.0+cu113 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.11.0+cu113 torchaudio-0.11.0+cu113 torchvision-0.12.0+cu113\n",
            "Requirement already satisfied: torch==1.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.11.0+cu113)\n",
            "Requirement already satisfied: torchvision>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.12.0+cu113)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.4.9)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (3.7.1)\n",
            "Collecting configargparse (from -r requirements.txt (line 8))\n",
            "  Downloading ConfigArgParse-1.7-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tensorboard>=2.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (2.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (4.66.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (4.8.0.76)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==1.11.0->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.1->-r requirements.txt (line 4)) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.1->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.9.1->-r requirements.txt (line 4)) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg->-r requirements.txt (line 6)) (67.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (4.46.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (1.60.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (3.5.1)\n",
            "Requirement already satisfied: protobuf<4.24,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (3.20.3)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.0->-r requirements.txt (line 9)) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.0->-r requirements.txt (line 9)) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.0->-r requirements.txt (line 9)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.0->-r requirements.txt (line 9)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.0->-r requirements.txt (line 9)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.1->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.1->-r requirements.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.1->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.9.1->-r requirements.txt (line 4)) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.0->-r requirements.txt (line 9)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.0->-r requirements.txt (line 9)) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.0->-r requirements.txt (line 9)) (3.2.2)\n",
            "Installing collected packages: configargparse\n",
            "Successfully installed configargparse-1.7\n"
          ]
        }
      ],
      "source": [
        "!cd nerf-pytorch/ && pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113 && pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQWQQkJeHciu"
      },
      "outputs": [],
      "source": [
        "# load the existing training log\n",
        "!cd nerf-pytorch && mkdir logs\n",
        "!cp -r /content/drive/MyDrive/HW3_res/logs_blender_paper_bottle_flip/blender_paper_bottle_flip ./nerf-pytorch/logs/blender_paper_bottle_flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqQ7CUNmWVbF"
      },
      "outputs": [],
      "source": [
        "# Reset train val splits on images and poses to 175:25\n",
        "!cd nerf-pytorch/ && python reset_train_val.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wmqn4E0-y6OS",
        "outputId": "0df7655e-8447-48aa-ea42-66ba2f4c99b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading with ./data/nerf_synthetic/bottles\n",
            "(200, 800, 800, 4)\n",
            "Loaded blender (200, 800, 800, 4) (400, 4, 4) torch.Size([40, 4, 4]) [800, 800, 875.0] ./data/nerf_synthetic/bottles\n",
            "[array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
            "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
            "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
            "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
            "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
            "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
            "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
            "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
            "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
            "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
            "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
            "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
            "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
            "       169, 170, 171, 172, 173, 174]), array([175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
            "       188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]), array([200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
            "       213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
            "       226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
            "       239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "       252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n",
            "       265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277,\n",
            "       278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290,\n",
            "       291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303,\n",
            "       304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
            "       317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
            "       330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
            "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
            "       356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368,\n",
            "       369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
            "       382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
            "       395, 396, 397, 398, 399])]\n",
            "Found ckpts ['./logs/blender_paper_bottle_flip/010000.tar', './logs/blender_paper_bottle_flip/020000.tar', './logs/blender_paper_bottle_flip/030000.tar', './logs/blender_paper_bottle_flip/040000.tar', './logs/blender_paper_bottle_flip/050000.tar', './logs/blender_paper_bottle_flip/060000.tar', './logs/blender_paper_bottle_flip/070000.tar', './logs/blender_paper_bottle_flip/080000.tar', './logs/blender_paper_bottle_flip/090000.tar', './logs/blender_paper_bottle_flip/100000.tar', './logs/blender_paper_bottle_flip/110000.tar', './logs/blender_paper_bottle_flip/120000.tar', './logs/blender_paper_bottle_flip/130000.tar', './logs/blender_paper_bottle_flip/140000.tar', './logs/blender_paper_bottle_flip/150000.tar', './logs/blender_paper_bottle_flip/160000.tar', './logs/blender_paper_bottle_flip/170000.tar', './logs/blender_paper_bottle_flip/180000.tar', './logs/blender_paper_bottle_flip/190000.tar', './logs/blender_paper_bottle_flip/200000.tar']\n",
            "Reloading from ./logs/blender_paper_bottle_flip/200000.tar\n",
            "Not ndc!\n",
            "Begin\n",
            "TRAIN views are [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
            "TEST views are [200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
            " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
            " 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289\n",
            " 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307\n",
            " 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325\n",
            " 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343\n",
            " 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397\n",
            " 398 399]\n",
            "VAL views are [175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192\n",
            " 193 194 195 196 197 198 199]\n",
            "  0% 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Saved checkpoints at ./logs/blender_paper_bottle_flip/200000.tar\n",
            "val poses shape torch.Size([25, 4, 4])\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A0 0.00033664703369140625\n",
            "torch.Size([800, 800, 3]) torch.Size([800, 800])\n",
            "\n",
            "  4% 1/25 [00:25<10:06, 25.29s/it]\u001b[A1 25.288264751434326\n",
            "\n",
            "  8% 2/25 [00:50<09:41, 25.28s/it]\u001b[A2 25.26585602760315\n",
            "\n",
            " 12% 3/25 [01:15<09:16, 25.28s/it]\u001b[A3 25.296344757080078\n",
            "\n",
            " 16% 4/25 [01:41<08:51, 25.29s/it]\u001b[A4 25.30081057548523\n",
            "\n",
            " 20% 5/25 [02:06<08:25, 25.29s/it]\u001b[A5 25.29029893875122\n",
            "\n",
            " 24% 6/25 [02:31<08:00, 25.29s/it]\u001b[A6 25.295814275741577\n",
            "\n",
            " 28% 7/25 [02:57<07:35, 25.30s/it]\u001b[A7 25.315484523773193\n",
            "\n",
            " 32% 8/25 [03:22<07:10, 25.30s/it]\u001b[A8 25.30797553062439\n",
            "\n",
            " 36% 9/25 [03:47<06:44, 25.30s/it]\u001b[A9 25.305291175842285\n",
            "\n",
            " 40% 10/25 [04:12<06:19, 25.30s/it]\u001b[A10 25.306934118270874\n",
            "\n",
            " 44% 11/25 [04:38<05:54, 25.31s/it]\u001b[A11 25.325127601623535\n",
            "\n",
            " 48% 12/25 [05:03<05:29, 25.31s/it]\u001b[A12 25.31888484954834\n",
            "\n",
            " 52% 13/25 [05:28<05:03, 25.31s/it]\u001b[A13 25.312501907348633\n",
            "\n",
            " 56% 14/25 [05:54<04:38, 25.31s/it]\u001b[A14 25.317728757858276\n",
            "\n",
            " 60% 15/25 [06:19<04:13, 25.32s/it]\u001b[A15 25.33406114578247\n",
            "\n",
            " 64% 16/25 [06:44<03:47, 25.32s/it]\u001b[A16 25.329335689544678\n",
            "\n",
            " 68% 17/25 [07:10<03:22, 25.33s/it]\u001b[A17 25.330096006393433\n",
            "\n",
            " 72% 18/25 [07:35<02:57, 25.33s/it]\u001b[A18 25.33399724960327\n",
            "\n",
            " 76% 19/25 [08:00<02:32, 25.34s/it]\u001b[A19 25.35387420654297\n",
            "\n",
            " 80% 20/25 [08:26<02:06, 25.33s/it]\u001b[A20 25.33051037788391\n",
            "\n",
            " 84% 21/25 [08:51<01:41, 25.33s/it]\u001b[A21 25.317963123321533\n",
            "\n",
            " 88% 22/25 [09:16<01:15, 25.33s/it]\u001b[A22 25.322161436080933\n",
            "\n",
            " 92% 23/25 [09:42<00:50, 25.33s/it]\u001b[A23 25.33110499382019\n",
            "\n",
            " 96% 24/25 [10:07<00:25, 25.33s/it]\u001b[A24 25.321609497070312\n",
            "\n",
            "100% 25/25 [10:32<00:00, 25.31s/it]\n",
            "(25, 800, 800, 3) (25, 800, 800, 3)\n",
            "0.0009263805113732815 30.33210563659668\n",
            "tensor([30.3321]) torch.Size([1])\n",
            "Validation PSNR = tensor([30.3321]) with validation size 25\n",
            "Saved val set\n",
            "[TRAIN] Iter: 200000 Loss: 0.003989002201706171  PSNR: 30.33210563659668\n",
            "100% 2/2 [10:39<00:00, 319.50s/it]\n"
          ]
        }
      ],
      "source": [
        "# nerf with positional encoding\n",
        "!cd nerf-pytorch/ && python run_nerf.py --config configs/bottles.txt --i_weights 10000 --N_iters 200001 --i_testset 300000 --i_valset 200000 --i_valsize 25 --i_video 300000 --lrate 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX4OZkhz7xuz",
        "outputId": "3fc3ccf3-b650-4e3e-8a6a-41ad265920df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading with ./data/nerf_synthetic/bottles\n",
            "(200, 800, 800, 4)\n",
            "Loaded blender (200, 800, 800, 4) (400, 4, 4) torch.Size([40, 4, 4]) [800, 800, 875.0] ./data/nerf_synthetic/bottles\n",
            "[array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
            "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
            "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
            "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
            "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
            "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
            "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
            "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
            "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
            "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
            "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
            "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
            "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
            "       169, 170, 171, 172, 173, 174]), array([175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
            "       188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199]), array([200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212,\n",
            "       213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225,\n",
            "       226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238,\n",
            "       239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
            "       252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264,\n",
            "       265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277,\n",
            "       278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290,\n",
            "       291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303,\n",
            "       304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316,\n",
            "       317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329,\n",
            "       330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342,\n",
            "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
            "       356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368,\n",
            "       369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
            "       382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
            "       395, 396, 397, 398, 399])]\n",
            "Found ckpts []\n",
            "Not ndc!\n",
            "Begin\n",
            "TRAIN views are [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
            "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
            "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
            "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
            "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
            "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
            " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
            " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
            " 162 163 164 165 166 167 168 169 170 171 172 173 174]\n",
            "TEST views are [200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217\n",
            " 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235\n",
            " 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253\n",
            " 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271\n",
            " 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289\n",
            " 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307\n",
            " 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325\n",
            " 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343\n",
            " 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361\n",
            " 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379\n",
            " 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397\n",
            " 398 399]\n",
            "VAL views are [175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192\n",
            " 193 194 195 196 197 198 199]\n",
            "  0% 0/200000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "[Config] Center cropping of size 400 x 400 is enabled until iter 500\n",
            "[TRAIN] Iter: 100 Loss: 0.25820034742355347  PSNR: 8.875168800354004\n",
            "[TRAIN] Iter: 200 Loss: 0.2559407353401184  PSNR: 8.892404556274414\n",
            "[TRAIN] Iter: 300 Loss: 0.2758968472480774  PSNR: 8.589468955993652\n",
            "[TRAIN] Iter: 400 Loss: 0.23464146256446838  PSNR: 9.251502990722656\n",
            "[TRAIN] Iter: 500 Loss: 0.19668911397457123  PSNR: 8.583820343017578\n",
            "[TRAIN] Iter: 600 Loss: 0.07562302052974701  PSNR: 14.011016845703125\n",
            "[TRAIN] Iter: 700 Loss: 0.06544952094554901  PSNR: 14.59830093383789\n",
            "[TRAIN] Iter: 800 Loss: 0.08011233061552048  PSNR: 13.926152229309082\n",
            "[TRAIN] Iter: 900 Loss: 0.05341357737779617  PSNR: 15.646312713623047\n",
            "[TRAIN] Iter: 1000 Loss: 0.043570324778556824  PSNR: 16.504640579223633\n",
            "[TRAIN] Iter: 1100 Loss: 0.04295269027352333  PSNR: 17.00040626525879\n",
            "[TRAIN] Iter: 1200 Loss: 0.03630339354276657  PSNR: 17.50156593322754\n",
            "[TRAIN] Iter: 1300 Loss: 0.03489357978105545  PSNR: 17.87667465209961\n",
            "[TRAIN] Iter: 1400 Loss: 0.04088851809501648  PSNR: 16.968503952026367\n",
            "[TRAIN] Iter: 1500 Loss: 0.04092423617839813  PSNR: 17.112783432006836\n",
            "[TRAIN] Iter: 1600 Loss: 0.03106122836470604  PSNR: 18.20220375061035\n",
            "[TRAIN] Iter: 1700 Loss: 0.03568575531244278  PSNR: 17.606735229492188\n",
            "[TRAIN] Iter: 1800 Loss: 0.040916986763477325  PSNR: 16.70688247680664\n",
            "[TRAIN] Iter: 1900 Loss: 0.028734419494867325  PSNR: 18.36701011657715\n",
            "[TRAIN] Iter: 2000 Loss: 0.03233332186937332  PSNR: 18.10438346862793\n",
            "[TRAIN] Iter: 2100 Loss: 0.03822052478790283  PSNR: 17.165435791015625\n",
            "[TRAIN] Iter: 2200 Loss: 0.02365632727742195  PSNR: 19.093027114868164\n",
            "[TRAIN] Iter: 2300 Loss: 0.03042626939713955  PSNR: 18.069963455200195\n",
            "[TRAIN] Iter: 2400 Loss: 0.027073021978139877  PSNR: 18.700172424316406\n",
            "[TRAIN] Iter: 2500 Loss: 0.0394611693918705  PSNR: 17.080533981323242\n",
            "[TRAIN] Iter: 2600 Loss: 0.03390680253505707  PSNR: 17.610855102539062\n",
            "[TRAIN] Iter: 2700 Loss: 0.03182881325483322  PSNR: 17.895509719848633\n",
            "[TRAIN] Iter: 2800 Loss: 0.029660992324352264  PSNR: 18.286678314208984\n",
            "[TRAIN] Iter: 2900 Loss: 0.025954976677894592  PSNR: 18.71518898010254\n",
            "[TRAIN] Iter: 3000 Loss: 0.024343237280845642  PSNR: 19.100893020629883\n",
            "[TRAIN] Iter: 3100 Loss: 0.034208349883556366  PSNR: 17.634201049804688\n",
            "[TRAIN] Iter: 3200 Loss: 0.036205701529979706  PSNR: 17.269121170043945\n",
            "[TRAIN] Iter: 3300 Loss: 0.02814543806016445  PSNR: 18.38258171081543\n",
            "[TRAIN] Iter: 3400 Loss: 0.019147630780935287  PSNR: 19.9542236328125\n",
            "[TRAIN] Iter: 3500 Loss: 0.023199496790766716  PSNR: 19.47671890258789\n",
            "[TRAIN] Iter: 3600 Loss: 0.025921659544110298  PSNR: 18.826993942260742\n",
            "[TRAIN] Iter: 3700 Loss: 0.024114331230521202  PSNR: 19.152875900268555\n",
            "[TRAIN] Iter: 3800 Loss: 0.022504612803459167  PSNR: 19.46281623840332\n",
            "[TRAIN] Iter: 3900 Loss: 0.0211686659604311  PSNR: 19.56919288635254\n",
            "[TRAIN] Iter: 4000 Loss: 0.029198072850704193  PSNR: 18.244831085205078\n",
            "[TRAIN] Iter: 4100 Loss: 0.01831253618001938  PSNR: 20.50259780883789\n",
            "[TRAIN] Iter: 4200 Loss: 0.026248585432767868  PSNR: 18.737083435058594\n",
            "[TRAIN] Iter: 4300 Loss: 0.02738180011510849  PSNR: 18.449913024902344\n",
            "[TRAIN] Iter: 4400 Loss: 0.024989448487758636  PSNR: 18.987380981445312\n",
            "[TRAIN] Iter: 4500 Loss: 0.024107586592435837  PSNR: 19.12046241760254\n",
            "[TRAIN] Iter: 4600 Loss: 0.02230609580874443  PSNR: 19.38686180114746\n",
            "[TRAIN] Iter: 4700 Loss: 0.026166554540395737  PSNR: 19.06844139099121\n",
            "[TRAIN] Iter: 4800 Loss: 0.030352897942066193  PSNR: 18.340045928955078\n",
            "[TRAIN] Iter: 4900 Loss: 0.025279050692915916  PSNR: 18.816553115844727\n",
            "[TRAIN] Iter: 5000 Loss: 0.021089516580104828  PSNR: 19.927030563354492\n",
            "[TRAIN] Iter: 5100 Loss: 0.02523191273212433  PSNR: 19.153966903686523\n",
            "[TRAIN] Iter: 5200 Loss: 0.02323063462972641  PSNR: 19.398408889770508\n",
            "[TRAIN] Iter: 5300 Loss: 0.017703307792544365  PSNR: 20.710477828979492\n",
            "[TRAIN] Iter: 5400 Loss: 0.03454890474677086  PSNR: 17.523170471191406\n",
            "[TRAIN] Iter: 5500 Loss: 0.018828805536031723  PSNR: 20.350650787353516\n",
            "[TRAIN] Iter: 5600 Loss: 0.026353253051638603  PSNR: 18.798938751220703\n",
            "[TRAIN] Iter: 5700 Loss: 0.025523468852043152  PSNR: 18.890321731567383\n",
            "[TRAIN] Iter: 5800 Loss: 0.019783448427915573  PSNR: 20.283679962158203\n",
            "[TRAIN] Iter: 5900 Loss: 0.026065334677696228  PSNR: 19.030963897705078\n",
            "[TRAIN] Iter: 6000 Loss: 0.031515154987573624  PSNR: 18.101770401000977\n",
            "[TRAIN] Iter: 6100 Loss: 0.02057412639260292  PSNR: 20.01761817932129\n",
            "[TRAIN] Iter: 6200 Loss: 0.022955231368541718  PSNR: 19.4193172454834\n",
            "[TRAIN] Iter: 6300 Loss: 0.01892750710248947  PSNR: 20.426799774169922\n",
            "[TRAIN] Iter: 6400 Loss: 0.020659327507019043  PSNR: 19.911270141601562\n",
            "[TRAIN] Iter: 6500 Loss: 0.02508590929210186  PSNR: 19.117813110351562\n",
            "[TRAIN] Iter: 6600 Loss: 0.018482431769371033  PSNR: 20.615774154663086\n",
            "[TRAIN] Iter: 6700 Loss: 0.025660954415798187  PSNR: 18.952621459960938\n",
            "[TRAIN] Iter: 6800 Loss: 0.021256403997540474  PSNR: 19.863819122314453\n",
            "[TRAIN] Iter: 6900 Loss: 0.0186031311750412  PSNR: 20.639450073242188\n",
            "[TRAIN] Iter: 7000 Loss: 0.02215958945453167  PSNR: 19.746299743652344\n",
            "[TRAIN] Iter: 7100 Loss: 0.019612852483987808  PSNR: 20.274656295776367\n",
            "[TRAIN] Iter: 7200 Loss: 0.017161034047603607  PSNR: 20.89501953125\n",
            "[TRAIN] Iter: 7300 Loss: 0.02041884884238243  PSNR: 20.046510696411133\n",
            "[TRAIN] Iter: 7400 Loss: 0.03641404211521149  PSNR: 17.46562385559082\n",
            "[TRAIN] Iter: 7500 Loss: 0.025460604578256607  PSNR: 18.778867721557617\n",
            "[TRAIN] Iter: 7600 Loss: 0.01700122281908989  PSNR: 20.956438064575195\n",
            "[TRAIN] Iter: 7700 Loss: 0.020491179078817368  PSNR: 20.113903045654297\n",
            "[TRAIN] Iter: 7800 Loss: 0.023467296734452248  PSNR: 19.60235595703125\n",
            "[TRAIN] Iter: 7900 Loss: 0.026271924376487732  PSNR: 18.716978073120117\n",
            "[TRAIN] Iter: 8000 Loss: 0.017590805888175964  PSNR: 20.485523223876953\n",
            "[TRAIN] Iter: 8100 Loss: 0.021935822442173958  PSNR: 19.72212791442871\n",
            "[TRAIN] Iter: 8200 Loss: 0.025860577821731567  PSNR: 18.97233772277832\n",
            "[TRAIN] Iter: 8300 Loss: 0.018910206854343414  PSNR: 20.417360305786133\n",
            "[TRAIN] Iter: 8400 Loss: 0.02068350464105606  PSNR: 20.16175079345703\n",
            "[TRAIN] Iter: 8500 Loss: 0.020452462136745453  PSNR: 19.926807403564453\n",
            "[TRAIN] Iter: 8600 Loss: 0.016677888110280037  PSNR: 20.933292388916016\n",
            "[TRAIN] Iter: 8700 Loss: 0.015900321304798126  PSNR: 21.149192810058594\n",
            "[TRAIN] Iter: 8800 Loss: 0.018754255026578903  PSNR: 20.517301559448242\n",
            "[TRAIN] Iter: 8900 Loss: 0.018687903881072998  PSNR: 20.228952407836914\n",
            "[TRAIN] Iter: 9000 Loss: 0.017056498676538467  PSNR: 20.931467056274414\n",
            "[TRAIN] Iter: 9100 Loss: 0.02342025376856327  PSNR: 19.46616554260254\n",
            "[TRAIN] Iter: 9200 Loss: 0.01608511433005333  PSNR: 20.945680618286133\n",
            "[TRAIN] Iter: 9300 Loss: 0.016743429005146027  PSNR: 20.753009796142578\n",
            "[TRAIN] Iter: 9400 Loss: 0.02048494666814804  PSNR: 19.924449920654297\n",
            "[TRAIN] Iter: 9500 Loss: 0.02151021920144558  PSNR: 19.640304565429688\n",
            "[TRAIN] Iter: 9600 Loss: 0.029090750962495804  PSNR: 18.476383209228516\n",
            "[TRAIN] Iter: 9700 Loss: 0.016914524137973785  PSNR: 20.71312141418457\n",
            "[TRAIN] Iter: 9800 Loss: 0.020046070218086243  PSNR: 20.224275588989258\n",
            "[TRAIN] Iter: 9900 Loss: 0.016631299629807472  PSNR: 20.932889938354492\n",
            "  5% 9999/200000 [19:34<6:08:16,  8.60it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/010000.tar\n",
            "[TRAIN] Iter: 10000 Loss: 0.016450680792331696  PSNR: 20.912433624267578\n",
            "[TRAIN] Iter: 10100 Loss: 0.020487159490585327  PSNR: 19.898054122924805\n",
            "[TRAIN] Iter: 10200 Loss: 0.024923568591475487  PSNR: 18.94032096862793\n",
            "[TRAIN] Iter: 10300 Loss: 0.014890884049236774  PSNR: 20.99493980407715\n",
            "[TRAIN] Iter: 10400 Loss: 0.025775054469704628  PSNR: 18.927988052368164\n",
            "[TRAIN] Iter: 10500 Loss: 0.022592797875404358  PSNR: 19.521326065063477\n",
            "[TRAIN] Iter: 10600 Loss: 0.012278609909117222  PSNR: 22.200422286987305\n",
            "[TRAIN] Iter: 10700 Loss: 0.024666693061590195  PSNR: 18.880407333374023\n",
            "[TRAIN] Iter: 10800 Loss: 0.02553674206137657  PSNR: 18.932998657226562\n",
            "[TRAIN] Iter: 10900 Loss: 0.011150473728775978  PSNR: 22.597753524780273\n",
            "[TRAIN] Iter: 11000 Loss: 0.017354518175125122  PSNR: 20.830326080322266\n",
            "[TRAIN] Iter: 11100 Loss: 0.023501891642808914  PSNR: 19.064043045043945\n",
            "[TRAIN] Iter: 11200 Loss: 0.021353531628847122  PSNR: 19.757835388183594\n",
            "[TRAIN] Iter: 11300 Loss: 0.027532486245036125  PSNR: 18.147254943847656\n",
            "[TRAIN] Iter: 11400 Loss: 0.013432605192065239  PSNR: 21.902618408203125\n",
            "[TRAIN] Iter: 11500 Loss: 0.018558965995907784  PSNR: 20.28354263305664\n",
            "[TRAIN] Iter: 11600 Loss: 0.02044438198208809  PSNR: 19.829652786254883\n",
            "[TRAIN] Iter: 11700 Loss: 0.01769491657614708  PSNR: 20.695938110351562\n",
            "[TRAIN] Iter: 11800 Loss: 0.01752777211368084  PSNR: 20.543054580688477\n",
            "[TRAIN] Iter: 11900 Loss: 0.010101886466145515  PSNR: 23.01721954345703\n",
            "[TRAIN] Iter: 12000 Loss: 0.014206118881702423  PSNR: 21.425586700439453\n",
            "[TRAIN] Iter: 12100 Loss: 0.01729465276002884  PSNR: 20.625370025634766\n",
            "[TRAIN] Iter: 12200 Loss: 0.015919405966997147  PSNR: 21.110239028930664\n",
            "[TRAIN] Iter: 12300 Loss: 0.014076732099056244  PSNR: 21.669361114501953\n",
            "[TRAIN] Iter: 12400 Loss: 0.027141988277435303  PSNR: 18.747682571411133\n",
            "[TRAIN] Iter: 12500 Loss: 0.015352411195635796  PSNR: 21.274456024169922\n",
            "[TRAIN] Iter: 12600 Loss: 0.019540004432201385  PSNR: 19.96260643005371\n",
            "[TRAIN] Iter: 12700 Loss: 0.026973504573106766  PSNR: 18.613710403442383\n",
            "[TRAIN] Iter: 12800 Loss: 0.03225509822368622  PSNR: 18.013744354248047\n",
            "[TRAIN] Iter: 12900 Loss: 0.025021567940711975  PSNR: 19.071945190429688\n",
            "[TRAIN] Iter: 13000 Loss: 0.020835673436522484  PSNR: 19.840129852294922\n",
            "[TRAIN] Iter: 13100 Loss: 0.021320298314094543  PSNR: 19.71113395690918\n",
            "[TRAIN] Iter: 13200 Loss: 0.0172277744859457  PSNR: 20.728363037109375\n",
            "[TRAIN] Iter: 13300 Loss: 0.027794605121016502  PSNR: 18.75577735900879\n",
            "[TRAIN] Iter: 13400 Loss: 0.013637959025800228  PSNR: 21.682022094726562\n",
            "[TRAIN] Iter: 13500 Loss: 0.01449856162071228  PSNR: 21.466588973999023\n",
            "[TRAIN] Iter: 13600 Loss: 0.021761974319815636  PSNR: 19.68038558959961\n",
            "[TRAIN] Iter: 13700 Loss: 0.014206533320248127  PSNR: 21.617996215820312\n",
            "[TRAIN] Iter: 13800 Loss: 0.01805119402706623  PSNR: 20.51677894592285\n",
            "[TRAIN] Iter: 13900 Loss: 0.019733253866434097  PSNR: 20.0899658203125\n",
            "[TRAIN] Iter: 14000 Loss: 0.016641005873680115  PSNR: 21.045913696289062\n",
            "[TRAIN] Iter: 14100 Loss: 0.018853455781936646  PSNR: 20.23496437072754\n",
            "[TRAIN] Iter: 14200 Loss: 0.025198958814144135  PSNR: 18.974607467651367\n",
            "[TRAIN] Iter: 14300 Loss: 0.018398284912109375  PSNR: 20.199033737182617\n",
            "[TRAIN] Iter: 14400 Loss: 0.012389637529850006  PSNR: 22.157297134399414\n",
            "[TRAIN] Iter: 14500 Loss: 0.016821198165416718  PSNR: 20.738489151000977\n",
            "[TRAIN] Iter: 14600 Loss: 0.017816035076975822  PSNR: 20.3807315826416\n",
            "[TRAIN] Iter: 14700 Loss: 0.031068895012140274  PSNR: 18.170515060424805\n",
            "[TRAIN] Iter: 14800 Loss: 0.016151584684848785  PSNR: 21.007169723510742\n",
            "[TRAIN] Iter: 14900 Loss: 0.0274960920214653  PSNR: 18.712478637695312\n",
            "[TRAIN] Iter: 15000 Loss: 0.01529641542583704  PSNR: 21.138446807861328\n",
            "[TRAIN] Iter: 15100 Loss: 0.018940437585115433  PSNR: 20.35475730895996\n",
            "[TRAIN] Iter: 15200 Loss: 0.018218884244561195  PSNR: 20.701412200927734\n",
            "[TRAIN] Iter: 15300 Loss: 0.013979337178170681  PSNR: 21.60976219177246\n",
            "[TRAIN] Iter: 15400 Loss: 0.014678833074867725  PSNR: 21.510635375976562\n",
            "[TRAIN] Iter: 15500 Loss: 0.013502592220902443  PSNR: 21.88505744934082\n",
            "[TRAIN] Iter: 15600 Loss: 0.013995926827192307  PSNR: 21.658536911010742\n",
            "[TRAIN] Iter: 15700 Loss: 0.017527764663100243  PSNR: 20.561254501342773\n",
            "[TRAIN] Iter: 15800 Loss: 0.013470439240336418  PSNR: 21.853971481323242\n",
            "[TRAIN] Iter: 15900 Loss: 0.016889197751879692  PSNR: 21.171104431152344\n",
            "[TRAIN] Iter: 16000 Loss: 0.015339312143623829  PSNR: 21.477367401123047\n",
            "[TRAIN] Iter: 16100 Loss: 0.016408860683441162  PSNR: 20.807300567626953\n",
            "[TRAIN] Iter: 16200 Loss: 0.01998736709356308  PSNR: 20.30836296081543\n",
            "[TRAIN] Iter: 16300 Loss: 0.0179063118994236  PSNR: 20.721458435058594\n",
            "[TRAIN] Iter: 16400 Loss: 0.01229659654200077  PSNR: 22.310762405395508\n",
            "[TRAIN] Iter: 16500 Loss: 0.014256124384701252  PSNR: 21.594879150390625\n",
            "[TRAIN] Iter: 16600 Loss: 0.019050832837820053  PSNR: 20.28961181640625\n",
            "[TRAIN] Iter: 16700 Loss: 0.0220289696007967  PSNR: 19.43142318725586\n",
            "[TRAIN] Iter: 16800 Loss: 0.011135422624647617  PSNR: 22.66944694519043\n",
            "[TRAIN] Iter: 16900 Loss: 0.013543486595153809  PSNR: 21.860925674438477\n",
            "[TRAIN] Iter: 17000 Loss: 0.019129550084471703  PSNR: 20.020315170288086\n",
            "[TRAIN] Iter: 17100 Loss: 0.029097389429807663  PSNR: 18.2533016204834\n",
            "[TRAIN] Iter: 17200 Loss: 0.019483352079987526  PSNR: 20.32339096069336\n",
            "[TRAIN] Iter: 17300 Loss: 0.01874527335166931  PSNR: 20.48209571838379\n",
            "[TRAIN] Iter: 17400 Loss: 0.015474324114620686  PSNR: 21.27788543701172\n",
            "[TRAIN] Iter: 17500 Loss: 0.022777386009693146  PSNR: 19.574665069580078\n",
            "[TRAIN] Iter: 17600 Loss: 0.014690535143017769  PSNR: 21.50404930114746\n",
            "[TRAIN] Iter: 17700 Loss: 0.014196743257343769  PSNR: 21.538679122924805\n",
            "[TRAIN] Iter: 17800 Loss: 0.014050932601094246  PSNR: 21.770057678222656\n",
            "[TRAIN] Iter: 17900 Loss: 0.01582985371351242  PSNR: 21.499492645263672\n",
            "[TRAIN] Iter: 18000 Loss: 0.027750812470912933  PSNR: 18.682514190673828\n",
            "[TRAIN] Iter: 18100 Loss: 0.015650879591703415  PSNR: 21.318208694458008\n",
            "[TRAIN] Iter: 18200 Loss: 0.01778549700975418  PSNR: 20.85716438293457\n",
            "[TRAIN] Iter: 18300 Loss: 0.017719080671668053  PSNR: 20.614604949951172\n",
            "[TRAIN] Iter: 18400 Loss: 0.019067252054810524  PSNR: 20.47859001159668\n",
            "[TRAIN] Iter: 18500 Loss: 0.01850508525967598  PSNR: 20.803775787353516\n",
            "[TRAIN] Iter: 18600 Loss: 0.027953701093792915  PSNR: 18.558902740478516\n",
            "[TRAIN] Iter: 18700 Loss: 0.019141338765621185  PSNR: 20.521514892578125\n",
            "[TRAIN] Iter: 18800 Loss: 0.01825738698244095  PSNR: 20.880144119262695\n",
            "[TRAIN] Iter: 18900 Loss: 0.022774742916226387  PSNR: 19.47186279296875\n",
            "[TRAIN] Iter: 19000 Loss: 0.0188930481672287  PSNR: 20.3323974609375\n",
            "[TRAIN] Iter: 19100 Loss: 0.020598433911800385  PSNR: 20.11676788330078\n",
            "[TRAIN] Iter: 19200 Loss: 0.022173898294568062  PSNR: 19.604230880737305\n",
            "[TRAIN] Iter: 19300 Loss: 0.029797177761793137  PSNR: 18.293169021606445\n",
            "[TRAIN] Iter: 19400 Loss: 0.013936815783381462  PSNR: 21.949182510375977\n",
            "[TRAIN] Iter: 19500 Loss: 0.015117836184799671  PSNR: 21.21124839782715\n",
            "[TRAIN] Iter: 19600 Loss: 0.014594513922929764  PSNR: 21.497554779052734\n",
            "[TRAIN] Iter: 19700 Loss: 0.0181182399392128  PSNR: 20.60083770751953\n",
            "[TRAIN] Iter: 19800 Loss: 0.015025043860077858  PSNR: 21.344249725341797\n",
            "[TRAIN] Iter: 19900 Loss: 0.016774475574493408  PSNR: 20.85677719116211\n",
            " 10% 19999/200000 [39:13<5:51:42,  8.53it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/020000.tar\n",
            "[TRAIN] Iter: 20000 Loss: 0.017994357272982597  PSNR: 20.97220802307129\n",
            "[TRAIN] Iter: 20100 Loss: 0.020462941378355026  PSNR: 20.00548553466797\n",
            "[TRAIN] Iter: 20200 Loss: 0.017152361571788788  PSNR: 20.548255920410156\n",
            "[TRAIN] Iter: 20300 Loss: 0.015630967915058136  PSNR: 21.1052303314209\n",
            "[TRAIN] Iter: 20400 Loss: 0.014441127888858318  PSNR: 21.782564163208008\n",
            "[TRAIN] Iter: 20500 Loss: 0.016677621752023697  PSNR: 20.750940322875977\n",
            "[TRAIN] Iter: 20600 Loss: 0.0217682383954525  PSNR: 19.806171417236328\n",
            "[TRAIN] Iter: 20700 Loss: 0.01064981147646904  PSNR: 22.89406394958496\n",
            "[TRAIN] Iter: 20800 Loss: 0.011401347815990448  PSNR: 22.516103744506836\n",
            "[TRAIN] Iter: 20900 Loss: 0.015076367184519768  PSNR: 21.43018341064453\n",
            "[TRAIN] Iter: 21000 Loss: 0.016951605677604675  PSNR: 20.613140106201172\n",
            "[TRAIN] Iter: 21100 Loss: 0.01302566472440958  PSNR: 22.19083595275879\n",
            "[TRAIN] Iter: 21200 Loss: 0.017896628007292747  PSNR: 20.619089126586914\n",
            "[TRAIN] Iter: 21300 Loss: 0.009457592852413654  PSNR: 23.68842315673828\n",
            "[TRAIN] Iter: 21400 Loss: 0.0110003761947155  PSNR: 22.81414031982422\n",
            "[TRAIN] Iter: 21500 Loss: 0.011572632938623428  PSNR: 22.50107765197754\n",
            "[TRAIN] Iter: 21600 Loss: 0.018263578414916992  PSNR: 20.475297927856445\n",
            "[TRAIN] Iter: 21700 Loss: 0.021350041031837463  PSNR: 19.85000228881836\n",
            "[TRAIN] Iter: 21800 Loss: 0.015316632576286793  PSNR: 21.474756240844727\n",
            "[TRAIN] Iter: 21900 Loss: 0.019848516210913658  PSNR: 20.187870025634766\n",
            "[TRAIN] Iter: 22000 Loss: 0.015466510318219662  PSNR: 21.237192153930664\n",
            "[TRAIN] Iter: 22100 Loss: 0.014775346964597702  PSNR: 21.70509910583496\n",
            "[TRAIN] Iter: 22200 Loss: 0.015500957146286964  PSNR: 21.10440444946289\n",
            "[TRAIN] Iter: 22300 Loss: 0.01703830063343048  PSNR: 20.81383514404297\n",
            "[TRAIN] Iter: 22400 Loss: 0.021489111706614494  PSNR: 19.831241607666016\n",
            "[TRAIN] Iter: 22500 Loss: 0.0203078780323267  PSNR: 19.862964630126953\n",
            "[TRAIN] Iter: 22600 Loss: 0.017749037593603134  PSNR: 20.805675506591797\n",
            "[TRAIN] Iter: 22700 Loss: 0.0183353703469038  PSNR: 20.6025447845459\n",
            "[TRAIN] Iter: 22800 Loss: 0.02301754057407379  PSNR: 19.701045989990234\n",
            "[TRAIN] Iter: 22900 Loss: 0.018792565912008286  PSNR: 20.41828727722168\n",
            "[TRAIN] Iter: 23000 Loss: 0.017920885235071182  PSNR: 20.684463500976562\n",
            "[TRAIN] Iter: 23100 Loss: 0.016045501455664635  PSNR: 21.345239639282227\n",
            "[TRAIN] Iter: 23200 Loss: 0.021947823464870453  PSNR: 19.765077590942383\n",
            "[TRAIN] Iter: 23300 Loss: 0.013383360579609871  PSNR: 21.836238861083984\n",
            "[TRAIN] Iter: 23400 Loss: 0.0170615091919899  PSNR: 20.490636825561523\n",
            "[TRAIN] Iter: 23500 Loss: 0.02051781676709652  PSNR: 20.171228408813477\n",
            "[TRAIN] Iter: 23600 Loss: 0.015806952491402626  PSNR: 21.069377899169922\n",
            "[TRAIN] Iter: 23700 Loss: 0.02420048415660858  PSNR: 19.156890869140625\n",
            "[TRAIN] Iter: 23800 Loss: 0.01572549156844616  PSNR: 21.349103927612305\n",
            "[TRAIN] Iter: 23900 Loss: 0.026554565876722336  PSNR: 18.94717788696289\n",
            "[TRAIN] Iter: 24000 Loss: 0.007763960398733616  PSNR: 24.206520080566406\n",
            "[TRAIN] Iter: 24100 Loss: 0.01969757303595543  PSNR: 20.064647674560547\n",
            "[TRAIN] Iter: 24200 Loss: 0.023057829588651657  PSNR: 19.88759422302246\n",
            "[TRAIN] Iter: 24300 Loss: 0.016922440379858017  PSNR: 20.925750732421875\n",
            "[TRAIN] Iter: 24400 Loss: 0.014703242108225822  PSNR: 22.05121612548828\n",
            "[TRAIN] Iter: 24500 Loss: 0.01654510572552681  PSNR: 21.230022430419922\n",
            "[TRAIN] Iter: 24600 Loss: 0.017985910177230835  PSNR: 20.413801193237305\n",
            "[TRAIN] Iter: 24700 Loss: 0.01836809329688549  PSNR: 20.401851654052734\n",
            "[TRAIN] Iter: 24800 Loss: 0.011887484230101109  PSNR: 22.6805419921875\n",
            "[TRAIN] Iter: 24900 Loss: 0.02301671914756298  PSNR: 19.45364761352539\n",
            "[TRAIN] Iter: 25000 Loss: 0.01592075079679489  PSNR: 21.042625427246094\n",
            "[TRAIN] Iter: 25100 Loss: 0.01671108789741993  PSNR: 20.917055130004883\n",
            "[TRAIN] Iter: 25200 Loss: 0.021685725077986717  PSNR: 19.573036193847656\n",
            "[TRAIN] Iter: 25300 Loss: 0.011520413681864738  PSNR: 22.315099716186523\n",
            "[TRAIN] Iter: 25400 Loss: 0.020451493561267853  PSNR: 19.895740509033203\n",
            "[TRAIN] Iter: 25500 Loss: 0.012314317747950554  PSNR: 22.150449752807617\n",
            "[TRAIN] Iter: 25600 Loss: 0.01711449958384037  PSNR: 20.771196365356445\n",
            "[TRAIN] Iter: 25700 Loss: 0.011493690311908722  PSNR: 22.441499710083008\n",
            "[TRAIN] Iter: 25800 Loss: 0.016652768477797508  PSNR: 21.12782096862793\n",
            "[TRAIN] Iter: 25900 Loss: 0.013766847550868988  PSNR: 21.770893096923828\n",
            "[TRAIN] Iter: 26000 Loss: 0.013667813502252102  PSNR: 21.891080856323242\n",
            "[TRAIN] Iter: 26100 Loss: 0.011509284377098083  PSNR: 22.543025970458984\n",
            "[TRAIN] Iter: 26200 Loss: 0.01095452718436718  PSNR: 22.843852996826172\n",
            "[TRAIN] Iter: 26300 Loss: 0.01944543421268463  PSNR: 19.862321853637695\n",
            "[TRAIN] Iter: 26400 Loss: 0.01939999870955944  PSNR: 20.19658851623535\n",
            "[TRAIN] Iter: 26500 Loss: 0.01448013074696064  PSNR: 21.527982711791992\n",
            "[TRAIN] Iter: 26600 Loss: 0.01274469681084156  PSNR: 21.964513778686523\n",
            "[TRAIN] Iter: 26700 Loss: 0.01200142689049244  PSNR: 22.379610061645508\n",
            "[TRAIN] Iter: 26800 Loss: 0.01208187174052  PSNR: 22.162487030029297\n",
            "[TRAIN] Iter: 26900 Loss: 0.014758409932255745  PSNR: 21.546899795532227\n",
            "[TRAIN] Iter: 27000 Loss: 0.017727341502904892  PSNR: 20.68212890625\n",
            "[TRAIN] Iter: 27100 Loss: 0.02047358825802803  PSNR: 19.918222427368164\n",
            "[TRAIN] Iter: 27200 Loss: 0.023209959268569946  PSNR: 19.529483795166016\n",
            "[TRAIN] Iter: 27300 Loss: 0.013861356303095818  PSNR: 22.025020599365234\n",
            "[TRAIN] Iter: 27400 Loss: 0.01387243252247572  PSNR: 21.715747833251953\n",
            "[TRAIN] Iter: 27500 Loss: 0.018536841496825218  PSNR: 20.555696487426758\n",
            "[TRAIN] Iter: 27600 Loss: 0.014509771019220352  PSNR: 21.767297744750977\n",
            "[TRAIN] Iter: 27700 Loss: 0.01446271687746048  PSNR: 21.36849594116211\n",
            "[TRAIN] Iter: 27800 Loss: 0.019435331225395203  PSNR: 20.052148818969727\n",
            "[TRAIN] Iter: 27900 Loss: 0.020729146897792816  PSNR: 20.050012588500977\n",
            "[TRAIN] Iter: 28000 Loss: 0.016770070418715477  PSNR: 20.983291625976562\n",
            "[TRAIN] Iter: 28100 Loss: 0.011451136320829391  PSNR: 22.681142807006836\n",
            "[TRAIN] Iter: 28200 Loss: 0.01652596890926361  PSNR: 21.156047821044922\n",
            "[TRAIN] Iter: 28300 Loss: 0.009721150621771812  PSNR: 23.313852310180664\n",
            "[TRAIN] Iter: 28400 Loss: 0.020976664498448372  PSNR: 19.999401092529297\n",
            "[TRAIN] Iter: 28500 Loss: 0.016155149787664413  PSNR: 21.123966217041016\n",
            "[TRAIN] Iter: 28600 Loss: 0.010474810376763344  PSNR: 23.275386810302734\n",
            "[TRAIN] Iter: 28700 Loss: 0.01712755300104618  PSNR: 20.663923263549805\n",
            "[TRAIN] Iter: 28800 Loss: 0.011536702513694763  PSNR: 22.716537475585938\n",
            "[TRAIN] Iter: 28900 Loss: 0.022047938778996468  PSNR: 19.47592544555664\n",
            "[TRAIN] Iter: 29000 Loss: 0.019515030086040497  PSNR: 20.39606475830078\n",
            "[TRAIN] Iter: 29100 Loss: 0.01284576766192913  PSNR: 22.372692108154297\n",
            "[TRAIN] Iter: 29200 Loss: 0.013955028727650642  PSNR: 21.54292106628418\n",
            "[TRAIN] Iter: 29300 Loss: 0.011553380638360977  PSNR: 22.60685157775879\n",
            "[TRAIN] Iter: 29400 Loss: 0.012883365154266357  PSNR: 21.95961570739746\n",
            "[TRAIN] Iter: 29500 Loss: 0.025108762085437775  PSNR: 19.13631820678711\n",
            "[TRAIN] Iter: 29600 Loss: 0.019568277522921562  PSNR: 20.348142623901367\n",
            "[TRAIN] Iter: 29700 Loss: 0.021852781996130943  PSNR: 19.724546432495117\n",
            "[TRAIN] Iter: 29800 Loss: 0.011360790580511093  PSNR: 22.546049118041992\n",
            "[TRAIN] Iter: 29900 Loss: 0.014305292628705502  PSNR: 21.618520736694336\n",
            " 15% 29999/200000 [58:51<5:29:38,  8.60it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/030000.tar\n",
            "[TRAIN] Iter: 30000 Loss: 0.014325164258480072  PSNR: 21.38578987121582\n",
            "[TRAIN] Iter: 30100 Loss: 0.01583682745695114  PSNR: 21.035219192504883\n",
            "[TRAIN] Iter: 30200 Loss: 0.013439248315989971  PSNR: 22.1135311126709\n",
            "[TRAIN] Iter: 30300 Loss: 0.020893584936857224  PSNR: 20.44491958618164\n",
            "[TRAIN] Iter: 30400 Loss: 0.01791226491332054  PSNR: 20.591266632080078\n",
            "[TRAIN] Iter: 30500 Loss: 0.01797713339328766  PSNR: 20.653053283691406\n",
            "[TRAIN] Iter: 30600 Loss: 0.010756680741906166  PSNR: 22.85544204711914\n",
            "[TRAIN] Iter: 30700 Loss: 0.01708994060754776  PSNR: 20.903430938720703\n",
            "[TRAIN] Iter: 30800 Loss: 0.021068714559078217  PSNR: 19.874588012695312\n",
            "[TRAIN] Iter: 30900 Loss: 0.01838737353682518  PSNR: 20.510251998901367\n",
            "[TRAIN] Iter: 31000 Loss: 0.016577495262026787  PSNR: 20.69877815246582\n",
            "[TRAIN] Iter: 31100 Loss: 0.022366434335708618  PSNR: 19.75078582763672\n",
            "[TRAIN] Iter: 31200 Loss: 0.01104510948061943  PSNR: 22.620372772216797\n",
            "[TRAIN] Iter: 31300 Loss: 0.01304866373538971  PSNR: 22.399606704711914\n",
            "[TRAIN] Iter: 31400 Loss: 0.01016682293266058  PSNR: 23.268962860107422\n",
            "[TRAIN] Iter: 31500 Loss: 0.02305394969880581  PSNR: 19.572654724121094\n",
            "[TRAIN] Iter: 31600 Loss: 0.012248818762600422  PSNR: 22.18625259399414\n",
            "[TRAIN] Iter: 31700 Loss: 0.014409059658646584  PSNR: 21.579702377319336\n",
            "[TRAIN] Iter: 31800 Loss: 0.016961075365543365  PSNR: 20.625516891479492\n",
            "[TRAIN] Iter: 31900 Loss: 0.02172708511352539  PSNR: 19.524120330810547\n",
            "[TRAIN] Iter: 32000 Loss: 0.018153294920921326  PSNR: 20.663719177246094\n",
            "[TRAIN] Iter: 32100 Loss: 0.011365115642547607  PSNR: 22.792133331298828\n",
            "[TRAIN] Iter: 32200 Loss: 0.010847058147192001  PSNR: 22.88089942932129\n",
            "[TRAIN] Iter: 32300 Loss: 0.01239714864641428  PSNR: 22.27570152282715\n",
            "[TRAIN] Iter: 32400 Loss: 0.018550574779510498  PSNR: 20.342775344848633\n",
            "[TRAIN] Iter: 32500 Loss: 0.011704841628670692  PSNR: 22.81119155883789\n",
            "[TRAIN] Iter: 32600 Loss: 0.014812729321420193  PSNR: 21.28264617919922\n",
            "[TRAIN] Iter: 32700 Loss: 0.02051771990954876  PSNR: 20.105918884277344\n",
            "[TRAIN] Iter: 32800 Loss: 0.009244147688150406  PSNR: 23.811321258544922\n",
            "[TRAIN] Iter: 32900 Loss: 0.013065638951957226  PSNR: 21.989547729492188\n",
            "[TRAIN] Iter: 33000 Loss: 0.013627677224576473  PSNR: 21.827957153320312\n",
            "[TRAIN] Iter: 33100 Loss: 0.018360888585448265  PSNR: 20.71626853942871\n",
            "[TRAIN] Iter: 33200 Loss: 0.021056270226836205  PSNR: 19.945430755615234\n",
            "[TRAIN] Iter: 33300 Loss: 0.012704860419034958  PSNR: 22.299470901489258\n",
            "[TRAIN] Iter: 33400 Loss: 0.01673227734863758  PSNR: 20.945043563842773\n",
            "[TRAIN] Iter: 33500 Loss: 0.015319150872528553  PSNR: 21.356040954589844\n",
            "[TRAIN] Iter: 33600 Loss: 0.009699158370494843  PSNR: 23.263559341430664\n",
            "[TRAIN] Iter: 33700 Loss: 0.01406838372349739  PSNR: 21.796958923339844\n",
            "[TRAIN] Iter: 33800 Loss: 0.015566700138151646  PSNR: 21.429752349853516\n",
            "[TRAIN] Iter: 33900 Loss: 0.02293456345796585  PSNR: 19.504348754882812\n",
            "[TRAIN] Iter: 34000 Loss: 0.017304515466094017  PSNR: 20.919044494628906\n",
            "[TRAIN] Iter: 34100 Loss: 0.01642885059118271  PSNR: 21.554607391357422\n",
            "[TRAIN] Iter: 34200 Loss: 0.0181463323533535  PSNR: 20.475040435791016\n",
            "[TRAIN] Iter: 34300 Loss: 0.018032699823379517  PSNR: 20.759326934814453\n",
            "[TRAIN] Iter: 34400 Loss: 0.007766153663396835  PSNR: 24.654693603515625\n",
            "[TRAIN] Iter: 34500 Loss: 0.014070792123675346  PSNR: 21.772571563720703\n",
            "[TRAIN] Iter: 34600 Loss: 0.014270498417317867  PSNR: 21.867843627929688\n",
            "[TRAIN] Iter: 34700 Loss: 0.009880200028419495  PSNR: 23.533763885498047\n",
            "[TRAIN] Iter: 34800 Loss: 0.016374286264181137  PSNR: 20.928842544555664\n",
            "[TRAIN] Iter: 34900 Loss: 0.014786278828978539  PSNR: 21.42582130432129\n",
            "[TRAIN] Iter: 35000 Loss: 0.015385066159069538  PSNR: 21.40244483947754\n",
            "[TRAIN] Iter: 35100 Loss: 0.011218956671655178  PSNR: 22.57502555847168\n",
            "[TRAIN] Iter: 35200 Loss: 0.012378125451505184  PSNR: 22.494768142700195\n",
            "[TRAIN] Iter: 35300 Loss: 0.013476524502038956  PSNR: 21.928434371948242\n",
            "[TRAIN] Iter: 35400 Loss: 0.013161500915884972  PSNR: 21.929248809814453\n",
            "[TRAIN] Iter: 35500 Loss: 0.011643542908132076  PSNR: 22.8632755279541\n",
            "[TRAIN] Iter: 35600 Loss: 0.015045848675072193  PSNR: 21.320547103881836\n",
            "[TRAIN] Iter: 35700 Loss: 0.018308531492948532  PSNR: 20.847742080688477\n",
            "[TRAIN] Iter: 35800 Loss: 0.015590507537126541  PSNR: 21.268918991088867\n",
            "[TRAIN] Iter: 35900 Loss: 0.010300485417246819  PSNR: 22.635826110839844\n",
            "[TRAIN] Iter: 36000 Loss: 0.014540700241923332  PSNR: 21.45916748046875\n",
            "[TRAIN] Iter: 36100 Loss: 0.016958914697170258  PSNR: 20.700979232788086\n",
            "[TRAIN] Iter: 36200 Loss: 0.020438222214579582  PSNR: 20.02339744567871\n",
            "[TRAIN] Iter: 36300 Loss: 0.019206207245588303  PSNR: 20.314653396606445\n",
            "[TRAIN] Iter: 36400 Loss: 0.019119786098599434  PSNR: 20.528179168701172\n",
            "[TRAIN] Iter: 36500 Loss: 0.017213860526680946  PSNR: 20.777992248535156\n",
            "[TRAIN] Iter: 36600 Loss: 0.013676545582711697  PSNR: 21.91804313659668\n",
            "[TRAIN] Iter: 36700 Loss: 0.012014934793114662  PSNR: 22.591096878051758\n",
            "[TRAIN] Iter: 36800 Loss: 0.01726202666759491  PSNR: 21.171537399291992\n",
            "[TRAIN] Iter: 36900 Loss: 0.016212912276387215  PSNR: 21.065357208251953\n",
            "[TRAIN] Iter: 37000 Loss: 0.016761131584644318  PSNR: 21.120203018188477\n",
            "[TRAIN] Iter: 37100 Loss: 0.01839314214885235  PSNR: 20.571796417236328\n",
            "[TRAIN] Iter: 37200 Loss: 0.011081315577030182  PSNR: 22.602216720581055\n",
            "[TRAIN] Iter: 37300 Loss: 0.011979799717664719  PSNR: 22.62786293029785\n",
            "[TRAIN] Iter: 37400 Loss: 0.017677392810583115  PSNR: 20.913022994995117\n",
            "[TRAIN] Iter: 37500 Loss: 0.010820061899721622  PSNR: 23.150468826293945\n",
            "[TRAIN] Iter: 37600 Loss: 0.021254578605294228  PSNR: 20.08930778503418\n",
            "[TRAIN] Iter: 37700 Loss: 0.013649753294885159  PSNR: 21.673425674438477\n",
            "[TRAIN] Iter: 37800 Loss: 0.009269153699278831  PSNR: 23.38431739807129\n",
            "[TRAIN] Iter: 37900 Loss: 0.015114514157176018  PSNR: 21.3271484375\n",
            "[TRAIN] Iter: 38000 Loss: 0.013241964392364025  PSNR: 21.964706420898438\n",
            "[TRAIN] Iter: 38100 Loss: 0.02140221931040287  PSNR: 19.9818115234375\n",
            "[TRAIN] Iter: 38200 Loss: 0.009837456978857517  PSNR: 23.314279556274414\n",
            "[TRAIN] Iter: 38300 Loss: 0.012822852469980717  PSNR: 22.11306381225586\n",
            "[TRAIN] Iter: 38400 Loss: 0.011242782697081566  PSNR: 22.860759735107422\n",
            "[TRAIN] Iter: 38500 Loss: 0.0162610225379467  PSNR: 21.18299674987793\n",
            "[TRAIN] Iter: 38600 Loss: 0.01749344915151596  PSNR: 21.027698516845703\n",
            "[TRAIN] Iter: 38700 Loss: 0.017444375902414322  PSNR: 20.746374130249023\n",
            "[TRAIN] Iter: 38800 Loss: 0.014294317923486233  PSNR: 21.779726028442383\n",
            "[TRAIN] Iter: 38900 Loss: 0.01661345735192299  PSNR: 21.13389778137207\n",
            "[TRAIN] Iter: 39000 Loss: 0.018076680600643158  PSNR: 20.921321868896484\n",
            "[TRAIN] Iter: 39100 Loss: 0.017363183200359344  PSNR: 20.761276245117188\n",
            "[TRAIN] Iter: 39200 Loss: 0.009930042549967766  PSNR: 23.45570182800293\n",
            "[TRAIN] Iter: 39300 Loss: 0.013282299973070621  PSNR: 21.82439613342285\n",
            "[TRAIN] Iter: 39400 Loss: 0.021808791905641556  PSNR: 19.748592376708984\n",
            "[TRAIN] Iter: 39500 Loss: 0.015071585774421692  PSNR: 21.77535057067871\n",
            "[TRAIN] Iter: 39600 Loss: 0.01173657551407814  PSNR: 22.569175720214844\n",
            "[TRAIN] Iter: 39700 Loss: 0.013937040232121944  PSNR: 21.829635620117188\n",
            "[TRAIN] Iter: 39800 Loss: 0.014355381950736046  PSNR: 21.980926513671875\n",
            "[TRAIN] Iter: 39900 Loss: 0.019390854984521866  PSNR: 20.453838348388672\n",
            " 20% 39999/200000 [1:18:30<5:19:06,  8.36it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/040000.tar\n",
            "[TRAIN] Iter: 40000 Loss: 0.017643187195062637  PSNR: 20.761943817138672\n",
            "[TRAIN] Iter: 40100 Loss: 0.014305870980024338  PSNR: 21.693222045898438\n",
            "[TRAIN] Iter: 40200 Loss: 0.017718933522701263  PSNR: 20.6069393157959\n",
            "[TRAIN] Iter: 40300 Loss: 0.012077823281288147  PSNR: 22.500926971435547\n",
            "[TRAIN] Iter: 40400 Loss: 0.017632704228162766  PSNR: 20.97330093383789\n",
            "[TRAIN] Iter: 40500 Loss: 0.010282693430781364  PSNR: 23.493806838989258\n",
            "[TRAIN] Iter: 40600 Loss: 0.017345111817121506  PSNR: 20.80438995361328\n",
            "[TRAIN] Iter: 40700 Loss: 0.016357310116291046  PSNR: 20.938364028930664\n",
            "[TRAIN] Iter: 40800 Loss: 0.015408932231366634  PSNR: 21.102956771850586\n",
            "[TRAIN] Iter: 40900 Loss: 0.009052173234522343  PSNR: 24.083810806274414\n",
            "[TRAIN] Iter: 41000 Loss: 0.019917968660593033  PSNR: 20.423105239868164\n",
            "[TRAIN] Iter: 41100 Loss: 0.014435948804020882  PSNR: 21.657514572143555\n",
            "[TRAIN] Iter: 41200 Loss: 0.014855453744530678  PSNR: 21.600658416748047\n",
            "[TRAIN] Iter: 41300 Loss: 0.016783276572823524  PSNR: 21.01724624633789\n",
            "[TRAIN] Iter: 41400 Loss: 0.012502940371632576  PSNR: 22.175748825073242\n",
            "[TRAIN] Iter: 41500 Loss: 0.021576762199401855  PSNR: 19.858789443969727\n",
            "[TRAIN] Iter: 41600 Loss: 0.01889185607433319  PSNR: 20.601335525512695\n",
            "[TRAIN] Iter: 41700 Loss: 0.01739906705915928  PSNR: 20.883310317993164\n",
            "[TRAIN] Iter: 41800 Loss: 0.014072746969759464  PSNR: 21.537736892700195\n",
            "[TRAIN] Iter: 41900 Loss: 0.016771219670772552  PSNR: 20.749340057373047\n",
            "[TRAIN] Iter: 42000 Loss: 0.012720407918095589  PSNR: 22.277297973632812\n",
            "[TRAIN] Iter: 42100 Loss: 0.019311239942908287  PSNR: 20.47329330444336\n",
            "[TRAIN] Iter: 42200 Loss: 0.014894293621182442  PSNR: 21.368350982666016\n",
            "[TRAIN] Iter: 42300 Loss: 0.02623685449361801  PSNR: 18.815078735351562\n",
            "[TRAIN] Iter: 42400 Loss: 0.014583015814423561  PSNR: 21.77863121032715\n",
            "[TRAIN] Iter: 42500 Loss: 0.012002724222838879  PSNR: 22.39944076538086\n",
            "[TRAIN] Iter: 42600 Loss: 0.013156747445464134  PSNR: 21.864595413208008\n",
            "[TRAIN] Iter: 42700 Loss: 0.017716314643621445  PSNR: 20.913707733154297\n",
            "[TRAIN] Iter: 42800 Loss: 0.010569587349891663  PSNR: 23.29660987854004\n",
            "[TRAIN] Iter: 42900 Loss: 0.00918198749423027  PSNR: 22.864782333374023\n",
            "[TRAIN] Iter: 43000 Loss: 0.01642007753252983  PSNR: 21.02294158935547\n",
            "[TRAIN] Iter: 43100 Loss: 0.008780580013990402  PSNR: 23.723623275756836\n",
            "[TRAIN] Iter: 43200 Loss: 0.009799293242394924  PSNR: 23.429346084594727\n",
            "[TRAIN] Iter: 43300 Loss: 0.014116212725639343  PSNR: 22.068546295166016\n",
            "[TRAIN] Iter: 43400 Loss: 0.016362985596060753  PSNR: 21.043621063232422\n",
            "[TRAIN] Iter: 43500 Loss: 0.018775172531604767  PSNR: 20.319801330566406\n",
            "[TRAIN] Iter: 43600 Loss: 0.01932622864842415  PSNR: 20.267868041992188\n",
            "[TRAIN] Iter: 43700 Loss: 0.011102446354925632  PSNR: 22.83357810974121\n",
            "[TRAIN] Iter: 43800 Loss: 0.011710543185472488  PSNR: 22.87315559387207\n",
            "[TRAIN] Iter: 43900 Loss: 0.014475383795797825  PSNR: 21.807849884033203\n",
            "[TRAIN] Iter: 44000 Loss: 0.017141252756118774  PSNR: 20.785032272338867\n",
            "[TRAIN] Iter: 44100 Loss: 0.014942272566258907  PSNR: 21.46903419494629\n",
            "[TRAIN] Iter: 44200 Loss: 0.011974970810115337  PSNR: 22.824413299560547\n",
            "[TRAIN] Iter: 44300 Loss: 0.01503393892198801  PSNR: 21.512903213500977\n",
            "[TRAIN] Iter: 44400 Loss: 0.016347242519259453  PSNR: 21.301523208618164\n",
            "[TRAIN] Iter: 44500 Loss: 0.01690976694226265  PSNR: 20.979576110839844\n",
            "[TRAIN] Iter: 44600 Loss: 0.011674167588353157  PSNR: 22.524459838867188\n",
            "[TRAIN] Iter: 44700 Loss: 0.014235103502869606  PSNR: 21.951963424682617\n",
            "[TRAIN] Iter: 44800 Loss: 0.014506345614790916  PSNR: 21.696531295776367\n",
            "[TRAIN] Iter: 44900 Loss: 0.020461121574044228  PSNR: 20.292869567871094\n",
            "[TRAIN] Iter: 45000 Loss: 0.012725450098514557  PSNR: 22.232542037963867\n",
            "[TRAIN] Iter: 45100 Loss: 0.01312687061727047  PSNR: 22.269594192504883\n",
            "[TRAIN] Iter: 45200 Loss: 0.011799165979027748  PSNR: 22.487035751342773\n",
            "[TRAIN] Iter: 45300 Loss: 0.01513975765556097  PSNR: 21.65363121032715\n",
            "[TRAIN] Iter: 45400 Loss: 0.011874097399413586  PSNR: 22.709272384643555\n",
            "[TRAIN] Iter: 45500 Loss: 0.015368814580142498  PSNR: 21.370166778564453\n",
            "[TRAIN] Iter: 45600 Loss: 0.011122724041342735  PSNR: 22.789993286132812\n",
            "[TRAIN] Iter: 45700 Loss: 0.01925925724208355  PSNR: 20.50145149230957\n",
            "[TRAIN] Iter: 45800 Loss: 0.017003914341330528  PSNR: 20.977935791015625\n",
            "[TRAIN] Iter: 45900 Loss: 0.01743047498166561  PSNR: 20.74968147277832\n",
            "[TRAIN] Iter: 46000 Loss: 0.014067708514630795  PSNR: 22.150312423706055\n",
            "[TRAIN] Iter: 46100 Loss: 0.012765560299158096  PSNR: 22.545347213745117\n",
            "[TRAIN] Iter: 46200 Loss: 0.017270497977733612  PSNR: 21.208587646484375\n",
            "[TRAIN] Iter: 46300 Loss: 0.01748558133840561  PSNR: 20.592741012573242\n",
            "[TRAIN] Iter: 46400 Loss: 0.010289240628480911  PSNR: 23.53449821472168\n",
            "[TRAIN] Iter: 46500 Loss: 0.017491508275270462  PSNR: 20.772052764892578\n",
            "[TRAIN] Iter: 46600 Loss: 0.022168662399053574  PSNR: 19.826202392578125\n",
            "[TRAIN] Iter: 46700 Loss: 0.013203312642872334  PSNR: 22.013906478881836\n",
            "[TRAIN] Iter: 46800 Loss: 0.021641921252012253  PSNR: 19.48640251159668\n",
            "[TRAIN] Iter: 46900 Loss: 0.013996538706123829  PSNR: 21.969919204711914\n",
            "[TRAIN] Iter: 47000 Loss: 0.01350367534905672  PSNR: 21.72544288635254\n",
            "[TRAIN] Iter: 47100 Loss: 0.012034659273922443  PSNR: 22.301513671875\n",
            "[TRAIN] Iter: 47200 Loss: 0.016143685206770897  PSNR: 21.083202362060547\n",
            "[TRAIN] Iter: 47300 Loss: 0.017377816140651703  PSNR: 21.10875129699707\n",
            "[TRAIN] Iter: 47400 Loss: 0.014987008646130562  PSNR: 21.754100799560547\n",
            "[TRAIN] Iter: 47500 Loss: 0.016194693744182587  PSNR: 21.47679901123047\n",
            "[TRAIN] Iter: 47600 Loss: 0.02086290717124939  PSNR: 20.178030014038086\n",
            "[TRAIN] Iter: 47700 Loss: 0.017645427957177162  PSNR: 20.8702335357666\n",
            "[TRAIN] Iter: 47800 Loss: 0.01566297374665737  PSNR: 21.459129333496094\n",
            "[TRAIN] Iter: 47900 Loss: 0.01245870627462864  PSNR: 22.286510467529297\n",
            "[TRAIN] Iter: 48000 Loss: 0.021434485912322998  PSNR: 19.961624145507812\n",
            "[TRAIN] Iter: 48100 Loss: 0.014465566724538803  PSNR: 21.364171981811523\n",
            "[TRAIN] Iter: 48200 Loss: 0.01196545921266079  PSNR: 22.75994110107422\n",
            "[TRAIN] Iter: 48300 Loss: 0.02099047228693962  PSNR: 19.916662216186523\n",
            "[TRAIN] Iter: 48400 Loss: 0.01361524686217308  PSNR: 22.048994064331055\n",
            "[TRAIN] Iter: 48500 Loss: 0.012485950253903866  PSNR: 22.504106521606445\n",
            "[TRAIN] Iter: 48600 Loss: 0.018216572701931  PSNR: 20.73951530456543\n",
            "[TRAIN] Iter: 48700 Loss: 0.020803302526474  PSNR: 20.509159088134766\n",
            "[TRAIN] Iter: 48800 Loss: 0.014249144122004509  PSNR: 21.596982955932617\n",
            "[TRAIN] Iter: 48900 Loss: 0.009750887751579285  PSNR: 23.63097381591797\n",
            "[TRAIN] Iter: 49000 Loss: 0.01946074143052101  PSNR: 20.185579299926758\n",
            "[TRAIN] Iter: 49100 Loss: 0.01126796193420887  PSNR: 22.62743377685547\n",
            "[TRAIN] Iter: 49200 Loss: 0.013407871127128601  PSNR: 22.145740509033203\n",
            "[TRAIN] Iter: 49300 Loss: 0.01535324938595295  PSNR: 21.282499313354492\n",
            "[TRAIN] Iter: 49400 Loss: 0.014498032629489899  PSNR: 21.928007125854492\n",
            "[TRAIN] Iter: 49500 Loss: 0.008333694189786911  PSNR: 24.448623657226562\n",
            "[TRAIN] Iter: 49600 Loss: 0.013468889519572258  PSNR: 22.012706756591797\n",
            "[TRAIN] Iter: 49700 Loss: 0.015928471460938454  PSNR: 20.846546173095703\n",
            "[TRAIN] Iter: 49800 Loss: 0.014736907556653023  PSNR: 22.07532501220703\n",
            "[TRAIN] Iter: 49900 Loss: 0.013948257081210613  PSNR: 21.899784088134766\n",
            " 25% 49999/200000 [1:38:19<4:56:20,  8.44it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/050000.tar\n",
            "[TRAIN] Iter: 50000 Loss: 0.014338092878460884  PSNR: 21.636430740356445\n",
            "[TRAIN] Iter: 50100 Loss: 0.02204623445868492  PSNR: 19.473899841308594\n",
            "[TRAIN] Iter: 50200 Loss: 0.017281167209148407  PSNR: 20.99755096435547\n",
            "[TRAIN] Iter: 50300 Loss: 0.019259069114923477  PSNR: 20.575138092041016\n",
            "[TRAIN] Iter: 50400 Loss: 0.016423838213086128  PSNR: 21.680768966674805\n",
            "[TRAIN] Iter: 50500 Loss: 0.01186714693903923  PSNR: 22.60831642150879\n",
            "[TRAIN] Iter: 50600 Loss: 0.010708959773182869  PSNR: 23.054241180419922\n",
            "[TRAIN] Iter: 50700 Loss: 0.014319302514195442  PSNR: 21.86737823486328\n",
            "[TRAIN] Iter: 50800 Loss: 0.012231530621647835  PSNR: 22.54459571838379\n",
            "[TRAIN] Iter: 50900 Loss: 0.011713704094290733  PSNR: 22.56609535217285\n",
            "[TRAIN] Iter: 51000 Loss: 0.012440563179552555  PSNR: 22.47393226623535\n",
            "[TRAIN] Iter: 51100 Loss: 0.014318954199552536  PSNR: 22.28444480895996\n",
            "[TRAIN] Iter: 51200 Loss: 0.022576984018087387  PSNR: 19.638696670532227\n",
            "[TRAIN] Iter: 51300 Loss: 0.012761255726218224  PSNR: 22.201919555664062\n",
            "[TRAIN] Iter: 51400 Loss: 0.013498018495738506  PSNR: 22.35376739501953\n",
            "[TRAIN] Iter: 51500 Loss: 0.011767331510782242  PSNR: 22.84158706665039\n",
            "[TRAIN] Iter: 51600 Loss: 0.015122000128030777  PSNR: 21.54100227355957\n",
            "[TRAIN] Iter: 51700 Loss: 0.023237064480781555  PSNR: 19.51913070678711\n",
            "[TRAIN] Iter: 51800 Loss: 0.016481641680002213  PSNR: 21.143047332763672\n",
            "[TRAIN] Iter: 51900 Loss: 0.009046364575624466  PSNR: 24.195438385009766\n",
            "[TRAIN] Iter: 52000 Loss: 0.010212439112365246  PSNR: 22.919580459594727\n",
            "[TRAIN] Iter: 52100 Loss: 0.014172758907079697  PSNR: 21.91109275817871\n",
            "[TRAIN] Iter: 52200 Loss: 0.014395827427506447  PSNR: 21.576583862304688\n",
            "[TRAIN] Iter: 52300 Loss: 0.014362549409270287  PSNR: 21.928972244262695\n",
            "[TRAIN] Iter: 52400 Loss: 0.01140584982931614  PSNR: 22.855024337768555\n",
            "[TRAIN] Iter: 52500 Loss: 0.011728335171937943  PSNR: 22.94493865966797\n",
            "[TRAIN] Iter: 52600 Loss: 0.013048198074102402  PSNR: 22.22159194946289\n",
            "[TRAIN] Iter: 52700 Loss: 0.021456969901919365  PSNR: 19.753341674804688\n",
            "[TRAIN] Iter: 52800 Loss: 0.02164558693766594  PSNR: 20.084522247314453\n",
            "[TRAIN] Iter: 52900 Loss: 0.012671162374317646  PSNR: 22.14356231689453\n",
            "[TRAIN] Iter: 53000 Loss: 0.011283007450401783  PSNR: 23.01039695739746\n",
            "[TRAIN] Iter: 53100 Loss: 0.014909479767084122  PSNR: 21.72014808654785\n",
            "[TRAIN] Iter: 53200 Loss: 0.01218800712376833  PSNR: 22.564180374145508\n",
            "[TRAIN] Iter: 53300 Loss: 0.012986741028726101  PSNR: 22.43630027770996\n",
            "[TRAIN] Iter: 53400 Loss: 0.015553988516330719  PSNR: 21.317781448364258\n",
            "[TRAIN] Iter: 53500 Loss: 0.015089292079210281  PSNR: 21.365516662597656\n",
            "[TRAIN] Iter: 53600 Loss: 0.021449191495776176  PSNR: 20.163898468017578\n",
            "[TRAIN] Iter: 53700 Loss: 0.018602052703499794  PSNR: 20.468904495239258\n",
            "[TRAIN] Iter: 53800 Loss: 0.01042993739247322  PSNR: 22.86199378967285\n",
            "[TRAIN] Iter: 53900 Loss: 0.019753731787204742  PSNR: 19.99132537841797\n",
            "[TRAIN] Iter: 54000 Loss: 0.015115320682525635  PSNR: 22.110576629638672\n",
            "[TRAIN] Iter: 54100 Loss: 0.018267352133989334  PSNR: 20.730627059936523\n",
            "[TRAIN] Iter: 54200 Loss: 0.011547894217073917  PSNR: 22.8781681060791\n",
            "[TRAIN] Iter: 54300 Loss: 0.014074442908167839  PSNR: 21.61660385131836\n",
            "[TRAIN] Iter: 54400 Loss: 0.009444723837077618  PSNR: 23.885921478271484\n",
            "[TRAIN] Iter: 54500 Loss: 0.0155311468988657  PSNR: 21.706085205078125\n",
            "[TRAIN] Iter: 54600 Loss: 0.01561308465898037  PSNR: 21.74095344543457\n",
            "[TRAIN] Iter: 54700 Loss: 0.015358067117631435  PSNR: 21.555471420288086\n",
            "[TRAIN] Iter: 54800 Loss: 0.010312605649232864  PSNR: 22.98924446105957\n",
            "[TRAIN] Iter: 54900 Loss: 0.013301842845976353  PSNR: 22.334455490112305\n",
            "[TRAIN] Iter: 55000 Loss: 0.018955104053020477  PSNR: 20.23592758178711\n",
            "[TRAIN] Iter: 55100 Loss: 0.012165195308625698  PSNR: 22.16927146911621\n",
            "[TRAIN] Iter: 55200 Loss: 0.006558769848197699  PSNR: 24.720951080322266\n",
            "[TRAIN] Iter: 55300 Loss: 0.016614094376564026  PSNR: 20.968074798583984\n",
            "[TRAIN] Iter: 55400 Loss: 0.012093521654605865  PSNR: 22.82726287841797\n",
            "[TRAIN] Iter: 55500 Loss: 0.013915789313614368  PSNR: 22.327362060546875\n",
            "[TRAIN] Iter: 55600 Loss: 0.01472314540296793  PSNR: 21.41037368774414\n",
            "[TRAIN] Iter: 55700 Loss: 0.01726367324590683  PSNR: 21.02717399597168\n",
            "[TRAIN] Iter: 55800 Loss: 0.01351192593574524  PSNR: 21.771240234375\n",
            "[TRAIN] Iter: 55900 Loss: 0.022546617314219475  PSNR: 19.876300811767578\n",
            "[TRAIN] Iter: 56000 Loss: 0.015982026234269142  PSNR: 21.390838623046875\n",
            "[TRAIN] Iter: 56100 Loss: 0.011291957460343838  PSNR: 22.745250701904297\n",
            "[TRAIN] Iter: 56200 Loss: 0.014883925206959248  PSNR: 21.620582580566406\n",
            "[TRAIN] Iter: 56300 Loss: 0.010883290320634842  PSNR: 22.869407653808594\n",
            "[TRAIN] Iter: 56400 Loss: 0.023726634681224823  PSNR: 19.59510612487793\n",
            "[TRAIN] Iter: 56500 Loss: 0.010206477716565132  PSNR: 23.043224334716797\n",
            "[TRAIN] Iter: 56600 Loss: 0.013785450719296932  PSNR: 21.918067932128906\n",
            "[TRAIN] Iter: 56700 Loss: 0.016575675457715988  PSNR: 21.326568603515625\n",
            "[TRAIN] Iter: 56800 Loss: 0.011925885453820229  PSNR: 22.61100196838379\n",
            "[TRAIN] Iter: 56900 Loss: 0.009932072833180428  PSNR: 23.734167098999023\n",
            "[TRAIN] Iter: 57000 Loss: 0.018739789724349976  PSNR: 20.76190757751465\n",
            "[TRAIN] Iter: 57100 Loss: 0.020561441779136658  PSNR: 19.837888717651367\n",
            "[TRAIN] Iter: 57200 Loss: 0.014075852930545807  PSNR: 22.238056182861328\n",
            "[TRAIN] Iter: 57300 Loss: 0.011903589591383934  PSNR: 22.736034393310547\n",
            "[TRAIN] Iter: 57400 Loss: 0.013063429854810238  PSNR: 21.9488468170166\n",
            "[TRAIN] Iter: 57500 Loss: 0.012747347354888916  PSNR: 22.385942459106445\n",
            "[TRAIN] Iter: 57600 Loss: 0.014054538682103157  PSNR: 21.667224884033203\n",
            "[TRAIN] Iter: 57700 Loss: 0.012599660083651543  PSNR: 22.02186393737793\n",
            "[TRAIN] Iter: 57800 Loss: 0.01983138732612133  PSNR: 20.507654190063477\n",
            "[TRAIN] Iter: 57900 Loss: 0.020767880603671074  PSNR: 20.080305099487305\n",
            "[TRAIN] Iter: 58000 Loss: 0.016889162361621857  PSNR: 21.123132705688477\n",
            "[TRAIN] Iter: 58100 Loss: 0.012991162948310375  PSNR: 21.99368667602539\n",
            "[TRAIN] Iter: 58200 Loss: 0.014660460874438286  PSNR: 21.847963333129883\n",
            "[TRAIN] Iter: 58300 Loss: 0.014735549688339233  PSNR: 21.5025577545166\n",
            "[TRAIN] Iter: 58400 Loss: 0.01868598721921444  PSNR: 20.548181533813477\n",
            "[TRAIN] Iter: 58500 Loss: 0.013892021030187607  PSNR: 22.02865982055664\n",
            "[TRAIN] Iter: 58600 Loss: 0.014875788241624832  PSNR: 21.932666778564453\n",
            "[TRAIN] Iter: 58700 Loss: 0.017788976430892944  PSNR: 20.965036392211914\n",
            "[TRAIN] Iter: 58800 Loss: 0.01129432488232851  PSNR: 23.199377059936523\n",
            "[TRAIN] Iter: 58900 Loss: 0.016829172149300575  PSNR: 21.376920700073242\n",
            "[TRAIN] Iter: 59000 Loss: 0.008288602344691753  PSNR: 23.95635986328125\n",
            "[TRAIN] Iter: 59100 Loss: 0.010499607771635056  PSNR: 23.26558494567871\n",
            "[TRAIN] Iter: 59200 Loss: 0.011212533339858055  PSNR: 22.799060821533203\n",
            "[TRAIN] Iter: 59300 Loss: 0.013069943524897099  PSNR: 22.135860443115234\n",
            "[TRAIN] Iter: 59400 Loss: 0.011133632622659206  PSNR: 22.70465087890625\n",
            "[TRAIN] Iter: 59500 Loss: 0.01653374545276165  PSNR: 20.760705947875977\n",
            "[TRAIN] Iter: 59600 Loss: 0.015582332387566566  PSNR: 21.21517562866211\n",
            "[TRAIN] Iter: 59700 Loss: 0.011408149264752865  PSNR: 22.753854751586914\n",
            "[TRAIN] Iter: 59800 Loss: 0.013952530920505524  PSNR: 22.035297393798828\n",
            "[TRAIN] Iter: 59900 Loss: 0.011940427124500275  PSNR: 22.33174705505371\n",
            " 30% 59999/200000 [1:58:03<4:31:51,  8.58it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/060000.tar\n",
            "[TRAIN] Iter: 60000 Loss: 0.015064622275531292  PSNR: 22.159591674804688\n",
            "[TRAIN] Iter: 60100 Loss: 0.015124495141208172  PSNR: 21.93519401550293\n",
            "[TRAIN] Iter: 60200 Loss: 0.011822951957583427  PSNR: 22.772428512573242\n",
            "[TRAIN] Iter: 60300 Loss: 0.013226162642240524  PSNR: 22.126148223876953\n",
            "[TRAIN] Iter: 60400 Loss: 0.01559707522392273  PSNR: 21.428638458251953\n",
            "[TRAIN] Iter: 60500 Loss: 0.014077129773795605  PSNR: 22.468990325927734\n",
            "[TRAIN] Iter: 60600 Loss: 0.009925411082804203  PSNR: 23.069223403930664\n",
            "[TRAIN] Iter: 60700 Loss: 0.013429418206214905  PSNR: 21.590312957763672\n",
            "[TRAIN] Iter: 60800 Loss: 0.01717820018529892  PSNR: 21.084930419921875\n",
            "[TRAIN] Iter: 60900 Loss: 0.008779019117355347  PSNR: 23.733213424682617\n",
            "[TRAIN] Iter: 61000 Loss: 0.01624119281768799  PSNR: 21.10557746887207\n",
            "[TRAIN] Iter: 61100 Loss: 0.01388641633093357  PSNR: 22.062931060791016\n",
            "[TRAIN] Iter: 61200 Loss: 0.008900497108697891  PSNR: 24.280475616455078\n",
            "[TRAIN] Iter: 61300 Loss: 0.014476973563432693  PSNR: 21.67049789428711\n",
            "[TRAIN] Iter: 61400 Loss: 0.009634417481720448  PSNR: 23.318071365356445\n",
            "[TRAIN] Iter: 61500 Loss: 0.010930059477686882  PSNR: 23.002370834350586\n",
            "[TRAIN] Iter: 61600 Loss: 0.009394743479788303  PSNR: 23.83523941040039\n",
            "[TRAIN] Iter: 61700 Loss: 0.012817179784178734  PSNR: 22.36348533630371\n",
            "[TRAIN] Iter: 61800 Loss: 0.01090223528444767  PSNR: 23.142276763916016\n",
            "[TRAIN] Iter: 61900 Loss: 0.010486740618944168  PSNR: 22.925037384033203\n",
            "[TRAIN] Iter: 62000 Loss: 0.016724109649658203  PSNR: 21.028329849243164\n",
            "[TRAIN] Iter: 62100 Loss: 0.012025355361402035  PSNR: 21.819149017333984\n",
            "[TRAIN] Iter: 62200 Loss: 0.011111798696219921  PSNR: 22.74456214904785\n",
            "[TRAIN] Iter: 62300 Loss: 0.020663287490606308  PSNR: 19.867172241210938\n",
            "[TRAIN] Iter: 62400 Loss: 0.017494089901447296  PSNR: 21.10696029663086\n",
            "[TRAIN] Iter: 62500 Loss: 0.007729824632406235  PSNR: 24.398948669433594\n",
            "[TRAIN] Iter: 62600 Loss: 0.014905966818332672  PSNR: 21.60765838623047\n",
            "[TRAIN] Iter: 62700 Loss: 0.014908380806446075  PSNR: 21.528297424316406\n",
            "[TRAIN] Iter: 62800 Loss: 0.013961933553218842  PSNR: 21.35313606262207\n",
            "[TRAIN] Iter: 62900 Loss: 0.012540332041680813  PSNR: 22.450315475463867\n",
            "[TRAIN] Iter: 63000 Loss: 0.009813519194722176  PSNR: 23.924972534179688\n",
            "[TRAIN] Iter: 63100 Loss: 0.014821426942944527  PSNR: 21.64984703063965\n",
            "[TRAIN] Iter: 63200 Loss: 0.011293521150946617  PSNR: 22.87636375427246\n",
            "[TRAIN] Iter: 63300 Loss: 0.012801825068891048  PSNR: 22.42290496826172\n",
            "[TRAIN] Iter: 63400 Loss: 0.01631321758031845  PSNR: 21.513492584228516\n",
            "[TRAIN] Iter: 63500 Loss: 0.01347755640745163  PSNR: 22.42757225036621\n",
            "[TRAIN] Iter: 63600 Loss: 0.01003942359238863  PSNR: 23.41944694519043\n",
            "[TRAIN] Iter: 63700 Loss: 0.012323707342147827  PSNR: 22.847585678100586\n",
            "[TRAIN] Iter: 63800 Loss: 0.010934528894722462  PSNR: 23.44479751586914\n",
            "[TRAIN] Iter: 63900 Loss: 0.011201174929738045  PSNR: 22.816328048706055\n",
            "[TRAIN] Iter: 64000 Loss: 0.017155427485704422  PSNR: 21.133522033691406\n",
            "[TRAIN] Iter: 64100 Loss: 0.009074581786990166  PSNR: 23.63010025024414\n",
            "[TRAIN] Iter: 64200 Loss: 0.014503143727779388  PSNR: 21.9661808013916\n",
            "[TRAIN] Iter: 64300 Loss: 0.02097311057150364  PSNR: 20.013879776000977\n",
            "[TRAIN] Iter: 64400 Loss: 0.016777481883764267  PSNR: 21.371089935302734\n",
            "[TRAIN] Iter: 64500 Loss: 0.010275197215378284  PSNR: 23.472946166992188\n",
            "[TRAIN] Iter: 64600 Loss: 0.01120778638869524  PSNR: 22.648395538330078\n",
            "[TRAIN] Iter: 64700 Loss: 0.007237122394144535  PSNR: 24.590909957885742\n",
            "[TRAIN] Iter: 64800 Loss: 0.008706016466021538  PSNR: 23.846435546875\n",
            "[TRAIN] Iter: 64900 Loss: 0.014761928468942642  PSNR: 21.92565155029297\n",
            "[TRAIN] Iter: 65000 Loss: 0.012112128548324108  PSNR: 22.723217010498047\n",
            "[TRAIN] Iter: 65100 Loss: 0.012337036430835724  PSNR: 21.951242446899414\n",
            "[TRAIN] Iter: 65200 Loss: 0.020006094127893448  PSNR: 20.234806060791016\n",
            "[TRAIN] Iter: 65300 Loss: 0.015969635918736458  PSNR: 21.104135513305664\n",
            "[TRAIN] Iter: 65400 Loss: 0.01795319840312004  PSNR: 20.601364135742188\n",
            "[TRAIN] Iter: 65500 Loss: 0.01221255399286747  PSNR: 23.22419548034668\n",
            "[TRAIN] Iter: 65600 Loss: 0.022999076172709465  PSNR: 19.90481185913086\n",
            "[TRAIN] Iter: 65700 Loss: 0.016833728179335594  PSNR: 21.209260940551758\n",
            "[TRAIN] Iter: 65800 Loss: 0.010708210989832878  PSNR: 23.151052474975586\n",
            "[TRAIN] Iter: 65900 Loss: 0.012354294769465923  PSNR: 22.847095489501953\n",
            "[TRAIN] Iter: 66000 Loss: 0.010992780327796936  PSNR: 23.23555564880371\n",
            "[TRAIN] Iter: 66100 Loss: 0.012895588763058186  PSNR: 22.262643814086914\n",
            "[TRAIN] Iter: 66200 Loss: 0.016484923660755157  PSNR: 21.008460998535156\n",
            "[TRAIN] Iter: 66300 Loss: 0.012914735823869705  PSNR: 22.302091598510742\n",
            "[TRAIN] Iter: 66400 Loss: 0.017026662826538086  PSNR: 20.92182731628418\n",
            "[TRAIN] Iter: 66500 Loss: 0.008490475825965405  PSNR: 24.006145477294922\n",
            "[TRAIN] Iter: 66600 Loss: 0.011439486406743526  PSNR: 22.716766357421875\n",
            "[TRAIN] Iter: 66700 Loss: 0.012379785999655724  PSNR: 22.1948299407959\n",
            "[TRAIN] Iter: 66800 Loss: 0.011120308190584183  PSNR: 23.118270874023438\n",
            "[TRAIN] Iter: 66900 Loss: 0.009308483451604843  PSNR: 23.697229385375977\n",
            "[TRAIN] Iter: 67000 Loss: 0.009242992848157883  PSNR: 23.386863708496094\n",
            "[TRAIN] Iter: 67100 Loss: 0.01484687626361847  PSNR: 21.60074234008789\n",
            "[TRAIN] Iter: 67200 Loss: 0.01676560379564762  PSNR: 21.348838806152344\n",
            "[TRAIN] Iter: 67300 Loss: 0.011759614571928978  PSNR: 22.6685848236084\n",
            "[TRAIN] Iter: 67400 Loss: 0.014381393790245056  PSNR: 21.653202056884766\n",
            "[TRAIN] Iter: 67500 Loss: 0.009307853877544403  PSNR: 23.7005558013916\n",
            "[TRAIN] Iter: 67600 Loss: 0.01631719246506691  PSNR: 21.57649803161621\n",
            "[TRAIN] Iter: 67700 Loss: 0.01126897893846035  PSNR: 22.619232177734375\n",
            "[TRAIN] Iter: 67800 Loss: 0.011942350305616856  PSNR: 22.735742568969727\n",
            "[TRAIN] Iter: 67900 Loss: 0.01790517568588257  PSNR: 20.674335479736328\n",
            "[TRAIN] Iter: 68000 Loss: 0.009663321077823639  PSNR: 23.569093704223633\n",
            "[TRAIN] Iter: 68100 Loss: 0.01016145758330822  PSNR: 23.341150283813477\n",
            "[TRAIN] Iter: 68200 Loss: 0.009979305788874626  PSNR: 23.20677375793457\n",
            "[TRAIN] Iter: 68300 Loss: 0.01006111316382885  PSNR: 23.285449981689453\n",
            "[TRAIN] Iter: 68400 Loss: 0.016067570075392723  PSNR: 21.35068130493164\n",
            "[TRAIN] Iter: 68500 Loss: 0.014035460539162159  PSNR: 21.592252731323242\n",
            "[TRAIN] Iter: 68600 Loss: 0.01498456858098507  PSNR: 21.34589385986328\n",
            "[TRAIN] Iter: 68700 Loss: 0.016744844615459442  PSNR: 21.27301025390625\n",
            "[TRAIN] Iter: 68800 Loss: 0.013616969808936119  PSNR: 22.677349090576172\n",
            "[TRAIN] Iter: 68900 Loss: 0.011123375967144966  PSNR: 23.029033660888672\n",
            "[TRAIN] Iter: 69000 Loss: 0.016843928024172783  PSNR: 21.079933166503906\n",
            "[TRAIN] Iter: 69100 Loss: 0.009926101192831993  PSNR: 23.184185028076172\n",
            "[TRAIN] Iter: 69200 Loss: 0.010998640209436417  PSNR: 23.31401252746582\n",
            "[TRAIN] Iter: 69300 Loss: 0.011042611673474312  PSNR: 22.821489334106445\n",
            "[TRAIN] Iter: 69400 Loss: 0.010947511531412601  PSNR: 22.578453063964844\n",
            "[TRAIN] Iter: 69500 Loss: 0.018401311710476875  PSNR: 21.072179794311523\n",
            "[TRAIN] Iter: 69600 Loss: 0.014608361758291721  PSNR: 22.159242630004883\n",
            "[TRAIN] Iter: 69700 Loss: 0.014291106723248959  PSNR: 21.5966739654541\n",
            "[TRAIN] Iter: 69800 Loss: 0.016094118356704712  PSNR: 21.686922073364258\n",
            "[TRAIN] Iter: 69900 Loss: 0.012922226451337337  PSNR: 22.479877471923828\n",
            " 35% 69999/200000 [2:17:40<4:14:20,  8.52it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/070000.tar\n",
            "[TRAIN] Iter: 70000 Loss: 0.013176362961530685  PSNR: 21.87718963623047\n",
            "[TRAIN] Iter: 70100 Loss: 0.014042668975889683  PSNR: 21.793397903442383\n",
            "[TRAIN] Iter: 70200 Loss: 0.01395096443593502  PSNR: 21.81223487854004\n",
            "[TRAIN] Iter: 70300 Loss: 0.013209102675318718  PSNR: 21.979572296142578\n",
            "[TRAIN] Iter: 70400 Loss: 0.013313128612935543  PSNR: 22.22133445739746\n",
            "[TRAIN] Iter: 70500 Loss: 0.009373214095830917  PSNR: 24.104190826416016\n",
            "[TRAIN] Iter: 70600 Loss: 0.014839421957731247  PSNR: 22.3587589263916\n",
            "[TRAIN] Iter: 70700 Loss: 0.011282924562692642  PSNR: 23.555566787719727\n",
            "[TRAIN] Iter: 70800 Loss: 0.018176276236772537  PSNR: 21.046306610107422\n",
            "[TRAIN] Iter: 70900 Loss: 0.016751477494835854  PSNR: 21.242610931396484\n",
            "[TRAIN] Iter: 71000 Loss: 0.01122280489653349  PSNR: 22.687458038330078\n",
            "[TRAIN] Iter: 71100 Loss: 0.00900938268750906  PSNR: 24.0928955078125\n",
            "[TRAIN] Iter: 71200 Loss: 0.012035045772790909  PSNR: 22.47806167602539\n",
            "[TRAIN] Iter: 71300 Loss: 0.016851121559739113  PSNR: 21.258281707763672\n",
            "[TRAIN] Iter: 71400 Loss: 0.011090369895100594  PSNR: 23.081220626831055\n",
            "[TRAIN] Iter: 71500 Loss: 0.011542683467268944  PSNR: 22.75044059753418\n",
            "[TRAIN] Iter: 71600 Loss: 0.0075459955260157585  PSNR: 24.584957122802734\n",
            "[TRAIN] Iter: 71700 Loss: 0.014718899503350258  PSNR: 21.797143936157227\n",
            "[TRAIN] Iter: 71800 Loss: 0.016207870095968246  PSNR: 21.12420654296875\n",
            "[TRAIN] Iter: 71900 Loss: 0.011540129780769348  PSNR: 22.904600143432617\n",
            "[TRAIN] Iter: 72000 Loss: 0.011444954201579094  PSNR: 22.71451187133789\n",
            "[TRAIN] Iter: 72100 Loss: 0.012733031064271927  PSNR: 22.52027702331543\n",
            "[TRAIN] Iter: 72200 Loss: 0.015133294276893139  PSNR: 21.622892379760742\n",
            "[TRAIN] Iter: 72300 Loss: 0.012500185519456863  PSNR: 22.304054260253906\n",
            "[TRAIN] Iter: 72400 Loss: 0.013134731911122799  PSNR: 22.010377883911133\n",
            "[TRAIN] Iter: 72500 Loss: 0.01207193173468113  PSNR: 22.65828514099121\n",
            "[TRAIN] Iter: 72600 Loss: 0.008732826448976994  PSNR: 23.92820930480957\n",
            "[TRAIN] Iter: 72700 Loss: 0.014317499473690987  PSNR: 21.98133087158203\n",
            "[TRAIN] Iter: 72800 Loss: 0.010510072112083435  PSNR: 22.966535568237305\n",
            "[TRAIN] Iter: 72900 Loss: 0.016840165480971336  PSNR: 20.788381576538086\n",
            "[TRAIN] Iter: 73000 Loss: 0.008004558272659779  PSNR: 23.940736770629883\n",
            "[TRAIN] Iter: 73100 Loss: 0.017722491174936295  PSNR: 21.193836212158203\n",
            "[TRAIN] Iter: 73200 Loss: 0.014708118513226509  PSNR: 21.5755615234375\n",
            "[TRAIN] Iter: 73300 Loss: 0.016414551064372063  PSNR: 21.282943725585938\n",
            "[TRAIN] Iter: 73400 Loss: 0.012225023470818996  PSNR: 22.543771743774414\n",
            "[TRAIN] Iter: 73500 Loss: 0.019577356055378914  PSNR: 20.68943977355957\n",
            "[TRAIN] Iter: 73600 Loss: 0.011801259592175484  PSNR: 22.629671096801758\n",
            "[TRAIN] Iter: 73700 Loss: 0.013042639940977097  PSNR: 22.504901885986328\n",
            "[TRAIN] Iter: 73800 Loss: 0.01097352895885706  PSNR: 22.934965133666992\n",
            "[TRAIN] Iter: 73900 Loss: 0.014940031804144382  PSNR: 21.583194732666016\n",
            "[TRAIN] Iter: 74000 Loss: 0.00934482179582119  PSNR: 23.468523025512695\n",
            "[TRAIN] Iter: 74100 Loss: 0.010089891962707043  PSNR: 23.124677658081055\n",
            "[TRAIN] Iter: 74200 Loss: 0.01433546096086502  PSNR: 22.398265838623047\n",
            "[TRAIN] Iter: 74300 Loss: 0.016441598534584045  PSNR: 21.216489791870117\n",
            "[TRAIN] Iter: 74400 Loss: 0.011876709759235382  PSNR: 22.614370346069336\n",
            "[TRAIN] Iter: 74500 Loss: 0.014821775257587433  PSNR: 21.5352840423584\n",
            "[TRAIN] Iter: 74600 Loss: 0.010946208611130714  PSNR: 23.099510192871094\n",
            "[TRAIN] Iter: 74700 Loss: 0.008528192527592182  PSNR: 24.256147384643555\n",
            "[TRAIN] Iter: 74800 Loss: 0.017550859600305557  PSNR: 21.187898635864258\n",
            "[TRAIN] Iter: 74900 Loss: 0.014921960420906544  PSNR: 21.58102035522461\n",
            "[TRAIN] Iter: 75000 Loss: 0.012237265706062317  PSNR: 22.48154067993164\n",
            "[TRAIN] Iter: 75100 Loss: 0.0138669703155756  PSNR: 22.45782470703125\n",
            "[TRAIN] Iter: 75200 Loss: 0.015965834259986877  PSNR: 21.63656234741211\n",
            "[TRAIN] Iter: 75300 Loss: 0.010220199823379517  PSNR: 24.209186553955078\n",
            "[TRAIN] Iter: 75400 Loss: 0.01642822101712227  PSNR: 21.194847106933594\n",
            "[TRAIN] Iter: 75500 Loss: 0.00909927487373352  PSNR: 23.84881019592285\n",
            "[TRAIN] Iter: 75600 Loss: 0.01060735248029232  PSNR: 23.036048889160156\n",
            "[TRAIN] Iter: 75700 Loss: 0.01501978375017643  PSNR: 21.804996490478516\n",
            "[TRAIN] Iter: 75800 Loss: 0.013474921695888042  PSNR: 22.583904266357422\n",
            "[TRAIN] Iter: 75900 Loss: 0.012941084802150726  PSNR: 22.15447998046875\n",
            "[TRAIN] Iter: 76000 Loss: 0.014984667301177979  PSNR: 22.025896072387695\n",
            "[TRAIN] Iter: 76100 Loss: 0.01806357502937317  PSNR: 20.63357925415039\n",
            "[TRAIN] Iter: 76200 Loss: 0.017177395522594452  PSNR: 21.436519622802734\n",
            "[TRAIN] Iter: 76300 Loss: 0.010830676183104515  PSNR: 23.135868072509766\n",
            "[TRAIN] Iter: 76400 Loss: 0.013436761684715748  PSNR: 22.489826202392578\n",
            "[TRAIN] Iter: 76500 Loss: 0.012047888711094856  PSNR: 22.719968795776367\n",
            "[TRAIN] Iter: 76600 Loss: 0.014949691481888294  PSNR: 21.12841033935547\n",
            "[TRAIN] Iter: 76700 Loss: 0.015321302227675915  PSNR: 21.686059951782227\n",
            "[TRAIN] Iter: 76800 Loss: 0.014085129834711552  PSNR: 22.192169189453125\n",
            "[TRAIN] Iter: 76900 Loss: 0.011000004597008228  PSNR: 23.156179428100586\n",
            "[TRAIN] Iter: 77000 Loss: 0.021477553993463516  PSNR: 20.68132972717285\n",
            "[TRAIN] Iter: 77100 Loss: 0.010317924432456493  PSNR: 23.363189697265625\n",
            "[TRAIN] Iter: 77200 Loss: 0.01579204574227333  PSNR: 21.288352966308594\n",
            "[TRAIN] Iter: 77300 Loss: 0.009415957145392895  PSNR: 23.67414093017578\n",
            "[TRAIN] Iter: 77400 Loss: 0.012292871251702309  PSNR: 22.434114456176758\n",
            "[TRAIN] Iter: 77500 Loss: 0.009563513100147247  PSNR: 23.544212341308594\n",
            "[TRAIN] Iter: 77600 Loss: 0.0176021046936512  PSNR: 20.673328399658203\n",
            "[TRAIN] Iter: 77700 Loss: 0.011229888536036015  PSNR: 22.455406188964844\n",
            "[TRAIN] Iter: 77800 Loss: 0.01595418155193329  PSNR: 21.61264419555664\n",
            "[TRAIN] Iter: 77900 Loss: 0.007937452755868435  PSNR: 24.434696197509766\n",
            "[TRAIN] Iter: 78000 Loss: 0.016662050038576126  PSNR: 20.890539169311523\n",
            "[TRAIN] Iter: 78100 Loss: 0.017128536477684975  PSNR: 21.323368072509766\n",
            "[TRAIN] Iter: 78200 Loss: 0.009057318791747093  PSNR: 23.749073028564453\n",
            "[TRAIN] Iter: 78300 Loss: 0.012312797829508781  PSNR: 22.156085968017578\n",
            "[TRAIN] Iter: 78400 Loss: 0.013587862253189087  PSNR: 22.034624099731445\n",
            "[TRAIN] Iter: 78500 Loss: 0.011948333121836185  PSNR: 22.743112564086914\n",
            "[TRAIN] Iter: 78600 Loss: 0.012534519657492638  PSNR: 22.485864639282227\n",
            "[TRAIN] Iter: 78700 Loss: 0.01669391058385372  PSNR: 21.35255241394043\n",
            "[TRAIN] Iter: 78800 Loss: 0.012928945943713188  PSNR: 22.43214988708496\n",
            "[TRAIN] Iter: 78900 Loss: 0.02159324288368225  PSNR: 19.94171714782715\n",
            "[TRAIN] Iter: 79000 Loss: 0.017704086378216743  PSNR: 21.214717864990234\n",
            "[TRAIN] Iter: 79100 Loss: 0.008208578452467918  PSNR: 24.372379302978516\n",
            "[TRAIN] Iter: 79200 Loss: 0.015655264258384705  PSNR: 21.293109893798828\n",
            "[TRAIN] Iter: 79300 Loss: 0.012594333849847317  PSNR: 22.33782196044922\n",
            "[TRAIN] Iter: 79400 Loss: 0.017473671585321426  PSNR: 21.074275970458984\n",
            "[TRAIN] Iter: 79500 Loss: 0.013800445944070816  PSNR: 21.9750919342041\n",
            "[TRAIN] Iter: 79600 Loss: 0.010342578403651714  PSNR: 23.477245330810547\n",
            "[TRAIN] Iter: 79700 Loss: 0.017287855967879295  PSNR: 21.33135986328125\n",
            "[TRAIN] Iter: 79800 Loss: 0.006886448711156845  PSNR: 25.088415145874023\n",
            "[TRAIN] Iter: 79900 Loss: 0.009759681299328804  PSNR: 23.47469711303711\n",
            " 40% 79999/200000 [2:37:23<3:54:01,  8.55it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/080000.tar\n",
            "[TRAIN] Iter: 80000 Loss: 0.018407907336950302  PSNR: 20.736907958984375\n",
            "[TRAIN] Iter: 80100 Loss: 0.0080029321834445  PSNR: 24.775423049926758\n",
            "[TRAIN] Iter: 80200 Loss: 0.018761616200208664  PSNR: 20.828821182250977\n",
            "[TRAIN] Iter: 80300 Loss: 0.01411750540137291  PSNR: 22.056642532348633\n",
            "[TRAIN] Iter: 80400 Loss: 0.00913950428366661  PSNR: 24.4121150970459\n",
            "[TRAIN] Iter: 80500 Loss: 0.009857716038823128  PSNR: 23.759952545166016\n",
            "[TRAIN] Iter: 80600 Loss: 0.011874904856085777  PSNR: 22.58660316467285\n",
            "[TRAIN] Iter: 80700 Loss: 0.017991118133068085  PSNR: 21.0074520111084\n",
            "[TRAIN] Iter: 80800 Loss: 0.010216022841632366  PSNR: 23.658336639404297\n",
            "[TRAIN] Iter: 80900 Loss: 0.01200194377452135  PSNR: 22.52052116394043\n",
            "[TRAIN] Iter: 81000 Loss: 0.011213832534849644  PSNR: 22.971256256103516\n",
            "[TRAIN] Iter: 81100 Loss: 0.01104920357465744  PSNR: 22.96024513244629\n",
            "[TRAIN] Iter: 81200 Loss: 0.015308360569179058  PSNR: 22.09451675415039\n",
            "[TRAIN] Iter: 81300 Loss: 0.009619902819395065  PSNR: 23.850650787353516\n",
            "[TRAIN] Iter: 81400 Loss: 0.009151777252554893  PSNR: 23.856111526489258\n",
            "[TRAIN] Iter: 81500 Loss: 0.008061617612838745  PSNR: 23.89670181274414\n",
            "[TRAIN] Iter: 81600 Loss: 0.009822054766118526  PSNR: 23.125186920166016\n",
            "[TRAIN] Iter: 81700 Loss: 0.010946878232061863  PSNR: 23.1646785736084\n",
            "[TRAIN] Iter: 81800 Loss: 0.012480455450713634  PSNR: 22.816917419433594\n",
            "[TRAIN] Iter: 81900 Loss: 0.009487858973443508  PSNR: 23.70470428466797\n",
            "[TRAIN] Iter: 82000 Loss: 0.01885535940527916  PSNR: 20.432218551635742\n",
            "[TRAIN] Iter: 82100 Loss: 0.014714549295604229  PSNR: 21.71783447265625\n",
            "[TRAIN] Iter: 82200 Loss: 0.018602387979626656  PSNR: 20.755666732788086\n",
            "[TRAIN] Iter: 82300 Loss: 0.015758927911520004  PSNR: 21.477758407592773\n",
            "[TRAIN] Iter: 82400 Loss: 0.015027442947030067  PSNR: 21.455568313598633\n",
            "[TRAIN] Iter: 82500 Loss: 0.011201037093997002  PSNR: 22.311580657958984\n",
            "[TRAIN] Iter: 82600 Loss: 0.016549397259950638  PSNR: 21.48232078552246\n",
            "[TRAIN] Iter: 82700 Loss: 0.01019038911908865  PSNR: 22.94272232055664\n",
            "[TRAIN] Iter: 82800 Loss: 0.0094912089407444  PSNR: 24.36204719543457\n",
            "[TRAIN] Iter: 82900 Loss: 0.014271331951022148  PSNR: 21.84040069580078\n",
            "[TRAIN] Iter: 83000 Loss: 0.01125417836010456  PSNR: 23.073123931884766\n",
            "[TRAIN] Iter: 83100 Loss: 0.013912588357925415  PSNR: 21.94875144958496\n",
            "[TRAIN] Iter: 83200 Loss: 0.015495803207159042  PSNR: 21.86762809753418\n",
            "[TRAIN] Iter: 83300 Loss: 0.00987827219069004  PSNR: 23.68826675415039\n",
            "[TRAIN] Iter: 83400 Loss: 0.012202469632029533  PSNR: 22.548681259155273\n",
            "[TRAIN] Iter: 83500 Loss: 0.013746513985097408  PSNR: 21.77011489868164\n",
            "[TRAIN] Iter: 83600 Loss: 0.01333070918917656  PSNR: 22.278987884521484\n",
            "[TRAIN] Iter: 83700 Loss: 0.0076567623764276505  PSNR: 24.524370193481445\n",
            "[TRAIN] Iter: 83800 Loss: 0.012086700648069382  PSNR: 22.677541732788086\n",
            "[TRAIN] Iter: 83900 Loss: 0.009558530524373055  PSNR: 23.545263290405273\n",
            "[TRAIN] Iter: 84000 Loss: 0.008656933903694153  PSNR: 24.296110153198242\n",
            "[TRAIN] Iter: 84100 Loss: 0.0178103968501091  PSNR: 21.09064292907715\n",
            "[TRAIN] Iter: 84200 Loss: 0.009509248659014702  PSNR: 23.465774536132812\n",
            "[TRAIN] Iter: 84300 Loss: 0.011520463973283768  PSNR: 22.8295841217041\n",
            "[TRAIN] Iter: 84400 Loss: 0.011342393234372139  PSNR: 22.776363372802734\n",
            "[TRAIN] Iter: 84500 Loss: 0.00991102121770382  PSNR: 23.2443790435791\n",
            "[TRAIN] Iter: 84600 Loss: 0.011210989207029343  PSNR: 24.070491790771484\n",
            "[TRAIN] Iter: 84700 Loss: 0.010048932395875454  PSNR: 23.444345474243164\n",
            "[TRAIN] Iter: 84800 Loss: 0.01173941045999527  PSNR: 22.836868286132812\n",
            "[TRAIN] Iter: 84900 Loss: 0.010179627686738968  PSNR: 23.995038986206055\n",
            "[TRAIN] Iter: 85000 Loss: 0.008820400573313236  PSNR: 24.500574111938477\n",
            "[TRAIN] Iter: 85100 Loss: 0.014544617384672165  PSNR: 21.732515335083008\n",
            "[TRAIN] Iter: 85200 Loss: 0.008037392050027847  PSNR: 24.286746978759766\n",
            "[TRAIN] Iter: 85300 Loss: 0.012076946906745434  PSNR: 22.252635955810547\n",
            "[TRAIN] Iter: 85400 Loss: 0.00950714386999607  PSNR: 24.26238441467285\n",
            "[TRAIN] Iter: 85500 Loss: 0.012324467301368713  PSNR: 22.747509002685547\n",
            "[TRAIN] Iter: 85600 Loss: 0.016806619241833687  PSNR: 21.566455841064453\n",
            "[TRAIN] Iter: 85700 Loss: 0.010414522141218185  PSNR: 23.227319717407227\n",
            "[TRAIN] Iter: 85800 Loss: 0.012104777619242668  PSNR: 22.502201080322266\n",
            "[TRAIN] Iter: 85900 Loss: 0.007373563013970852  PSNR: 24.493188858032227\n",
            "[TRAIN] Iter: 86000 Loss: 0.013354133814573288  PSNR: 22.161914825439453\n",
            "[TRAIN] Iter: 86100 Loss: 0.020024511963129044  PSNR: 20.337438583374023\n",
            "[TRAIN] Iter: 86200 Loss: 0.01663660630583763  PSNR: 21.230525970458984\n",
            "[TRAIN] Iter: 86300 Loss: 0.006447456777095795  PSNR: 25.629709243774414\n",
            "[TRAIN] Iter: 86400 Loss: 0.010637281462550163  PSNR: 23.233789443969727\n",
            "[TRAIN] Iter: 86500 Loss: 0.015768952667713165  PSNR: 21.062129974365234\n",
            "[TRAIN] Iter: 86600 Loss: 0.013968427665531635  PSNR: 21.981962203979492\n",
            "[TRAIN] Iter: 86700 Loss: 0.013383989222347736  PSNR: 22.20974349975586\n",
            "[TRAIN] Iter: 86800 Loss: 0.013910802081227303  PSNR: 21.884361267089844\n",
            "[TRAIN] Iter: 86900 Loss: 0.008278310298919678  PSNR: 24.260128021240234\n",
            "[TRAIN] Iter: 87000 Loss: 0.013602110557258129  PSNR: 22.047985076904297\n",
            "[TRAIN] Iter: 87100 Loss: 0.007698386441916227  PSNR: 24.971345901489258\n",
            "[TRAIN] Iter: 87200 Loss: 0.01397260557860136  PSNR: 22.072397232055664\n",
            "[TRAIN] Iter: 87300 Loss: 0.013728831894695759  PSNR: 21.873172760009766\n",
            "[TRAIN] Iter: 87400 Loss: 0.011832993477582932  PSNR: 22.62539291381836\n",
            "[TRAIN] Iter: 87500 Loss: 0.008589810691773891  PSNR: 24.142498016357422\n",
            "[TRAIN] Iter: 87600 Loss: 0.01324516162276268  PSNR: 21.784948348999023\n",
            "[TRAIN] Iter: 87700 Loss: 0.013919772580265999  PSNR: 22.03539276123047\n",
            "[TRAIN] Iter: 87800 Loss: 0.016071753576397896  PSNR: 21.606359481811523\n",
            "[TRAIN] Iter: 87900 Loss: 0.013108814135193825  PSNR: 22.34075164794922\n",
            "[TRAIN] Iter: 88000 Loss: 0.014940984547138214  PSNR: 21.944902420043945\n",
            "[TRAIN] Iter: 88100 Loss: 0.0122269531711936  PSNR: 22.723901748657227\n",
            "[TRAIN] Iter: 88200 Loss: 0.013785840012133121  PSNR: 22.146392822265625\n",
            "[TRAIN] Iter: 88300 Loss: 0.010572697967290878  PSNR: 23.008697509765625\n",
            "[TRAIN] Iter: 88400 Loss: 0.015498754568397999  PSNR: 21.60956382751465\n",
            "[TRAIN] Iter: 88500 Loss: 0.012423437088727951  PSNR: 22.753036499023438\n",
            "[TRAIN] Iter: 88600 Loss: 0.010211966931819916  PSNR: 23.67283821105957\n",
            "[TRAIN] Iter: 88700 Loss: 0.008702910505235195  PSNR: 23.728328704833984\n",
            "[TRAIN] Iter: 88800 Loss: 0.014928992837667465  PSNR: 21.505252838134766\n",
            "[TRAIN] Iter: 88900 Loss: 0.014840618707239628  PSNR: 21.746896743774414\n",
            "[TRAIN] Iter: 89000 Loss: 0.014291432686150074  PSNR: 21.888206481933594\n",
            "[TRAIN] Iter: 89100 Loss: 0.009360571391880512  PSNR: 24.17859649658203\n",
            "[TRAIN] Iter: 89200 Loss: 0.018909018486738205  PSNR: 20.693662643432617\n",
            "[TRAIN] Iter: 89300 Loss: 0.008218953385949135  PSNR: 24.786941528320312\n",
            "[TRAIN] Iter: 89400 Loss: 0.013025825843214989  PSNR: 22.02465057373047\n",
            "[TRAIN] Iter: 89500 Loss: 0.011322729289531708  PSNR: 22.730365753173828\n",
            "[TRAIN] Iter: 89600 Loss: 0.01507626473903656  PSNR: 21.915443420410156\n",
            "[TRAIN] Iter: 89700 Loss: 0.012621236965060234  PSNR: 22.724733352661133\n",
            "[TRAIN] Iter: 89800 Loss: 0.01030623447149992  PSNR: 23.555583953857422\n",
            "[TRAIN] Iter: 89900 Loss: 0.009715244174003601  PSNR: 23.70293426513672\n",
            " 45% 89999/200000 [2:56:59<3:34:18,  8.56it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/090000.tar\n",
            "[TRAIN] Iter: 90000 Loss: 0.012070360593497753  PSNR: 22.776599884033203\n",
            "[TRAIN] Iter: 90100 Loss: 0.008723622187972069  PSNR: 24.017906188964844\n",
            "[TRAIN] Iter: 90200 Loss: 0.010497936978936195  PSNR: 23.524974822998047\n",
            "[TRAIN] Iter: 90300 Loss: 0.013956334441900253  PSNR: 21.802316665649414\n",
            "[TRAIN] Iter: 90400 Loss: 0.00921179074794054  PSNR: 23.421937942504883\n",
            "[TRAIN] Iter: 90500 Loss: 0.01622549071907997  PSNR: 21.030763626098633\n",
            "[TRAIN] Iter: 90600 Loss: 0.01212594099342823  PSNR: 22.523794174194336\n",
            "[TRAIN] Iter: 90700 Loss: 0.011783680878579617  PSNR: 22.737585067749023\n",
            "[TRAIN] Iter: 90800 Loss: 0.011659372597932816  PSNR: 22.534883499145508\n",
            "[TRAIN] Iter: 90900 Loss: 0.015749480575323105  PSNR: 21.51146125793457\n",
            "[TRAIN] Iter: 91000 Loss: 0.011588215827941895  PSNR: 22.84501075744629\n",
            "[TRAIN] Iter: 91100 Loss: 0.009822601452469826  PSNR: 23.766860961914062\n",
            "[TRAIN] Iter: 91200 Loss: 0.00998761411756277  PSNR: 23.544504165649414\n",
            "[TRAIN] Iter: 91300 Loss: 0.010210860520601273  PSNR: 23.482147216796875\n",
            "[TRAIN] Iter: 91400 Loss: 0.008609026670455933  PSNR: 24.372806549072266\n",
            "[TRAIN] Iter: 91500 Loss: 0.01538856141269207  PSNR: 21.331493377685547\n",
            "[TRAIN] Iter: 91600 Loss: 0.007574752904474735  PSNR: 24.306377410888672\n",
            "[TRAIN] Iter: 91700 Loss: 0.008955896832048893  PSNR: 24.28624725341797\n",
            "[TRAIN] Iter: 91800 Loss: 0.006189687643200159  PSNR: 25.772560119628906\n",
            "[TRAIN] Iter: 91900 Loss: 0.019833602011203766  PSNR: 20.070056915283203\n",
            "[TRAIN] Iter: 92000 Loss: 0.008083032444119453  PSNR: 24.574951171875\n",
            "[TRAIN] Iter: 92100 Loss: 0.014945496805012226  PSNR: 21.613069534301758\n",
            "[TRAIN] Iter: 92200 Loss: 0.009506224654614925  PSNR: 23.78070640563965\n",
            "[TRAIN] Iter: 92300 Loss: 0.010625890456140041  PSNR: 23.3536319732666\n",
            "[TRAIN] Iter: 92400 Loss: 0.012654362246394157  PSNR: 22.78289031982422\n",
            "[TRAIN] Iter: 92500 Loss: 0.01923486590385437  PSNR: 19.91664695739746\n",
            "[TRAIN] Iter: 92600 Loss: 0.009663511998951435  PSNR: 23.569252014160156\n",
            "[TRAIN] Iter: 92700 Loss: 0.01224456261843443  PSNR: 22.584842681884766\n",
            "[TRAIN] Iter: 92800 Loss: 0.012581874616444111  PSNR: 22.44728660583496\n",
            "[TRAIN] Iter: 92900 Loss: 0.013178740628063679  PSNR: 21.70091438293457\n",
            "[TRAIN] Iter: 93000 Loss: 0.017145052552223206  PSNR: 20.867549896240234\n",
            "[TRAIN] Iter: 93100 Loss: 0.017039567232131958  PSNR: 21.214757919311523\n",
            "[TRAIN] Iter: 93200 Loss: 0.013924827799201012  PSNR: 22.77842903137207\n",
            "[TRAIN] Iter: 93300 Loss: 0.006898939143866301  PSNR: 25.29430389404297\n",
            "[TRAIN] Iter: 93400 Loss: 0.01178576797246933  PSNR: 22.255319595336914\n",
            "[TRAIN] Iter: 93500 Loss: 0.01419022586196661  PSNR: 21.815216064453125\n",
            "[TRAIN] Iter: 93600 Loss: 0.017485614866018295  PSNR: 20.340490341186523\n",
            "[TRAIN] Iter: 93700 Loss: 0.019056638702750206  PSNR: 20.98329734802246\n",
            "[TRAIN] Iter: 93800 Loss: 0.012151253409683704  PSNR: 23.34789276123047\n",
            "[TRAIN] Iter: 93900 Loss: 0.008838635869324207  PSNR: 24.11029815673828\n",
            "[TRAIN] Iter: 94000 Loss: 0.012602601200342178  PSNR: 22.35739517211914\n",
            "[TRAIN] Iter: 94100 Loss: 0.008518451824784279  PSNR: 23.91036605834961\n",
            "[TRAIN] Iter: 94200 Loss: 0.011263405904173851  PSNR: 23.595861434936523\n",
            "[TRAIN] Iter: 94300 Loss: 0.01221543364226818  PSNR: 22.40940284729004\n",
            "[TRAIN] Iter: 94400 Loss: 0.006156753282994032  PSNR: 26.238018035888672\n",
            "[TRAIN] Iter: 94500 Loss: 0.008375621400773525  PSNR: 24.204755783081055\n",
            "[TRAIN] Iter: 94600 Loss: 0.014807247556746006  PSNR: 21.320322036743164\n",
            "[TRAIN] Iter: 94700 Loss: 0.01522444374859333  PSNR: 21.316205978393555\n",
            "[TRAIN] Iter: 94800 Loss: 0.008370417170226574  PSNR: 23.98355484008789\n",
            "[TRAIN] Iter: 94900 Loss: 0.009090487845242023  PSNR: 23.402667999267578\n",
            "[TRAIN] Iter: 95000 Loss: 0.009056061506271362  PSNR: 23.175960540771484\n",
            "[TRAIN] Iter: 95100 Loss: 0.015045862644910812  PSNR: 21.41748809814453\n",
            "[TRAIN] Iter: 95200 Loss: 0.012749392539262772  PSNR: 22.08864402770996\n",
            "[TRAIN] Iter: 95300 Loss: 0.011987071484327316  PSNR: 22.83470344543457\n",
            "[TRAIN] Iter: 95400 Loss: 0.015044573694467545  PSNR: 22.190195083618164\n",
            "[TRAIN] Iter: 95500 Loss: 0.010016724467277527  PSNR: 23.200754165649414\n",
            "[TRAIN] Iter: 95600 Loss: 0.00658155232667923  PSNR: 25.89887046813965\n",
            "[TRAIN] Iter: 95700 Loss: 0.01939302682876587  PSNR: 20.122238159179688\n",
            "[TRAIN] Iter: 95800 Loss: 0.01041974313557148  PSNR: 23.288759231567383\n",
            "[TRAIN] Iter: 95900 Loss: 0.013187864795327187  PSNR: 22.228137969970703\n",
            "[TRAIN] Iter: 96000 Loss: 0.013546016067266464  PSNR: 22.10298728942871\n",
            "[TRAIN] Iter: 96100 Loss: 0.010399211198091507  PSNR: 24.49785804748535\n",
            "[TRAIN] Iter: 96200 Loss: 0.009381534531712532  PSNR: 24.07987403869629\n",
            "[TRAIN] Iter: 96300 Loss: 0.01253151148557663  PSNR: 22.55663299560547\n",
            "[TRAIN] Iter: 96400 Loss: 0.01529763638973236  PSNR: 21.74162483215332\n",
            "[TRAIN] Iter: 96500 Loss: 0.011223847977817059  PSNR: 22.669715881347656\n",
            "[TRAIN] Iter: 96600 Loss: 0.009782489389181137  PSNR: 23.491073608398438\n",
            "[TRAIN] Iter: 96700 Loss: 0.014006981626152992  PSNR: 21.67983627319336\n",
            "[TRAIN] Iter: 96800 Loss: 0.014710490591824055  PSNR: 21.308055877685547\n",
            "[TRAIN] Iter: 96900 Loss: 0.011387726292014122  PSNR: 22.667661666870117\n",
            "[TRAIN] Iter: 97000 Loss: 0.01268140971660614  PSNR: 22.233179092407227\n",
            "[TRAIN] Iter: 97100 Loss: 0.011744622141122818  PSNR: 22.480504989624023\n",
            "[TRAIN] Iter: 97200 Loss: 0.009806206449866295  PSNR: 23.25385093688965\n",
            "[TRAIN] Iter: 97300 Loss: 0.0113417599350214  PSNR: 22.686559677124023\n",
            "[TRAIN] Iter: 97400 Loss: 0.010097436606884003  PSNR: 23.300487518310547\n",
            "[TRAIN] Iter: 97500 Loss: 0.008981454186141491  PSNR: 24.01094627380371\n",
            "[TRAIN] Iter: 97600 Loss: 0.015904124826192856  PSNR: 20.56348419189453\n",
            "[TRAIN] Iter: 97700 Loss: 0.013535581529140472  PSNR: 21.781469345092773\n",
            "[TRAIN] Iter: 97800 Loss: 0.010010325349867344  PSNR: 23.77828025817871\n",
            "[TRAIN] Iter: 97900 Loss: 0.01060774177312851  PSNR: 22.707834243774414\n",
            "[TRAIN] Iter: 98000 Loss: 0.012134653516113758  PSNR: 22.439632415771484\n",
            "[TRAIN] Iter: 98100 Loss: 0.012226096354424953  PSNR: 22.018169403076172\n",
            "[TRAIN] Iter: 98200 Loss: 0.011004339903593063  PSNR: 23.130319595336914\n",
            "[TRAIN] Iter: 98300 Loss: 0.011811396107077599  PSNR: 23.076526641845703\n",
            "[TRAIN] Iter: 98400 Loss: 0.00875852257013321  PSNR: 23.594968795776367\n",
            "[TRAIN] Iter: 98500 Loss: 0.008325180038809776  PSNR: 25.105953216552734\n",
            "[TRAIN] Iter: 98600 Loss: 0.01035652868449688  PSNR: 23.21051025390625\n",
            "[TRAIN] Iter: 98700 Loss: 0.014550781808793545  PSNR: 21.859539031982422\n",
            "[TRAIN] Iter: 98800 Loss: 0.013338891789317131  PSNR: 22.512392044067383\n",
            "[TRAIN] Iter: 98900 Loss: 0.008516823872923851  PSNR: 24.285083770751953\n",
            "[TRAIN] Iter: 99000 Loss: 0.01038342248648405  PSNR: 23.728879928588867\n",
            "[TRAIN] Iter: 99100 Loss: 0.012000657618045807  PSNR: 23.06890296936035\n",
            "[TRAIN] Iter: 99200 Loss: 0.012904293835163116  PSNR: 23.335859298706055\n",
            "[TRAIN] Iter: 99300 Loss: 0.011865833774209023  PSNR: 23.007204055786133\n",
            "[TRAIN] Iter: 99400 Loss: 0.009936024434864521  PSNR: 23.058195114135742\n",
            "[TRAIN] Iter: 99500 Loss: 0.01240314170718193  PSNR: 22.929941177368164\n",
            "[TRAIN] Iter: 99600 Loss: 0.012296253815293312  PSNR: 22.947622299194336\n",
            "[TRAIN] Iter: 99700 Loss: 0.011928318999707699  PSNR: 23.106218338012695\n",
            "[TRAIN] Iter: 99800 Loss: 0.01814080774784088  PSNR: 20.76446533203125\n",
            "[TRAIN] Iter: 99900 Loss: 0.009964212775230408  PSNR: 22.906587600708008\n",
            " 50% 99999/200000 [3:16:31<3:14:50,  8.55it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/100000.tar\n",
            "[TRAIN] Iter: 100000 Loss: 0.015821412205696106  PSNR: 21.384798049926758\n",
            "[TRAIN] Iter: 100100 Loss: 0.016226066276431084  PSNR: 21.347942352294922\n",
            "[TRAIN] Iter: 100200 Loss: 0.0073994966223835945  PSNR: 24.899019241333008\n",
            "[TRAIN] Iter: 100300 Loss: 0.01105650793761015  PSNR: 22.722139358520508\n",
            "[TRAIN] Iter: 100400 Loss: 0.01565193198621273  PSNR: 21.286333084106445\n",
            "[TRAIN] Iter: 100500 Loss: 0.01043897308409214  PSNR: 23.06184196472168\n",
            "[TRAIN] Iter: 100600 Loss: 0.00971968937665224  PSNR: 23.37688636779785\n",
            "[TRAIN] Iter: 100700 Loss: 0.011290487833321095  PSNR: 22.917856216430664\n",
            "[TRAIN] Iter: 100800 Loss: 0.015308994799852371  PSNR: 21.86422348022461\n",
            "[TRAIN] Iter: 100900 Loss: 0.016394346952438354  PSNR: 21.556598663330078\n",
            "[TRAIN] Iter: 101000 Loss: 0.012421006336808205  PSNR: 22.80009651184082\n",
            "[TRAIN] Iter: 101100 Loss: 0.011089086532592773  PSNR: 23.47941780090332\n",
            "[TRAIN] Iter: 101200 Loss: 0.011369944550096989  PSNR: 22.697145462036133\n",
            "[TRAIN] Iter: 101300 Loss: 0.014970995485782623  PSNR: 21.454092025756836\n",
            "[TRAIN] Iter: 101400 Loss: 0.009412732906639576  PSNR: 23.805618286132812\n",
            "[TRAIN] Iter: 101500 Loss: 0.009623688645660877  PSNR: 23.269969940185547\n",
            "[TRAIN] Iter: 101600 Loss: 0.010792810469865799  PSNR: 23.338729858398438\n",
            "[TRAIN] Iter: 101700 Loss: 0.012785196304321289  PSNR: 22.482397079467773\n",
            "[TRAIN] Iter: 101800 Loss: 0.011797478422522545  PSNR: 22.43556785583496\n",
            "[TRAIN] Iter: 101900 Loss: 0.012899591587483883  PSNR: 21.911972045898438\n",
            "[TRAIN] Iter: 102000 Loss: 0.009422189556062222  PSNR: 23.8262996673584\n",
            "[TRAIN] Iter: 102100 Loss: 0.01141761988401413  PSNR: 23.00185775756836\n",
            "[TRAIN] Iter: 102200 Loss: 0.013723943382501602  PSNR: 22.11090660095215\n",
            "[TRAIN] Iter: 102300 Loss: 0.00914371944963932  PSNR: 24.292516708374023\n",
            "[TRAIN] Iter: 102400 Loss: 0.00835028663277626  PSNR: 24.517183303833008\n",
            "[TRAIN] Iter: 102500 Loss: 0.01531909964978695  PSNR: 21.01713752746582\n",
            "[TRAIN] Iter: 102600 Loss: 0.013698716647922993  PSNR: 22.038005828857422\n",
            "[TRAIN] Iter: 102700 Loss: 0.005824281834065914  PSNR: 25.245716094970703\n",
            "[TRAIN] Iter: 102800 Loss: 0.008324585855007172  PSNR: 24.41551971435547\n",
            "[TRAIN] Iter: 102900 Loss: 0.011456654407083988  PSNR: 22.51415252685547\n",
            "[TRAIN] Iter: 103000 Loss: 0.008448518812656403  PSNR: 24.229251861572266\n",
            "[TRAIN] Iter: 103100 Loss: 0.012384627014398575  PSNR: 22.007001876831055\n",
            "[TRAIN] Iter: 103200 Loss: 0.014254244044423103  PSNR: 21.774885177612305\n",
            "[TRAIN] Iter: 103300 Loss: 0.01196132879704237  PSNR: 22.561756134033203\n",
            "[TRAIN] Iter: 103400 Loss: 0.014549928717315197  PSNR: 21.476078033447266\n",
            "[TRAIN] Iter: 103500 Loss: 0.01238594762980938  PSNR: 22.724252700805664\n",
            "[TRAIN] Iter: 103600 Loss: 0.008737658150494099  PSNR: 24.20235824584961\n",
            "[TRAIN] Iter: 103700 Loss: 0.01275327242910862  PSNR: 22.154376983642578\n",
            "[TRAIN] Iter: 103800 Loss: 0.014120643958449364  PSNR: 21.896875381469727\n",
            "[TRAIN] Iter: 103900 Loss: 0.015644757077097893  PSNR: 21.010988235473633\n",
            "[TRAIN] Iter: 104000 Loss: 0.011357488110661507  PSNR: 22.524417877197266\n",
            "[TRAIN] Iter: 104100 Loss: 0.010995038785040379  PSNR: 23.098377227783203\n",
            "[TRAIN] Iter: 104200 Loss: 0.005389691796153784  PSNR: 26.2485408782959\n",
            "[TRAIN] Iter: 104300 Loss: 0.011981243267655373  PSNR: 21.93207550048828\n",
            "[TRAIN] Iter: 104400 Loss: 0.009538889862596989  PSNR: 23.72113800048828\n",
            "[TRAIN] Iter: 104500 Loss: 0.013098148629069328  PSNR: 22.29778289794922\n",
            "[TRAIN] Iter: 104600 Loss: 0.012217728421092033  PSNR: 22.391910552978516\n",
            "[TRAIN] Iter: 104700 Loss: 0.008526732213795185  PSNR: 23.846454620361328\n",
            "[TRAIN] Iter: 104800 Loss: 0.012357322499155998  PSNR: 22.51911735534668\n",
            "[TRAIN] Iter: 104900 Loss: 0.010985823348164558  PSNR: 22.827306747436523\n",
            "[TRAIN] Iter: 105000 Loss: 0.018975717946887016  PSNR: 20.353382110595703\n",
            "[TRAIN] Iter: 105100 Loss: 0.010632440447807312  PSNR: 23.260339736938477\n",
            "[TRAIN] Iter: 105200 Loss: 0.012704174034297466  PSNR: 22.736722946166992\n",
            "[TRAIN] Iter: 105300 Loss: 0.009540131315588951  PSNR: 23.365209579467773\n",
            "[TRAIN] Iter: 105400 Loss: 0.013956909999251366  PSNR: 21.87005615234375\n",
            "[TRAIN] Iter: 105500 Loss: 0.0171296875923872  PSNR: 20.87855339050293\n",
            "[TRAIN] Iter: 105600 Loss: 0.010755620896816254  PSNR: 22.775922775268555\n",
            "[TRAIN] Iter: 105700 Loss: 0.009951261803507805  PSNR: 23.796239852905273\n",
            "[TRAIN] Iter: 105800 Loss: 0.009589741006493568  PSNR: 23.52665138244629\n",
            "[TRAIN] Iter: 105900 Loss: 0.012019127607345581  PSNR: 22.192352294921875\n",
            "[TRAIN] Iter: 106000 Loss: 0.010827983729541302  PSNR: 23.095237731933594\n",
            "[TRAIN] Iter: 106100 Loss: 0.014641586691141129  PSNR: 21.969091415405273\n",
            "[TRAIN] Iter: 106200 Loss: 0.012300041504204273  PSNR: 22.05217170715332\n",
            "[TRAIN] Iter: 106300 Loss: 0.008508636616170406  PSNR: 24.56580924987793\n",
            "[TRAIN] Iter: 106400 Loss: 0.01544805895537138  PSNR: 21.91288948059082\n",
            "[TRAIN] Iter: 106500 Loss: 0.0187055841088295  PSNR: 20.623571395874023\n",
            "[TRAIN] Iter: 106600 Loss: 0.015054089017212391  PSNR: 21.414382934570312\n",
            "[TRAIN] Iter: 106700 Loss: 0.016477279365062714  PSNR: 21.105459213256836\n",
            "[TRAIN] Iter: 106800 Loss: 0.0097845159471035  PSNR: 23.531831741333008\n",
            "[TRAIN] Iter: 106900 Loss: 0.013959350064396858  PSNR: 22.536500930786133\n",
            "[TRAIN] Iter: 107000 Loss: 0.01063787192106247  PSNR: 23.486568450927734\n",
            "[TRAIN] Iter: 107100 Loss: 0.008309816010296345  PSNR: 23.97064208984375\n",
            "[TRAIN] Iter: 107200 Loss: 0.011058412492275238  PSNR: 23.573183059692383\n",
            "[TRAIN] Iter: 107300 Loss: 0.00859559141099453  PSNR: 24.439186096191406\n",
            "[TRAIN] Iter: 107400 Loss: 0.008516819216310978  PSNR: 24.122638702392578\n",
            "[TRAIN] Iter: 107500 Loss: 0.011486462317407131  PSNR: 23.387800216674805\n",
            "[TRAIN] Iter: 107600 Loss: 0.01574675552546978  PSNR: 20.933990478515625\n",
            "[TRAIN] Iter: 107700 Loss: 0.01412928756326437  PSNR: 22.041919708251953\n",
            "[TRAIN] Iter: 107800 Loss: 0.00685432692989707  PSNR: 24.51375389099121\n",
            "[TRAIN] Iter: 107900 Loss: 0.014360813423991203  PSNR: 21.044084548950195\n",
            "[TRAIN] Iter: 108000 Loss: 0.009813751094043255  PSNR: 24.009075164794922\n",
            "[TRAIN] Iter: 108100 Loss: 0.012046491727232933  PSNR: 22.13043785095215\n",
            "[TRAIN] Iter: 108200 Loss: 0.01157410815358162  PSNR: 22.190568923950195\n",
            "[TRAIN] Iter: 108300 Loss: 0.007617746479809284  PSNR: 24.752843856811523\n",
            "[TRAIN] Iter: 108400 Loss: 0.009692460298538208  PSNR: 23.90066909790039\n",
            "[TRAIN] Iter: 108500 Loss: 0.009922121651470661  PSNR: 23.72220230102539\n",
            "[TRAIN] Iter: 108600 Loss: 0.009102649055421352  PSNR: 24.125972747802734\n",
            "[TRAIN] Iter: 108700 Loss: 0.012555163353681564  PSNR: 21.94690704345703\n",
            "[TRAIN] Iter: 108800 Loss: 0.013725938275456429  PSNR: 21.389297485351562\n",
            "[TRAIN] Iter: 108900 Loss: 0.012046171352267265  PSNR: 21.921951293945312\n",
            "[TRAIN] Iter: 109000 Loss: 0.012777598574757576  PSNR: 22.485027313232422\n",
            "[TRAIN] Iter: 109100 Loss: 0.013999886810779572  PSNR: 21.871477127075195\n",
            "[TRAIN] Iter: 109200 Loss: 0.015111956745386124  PSNR: 21.39952278137207\n",
            "[TRAIN] Iter: 109300 Loss: 0.012175246141850948  PSNR: 22.53585433959961\n",
            "[TRAIN] Iter: 109400 Loss: 0.014895999804139137  PSNR: 21.005098342895508\n",
            "[TRAIN] Iter: 109500 Loss: 0.011342501267790794  PSNR: 22.994094848632812\n",
            "[TRAIN] Iter: 109600 Loss: 0.010065823793411255  PSNR: 23.31305694580078\n",
            "[TRAIN] Iter: 109700 Loss: 0.01042588334530592  PSNR: 23.809288024902344\n",
            "[TRAIN] Iter: 109800 Loss: 0.008784933015704155  PSNR: 24.15285301208496\n",
            "[TRAIN] Iter: 109900 Loss: 0.012816616334021091  PSNR: 22.439729690551758\n",
            " 55% 109999/200000 [3:36:06<2:56:27,  8.50it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/110000.tar\n",
            "[TRAIN] Iter: 110000 Loss: 0.016628040000796318  PSNR: 21.20939064025879\n",
            "[TRAIN] Iter: 110100 Loss: 0.012621145695447922  PSNR: 22.83751678466797\n",
            "[TRAIN] Iter: 110200 Loss: 0.012145310640335083  PSNR: 22.644729614257812\n",
            "[TRAIN] Iter: 110300 Loss: 0.01261591911315918  PSNR: 22.721216201782227\n",
            "[TRAIN] Iter: 110400 Loss: 0.011918559670448303  PSNR: 22.490989685058594\n",
            "[TRAIN] Iter: 110500 Loss: 0.01054542139172554  PSNR: 23.197317123413086\n",
            "[TRAIN] Iter: 110600 Loss: 0.009956307709217072  PSNR: 24.348419189453125\n",
            "[TRAIN] Iter: 110700 Loss: 0.00923106074333191  PSNR: 23.630817413330078\n",
            "[TRAIN] Iter: 110800 Loss: 0.01420687511563301  PSNR: 21.82090950012207\n",
            "[TRAIN] Iter: 110900 Loss: 0.0148840993642807  PSNR: 21.679065704345703\n",
            "[TRAIN] Iter: 111000 Loss: 0.011822800152003765  PSNR: 22.819194793701172\n",
            "[TRAIN] Iter: 111100 Loss: 0.011972456239163876  PSNR: 22.77825927734375\n",
            "[TRAIN] Iter: 111200 Loss: 0.011140253394842148  PSNR: 22.260194778442383\n",
            "[TRAIN] Iter: 111300 Loss: 0.008417350240051746  PSNR: 24.49155616760254\n",
            "[TRAIN] Iter: 111400 Loss: 0.011363297700881958  PSNR: 23.397146224975586\n",
            "[TRAIN] Iter: 111500 Loss: 0.01342764776200056  PSNR: 21.904756546020508\n",
            "[TRAIN] Iter: 111600 Loss: 0.016337532550096512  PSNR: 20.990310668945312\n",
            "[TRAIN] Iter: 111700 Loss: 0.008799999952316284  PSNR: 23.978233337402344\n",
            "[TRAIN] Iter: 111800 Loss: 0.01021233294159174  PSNR: 23.925308227539062\n",
            "[TRAIN] Iter: 111900 Loss: 0.008378066122531891  PSNR: 24.248226165771484\n",
            "[TRAIN] Iter: 112000 Loss: 0.01209872867912054  PSNR: 22.823795318603516\n",
            "[TRAIN] Iter: 112100 Loss: 0.013462553732097149  PSNR: 22.027976989746094\n",
            "[TRAIN] Iter: 112200 Loss: 0.008164359256625175  PSNR: 24.316499710083008\n",
            "[TRAIN] Iter: 112300 Loss: 0.013829898089170456  PSNR: 22.021564483642578\n",
            "[TRAIN] Iter: 112400 Loss: 0.009077874943614006  PSNR: 23.7982120513916\n",
            "[TRAIN] Iter: 112500 Loss: 0.006783305201679468  PSNR: 25.934181213378906\n",
            "[TRAIN] Iter: 112600 Loss: 0.012267800979316235  PSNR: 22.725568771362305\n",
            "[TRAIN] Iter: 112700 Loss: 0.008208001963794231  PSNR: 24.269922256469727\n",
            "[TRAIN] Iter: 112800 Loss: 0.009547347202897072  PSNR: 24.19704246520996\n",
            "[TRAIN] Iter: 112900 Loss: 0.009722307324409485  PSNR: 23.07962989807129\n",
            "[TRAIN] Iter: 113000 Loss: 0.01589595153927803  PSNR: 21.76958465576172\n",
            "[TRAIN] Iter: 113100 Loss: 0.01362777128815651  PSNR: 21.765880584716797\n",
            "[TRAIN] Iter: 113200 Loss: 0.012891463935375214  PSNR: 22.3082218170166\n",
            "[TRAIN] Iter: 113300 Loss: 0.008837725035846233  PSNR: 24.185373306274414\n",
            "[TRAIN] Iter: 113400 Loss: 0.014904925599694252  PSNR: 21.951894760131836\n",
            "[TRAIN] Iter: 113500 Loss: 0.00723995640873909  PSNR: 24.558137893676758\n",
            "[TRAIN] Iter: 113600 Loss: 0.011385707184672356  PSNR: 22.7303409576416\n",
            "[TRAIN] Iter: 113700 Loss: 0.01243626605719328  PSNR: 22.07235336303711\n",
            "[TRAIN] Iter: 113800 Loss: 0.013941786251962185  PSNR: 21.85219383239746\n",
            "[TRAIN] Iter: 113900 Loss: 0.00815371423959732  PSNR: 24.752466201782227\n",
            "[TRAIN] Iter: 114000 Loss: 0.01325521431863308  PSNR: 22.08489418029785\n",
            "[TRAIN] Iter: 114100 Loss: 0.01343652792274952  PSNR: 22.157032012939453\n",
            "[TRAIN] Iter: 114200 Loss: 0.012213310226798058  PSNR: 22.599775314331055\n",
            "[TRAIN] Iter: 114300 Loss: 0.009494752623140812  PSNR: 24.110942840576172\n",
            "[TRAIN] Iter: 114400 Loss: 0.012287418358027935  PSNR: 22.786226272583008\n",
            "[TRAIN] Iter: 114500 Loss: 0.009113231673836708  PSNR: 23.692047119140625\n",
            "[TRAIN] Iter: 114600 Loss: 0.008784091100096703  PSNR: 23.939966201782227\n",
            "[TRAIN] Iter: 114700 Loss: 0.01411968469619751  PSNR: 21.667020797729492\n",
            "[TRAIN] Iter: 114800 Loss: 0.008640427142381668  PSNR: 23.95511817932129\n",
            "[TRAIN] Iter: 114900 Loss: 0.013442493043839931  PSNR: 22.1204891204834\n",
            "[TRAIN] Iter: 115000 Loss: 0.009201930835843086  PSNR: 24.013103485107422\n",
            "[TRAIN] Iter: 115100 Loss: 0.012351637706160545  PSNR: 22.560522079467773\n",
            "[TRAIN] Iter: 115200 Loss: 0.009224358014762402  PSNR: 24.44155502319336\n",
            "[TRAIN] Iter: 115300 Loss: 0.013493587262928486  PSNR: 22.32027816772461\n",
            "[TRAIN] Iter: 115400 Loss: 0.013607591390609741  PSNR: 22.0106201171875\n",
            "[TRAIN] Iter: 115500 Loss: 0.01516963355243206  PSNR: 21.22873878479004\n",
            "[TRAIN] Iter: 115600 Loss: 0.012588828802108765  PSNR: 21.961685180664062\n",
            "[TRAIN] Iter: 115700 Loss: 0.01648438349366188  PSNR: 20.765138626098633\n",
            "[TRAIN] Iter: 115800 Loss: 0.014079171232879162  PSNR: 21.6070556640625\n",
            "[TRAIN] Iter: 115900 Loss: 0.011538836173713207  PSNR: 22.50927734375\n",
            "[TRAIN] Iter: 116000 Loss: 0.0070429397746920586  PSNR: 24.985687255859375\n",
            "[TRAIN] Iter: 116100 Loss: 0.011577066965401173  PSNR: 22.72062110900879\n",
            "[TRAIN] Iter: 116200 Loss: 0.009934056550264359  PSNR: 23.199777603149414\n",
            "[TRAIN] Iter: 116300 Loss: 0.015211970545351505  PSNR: 21.86846351623535\n",
            "[TRAIN] Iter: 116400 Loss: 0.010166285559535027  PSNR: 23.620468139648438\n",
            "[TRAIN] Iter: 116500 Loss: 0.008960183709859848  PSNR: 23.387065887451172\n",
            "[TRAIN] Iter: 116600 Loss: 0.01420782133936882  PSNR: 21.0125789642334\n",
            "[TRAIN] Iter: 116700 Loss: 0.011956270784139633  PSNR: 22.186817169189453\n",
            "[TRAIN] Iter: 116800 Loss: 0.009319356642663479  PSNR: 23.349084854125977\n",
            "[TRAIN] Iter: 116900 Loss: 0.010077578946948051  PSNR: 23.320068359375\n",
            "[TRAIN] Iter: 117000 Loss: 0.011698992922902107  PSNR: 23.04487419128418\n",
            "[TRAIN] Iter: 117100 Loss: 0.017663272097706795  PSNR: 20.953718185424805\n",
            "[TRAIN] Iter: 117200 Loss: 0.014541354030370712  PSNR: 21.85126495361328\n",
            "[TRAIN] Iter: 117300 Loss: 0.0140202222391963  PSNR: 21.638248443603516\n",
            "[TRAIN] Iter: 117400 Loss: 0.014142575673758984  PSNR: 21.937908172607422\n",
            "[TRAIN] Iter: 117500 Loss: 0.008871816098690033  PSNR: 24.02681541442871\n",
            "[TRAIN] Iter: 117600 Loss: 0.0076849269680678844  PSNR: 24.830387115478516\n",
            "[TRAIN] Iter: 117700 Loss: 0.011367928236722946  PSNR: 22.372303009033203\n",
            "[TRAIN] Iter: 117800 Loss: 0.008299456909298897  PSNR: 24.986434936523438\n",
            "[TRAIN] Iter: 117900 Loss: 0.01434238813817501  PSNR: 21.830734252929688\n",
            "[TRAIN] Iter: 118000 Loss: 0.012925805523991585  PSNR: 22.956275939941406\n",
            "[TRAIN] Iter: 118100 Loss: 0.00742379343137145  PSNR: 24.4444637298584\n",
            "[TRAIN] Iter: 118200 Loss: 0.011040724813938141  PSNR: 23.026187896728516\n",
            "[TRAIN] Iter: 118300 Loss: 0.009489799849689007  PSNR: 23.712862014770508\n",
            "[TRAIN] Iter: 118400 Loss: 0.01061866246163845  PSNR: 22.862220764160156\n",
            "[TRAIN] Iter: 118500 Loss: 0.012113736942410469  PSNR: 22.626094818115234\n",
            "[TRAIN] Iter: 118600 Loss: 0.008522413671016693  PSNR: 24.141672134399414\n",
            "[TRAIN] Iter: 118700 Loss: 0.010219436138868332  PSNR: 24.661182403564453\n",
            "[TRAIN] Iter: 118800 Loss: 0.008660824969410896  PSNR: 24.395185470581055\n",
            "[TRAIN] Iter: 118900 Loss: 0.013111615553498268  PSNR: 22.372533798217773\n",
            "[TRAIN] Iter: 119000 Loss: 0.012311962433159351  PSNR: 22.401641845703125\n",
            "[TRAIN] Iter: 119100 Loss: 0.011679952964186668  PSNR: 22.727479934692383\n",
            "[TRAIN] Iter: 119200 Loss: 0.006546406541019678  PSNR: 25.49503517150879\n",
            "[TRAIN] Iter: 119300 Loss: 0.011354705318808556  PSNR: 23.00874900817871\n",
            "[TRAIN] Iter: 119400 Loss: 0.010791653767228127  PSNR: 23.00150489807129\n",
            "[TRAIN] Iter: 119500 Loss: 0.010447262786328793  PSNR: 23.367307662963867\n",
            "[TRAIN] Iter: 119600 Loss: 0.00833978597074747  PSNR: 24.29518699645996\n",
            "[TRAIN] Iter: 119700 Loss: 0.012737477198243141  PSNR: 23.010934829711914\n",
            "[TRAIN] Iter: 119800 Loss: 0.01124511193484068  PSNR: 22.75160026550293\n",
            "[TRAIN] Iter: 119900 Loss: 0.010668456554412842  PSNR: 23.37165069580078\n",
            " 60% 119999/200000 [3:55:37<2:33:59,  8.66it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/120000.tar\n",
            "[TRAIN] Iter: 120000 Loss: 0.011783603578805923  PSNR: 23.084936141967773\n",
            "[TRAIN] Iter: 120100 Loss: 0.01146712526679039  PSNR: 23.15489387512207\n",
            "[TRAIN] Iter: 120200 Loss: 0.009814076125621796  PSNR: 23.77350425720215\n",
            "[TRAIN] Iter: 120300 Loss: 0.012594342231750488  PSNR: 22.813594818115234\n",
            "[TRAIN] Iter: 120400 Loss: 0.007210116367787123  PSNR: 25.15228843688965\n",
            "[TRAIN] Iter: 120500 Loss: 0.009472654201090336  PSNR: 23.449180603027344\n",
            "[TRAIN] Iter: 120600 Loss: 0.006714243441820145  PSNR: 25.541126251220703\n",
            "[TRAIN] Iter: 120700 Loss: 0.009292781352996826  PSNR: 24.10972785949707\n",
            "[TRAIN] Iter: 120800 Loss: 0.010879849083721638  PSNR: 22.910329818725586\n",
            "[TRAIN] Iter: 120900 Loss: 0.012649189680814743  PSNR: 22.1511173248291\n",
            "[TRAIN] Iter: 121000 Loss: 0.007915028370916843  PSNR: 24.841083526611328\n",
            "[TRAIN] Iter: 121100 Loss: 0.009383870288729668  PSNR: 24.171398162841797\n",
            "[TRAIN] Iter: 121200 Loss: 0.007674884982407093  PSNR: 24.970245361328125\n",
            "[TRAIN] Iter: 121300 Loss: 0.007568460423499346  PSNR: 24.501514434814453\n",
            "[TRAIN] Iter: 121400 Loss: 0.00970187596976757  PSNR: 23.734821319580078\n",
            "[TRAIN] Iter: 121500 Loss: 0.00805283896625042  PSNR: 24.42000961303711\n",
            "[TRAIN] Iter: 121600 Loss: 0.010532412678003311  PSNR: 23.706817626953125\n",
            "[TRAIN] Iter: 121700 Loss: 0.01299898698925972  PSNR: 22.829757690429688\n",
            "[TRAIN] Iter: 121800 Loss: 0.009362143464386463  PSNR: 24.152359008789062\n",
            "[TRAIN] Iter: 121900 Loss: 0.013234606944024563  PSNR: 22.206348419189453\n",
            "[TRAIN] Iter: 122000 Loss: 0.011227182112634182  PSNR: 22.7744197845459\n",
            "[TRAIN] Iter: 122100 Loss: 0.009368358179926872  PSNR: 23.519641876220703\n",
            "[TRAIN] Iter: 122200 Loss: 0.008582410402595997  PSNR: 24.642478942871094\n",
            "[TRAIN] Iter: 122300 Loss: 0.008811123669147491  PSNR: 24.3199520111084\n",
            "[TRAIN] Iter: 122400 Loss: 0.01718887686729431  PSNR: 21.244199752807617\n",
            "[TRAIN] Iter: 122500 Loss: 0.01256571151316166  PSNR: 22.93210792541504\n",
            "[TRAIN] Iter: 122600 Loss: 0.015931300818920135  PSNR: 21.424327850341797\n",
            "[TRAIN] Iter: 122700 Loss: 0.008576422929763794  PSNR: 24.09911346435547\n",
            "[TRAIN] Iter: 122800 Loss: 0.008973872289061546  PSNR: 24.402074813842773\n",
            "[TRAIN] Iter: 122900 Loss: 0.008687825873494148  PSNR: 24.62700080871582\n",
            "[TRAIN] Iter: 123000 Loss: 0.007962449453771114  PSNR: 24.086036682128906\n",
            "[TRAIN] Iter: 123100 Loss: 0.009023063816130161  PSNR: 24.058609008789062\n",
            "[TRAIN] Iter: 123200 Loss: 0.009873857721686363  PSNR: 23.361677169799805\n",
            "[TRAIN] Iter: 123300 Loss: 0.00926092080771923  PSNR: 23.491792678833008\n",
            "[TRAIN] Iter: 123400 Loss: 0.007769017945975065  PSNR: 24.4105224609375\n",
            "[TRAIN] Iter: 123500 Loss: 0.008028872311115265  PSNR: 24.33447265625\n",
            "[TRAIN] Iter: 123600 Loss: 0.009207488968968391  PSNR: 24.131202697753906\n",
            "[TRAIN] Iter: 123700 Loss: 0.011563766747713089  PSNR: 23.32655906677246\n",
            "[TRAIN] Iter: 123800 Loss: 0.01106553990393877  PSNR: 23.065420150756836\n",
            "[TRAIN] Iter: 123900 Loss: 0.007701540365815163  PSNR: 25.201696395874023\n",
            "[TRAIN] Iter: 124000 Loss: 0.016996091231703758  PSNR: 21.262271881103516\n",
            "[TRAIN] Iter: 124100 Loss: 0.01063325721770525  PSNR: 23.035696029663086\n",
            "[TRAIN] Iter: 124200 Loss: 0.010110377334058285  PSNR: 23.98427963256836\n",
            "[TRAIN] Iter: 124300 Loss: 0.011968047358095646  PSNR: 22.078777313232422\n",
            "[TRAIN] Iter: 124400 Loss: 0.013522863388061523  PSNR: 22.599533081054688\n",
            "[TRAIN] Iter: 124500 Loss: 0.012332210317254066  PSNR: 22.430194854736328\n",
            "[TRAIN] Iter: 124600 Loss: 0.007523532025516033  PSNR: 24.625591278076172\n",
            "[TRAIN] Iter: 124700 Loss: 0.007230858784168959  PSNR: 25.261478424072266\n",
            "[TRAIN] Iter: 124800 Loss: 0.014243175275623798  PSNR: 21.73452377319336\n",
            "[TRAIN] Iter: 124900 Loss: 0.012565276585519314  PSNR: 22.072072982788086\n",
            "[TRAIN] Iter: 125000 Loss: 0.010289795696735382  PSNR: 23.134550094604492\n",
            "[TRAIN] Iter: 125100 Loss: 0.010914123617112637  PSNR: 23.1927433013916\n",
            "[TRAIN] Iter: 125200 Loss: 0.013295009732246399  PSNR: 22.39051055908203\n",
            "[TRAIN] Iter: 125300 Loss: 0.008589448407292366  PSNR: 24.2900333404541\n",
            "[TRAIN] Iter: 125400 Loss: 0.008827008306980133  PSNR: 23.789426803588867\n",
            "[TRAIN] Iter: 125500 Loss: 0.015079488977789879  PSNR: 21.998146057128906\n",
            "[TRAIN] Iter: 125600 Loss: 0.01149028167128563  PSNR: 22.93455696105957\n",
            "[TRAIN] Iter: 125700 Loss: 0.012732584029436111  PSNR: 22.685699462890625\n",
            "[TRAIN] Iter: 125800 Loss: 0.00849401205778122  PSNR: 24.198389053344727\n",
            "[TRAIN] Iter: 125900 Loss: 0.017780551686882973  PSNR: 20.960742950439453\n",
            "[TRAIN] Iter: 126000 Loss: 0.007998071610927582  PSNR: 25.26573944091797\n",
            "[TRAIN] Iter: 126100 Loss: 0.010393115691840649  PSNR: 23.042633056640625\n",
            "[TRAIN] Iter: 126200 Loss: 0.010679421946406364  PSNR: 23.659130096435547\n",
            "[TRAIN] Iter: 126300 Loss: 0.007874052971601486  PSNR: 24.367616653442383\n",
            "[TRAIN] Iter: 126400 Loss: 0.008248120546340942  PSNR: 24.50246238708496\n",
            "[TRAIN] Iter: 126500 Loss: 0.011802604421973228  PSNR: 22.728092193603516\n",
            "[TRAIN] Iter: 126600 Loss: 0.006034954451024532  PSNR: 26.190837860107422\n",
            "[TRAIN] Iter: 126700 Loss: 0.010508284904062748  PSNR: 23.40270233154297\n",
            "[TRAIN] Iter: 126800 Loss: 0.007268369197845459  PSNR: 24.74835777282715\n",
            "[TRAIN] Iter: 126900 Loss: 0.01087581180036068  PSNR: 23.141809463500977\n",
            "[TRAIN] Iter: 127000 Loss: 0.005808541085571051  PSNR: 26.66196632385254\n",
            "[TRAIN] Iter: 127100 Loss: 0.013047847896814346  PSNR: 22.547828674316406\n",
            "[TRAIN] Iter: 127200 Loss: 0.008044753223657608  PSNR: 24.58051300048828\n",
            "[TRAIN] Iter: 127300 Loss: 0.008965892717242241  PSNR: 24.03749656677246\n",
            "[TRAIN] Iter: 127400 Loss: 0.011048232205212116  PSNR: 23.29506492614746\n",
            "[TRAIN] Iter: 127500 Loss: 0.013041956350207329  PSNR: 21.805315017700195\n",
            "[TRAIN] Iter: 127600 Loss: 0.008787862956523895  PSNR: 23.797826766967773\n",
            "[TRAIN] Iter: 127700 Loss: 0.011202518828213215  PSNR: 23.26681137084961\n",
            "[TRAIN] Iter: 127800 Loss: 0.010267344303429127  PSNR: 23.599069595336914\n",
            "[TRAIN] Iter: 127900 Loss: 0.00853248406201601  PSNR: 24.698711395263672\n",
            "[TRAIN] Iter: 128000 Loss: 0.008812205865979195  PSNR: 24.904762268066406\n",
            "[TRAIN] Iter: 128100 Loss: 0.01081579364836216  PSNR: 23.398853302001953\n",
            "[TRAIN] Iter: 128200 Loss: 0.008221978321671486  PSNR: 24.03911590576172\n",
            "[TRAIN] Iter: 128300 Loss: 0.012585065327584743  PSNR: 22.903200149536133\n",
            "[TRAIN] Iter: 128400 Loss: 0.011104514822363853  PSNR: 23.100936889648438\n",
            "[TRAIN] Iter: 128500 Loss: 0.012094402685761452  PSNR: 22.529436111450195\n",
            "[TRAIN] Iter: 128600 Loss: 0.012290189042687416  PSNR: 22.905031204223633\n",
            "[TRAIN] Iter: 128700 Loss: 0.010720349848270416  PSNR: 23.472360610961914\n",
            "[TRAIN] Iter: 128800 Loss: 0.008875662460923195  PSNR: 24.101348876953125\n",
            "[TRAIN] Iter: 128900 Loss: 0.01125964056700468  PSNR: 23.302309036254883\n",
            "[TRAIN] Iter: 129000 Loss: 0.008705541491508484  PSNR: 24.065080642700195\n",
            "[TRAIN] Iter: 129100 Loss: 0.011964418925344944  PSNR: 23.320608139038086\n",
            "[TRAIN] Iter: 129200 Loss: 0.013374059461057186  PSNR: 22.426992416381836\n",
            "[TRAIN] Iter: 129300 Loss: 0.007758760824799538  PSNR: 25.310571670532227\n",
            "[TRAIN] Iter: 129400 Loss: 0.011638719588518143  PSNR: 23.381732940673828\n",
            "[TRAIN] Iter: 129500 Loss: 0.00832538865506649  PSNR: 24.46884536743164\n",
            "[TRAIN] Iter: 129600 Loss: 0.008754433132708073  PSNR: 24.18724822998047\n",
            "[TRAIN] Iter: 129700 Loss: 0.00890798307955265  PSNR: 24.400222778320312\n",
            "[TRAIN] Iter: 129800 Loss: 0.01176455058157444  PSNR: 22.59908676147461\n",
            "[TRAIN] Iter: 129900 Loss: 0.00981675274670124  PSNR: 23.542057037353516\n",
            " 65% 129999/200000 [4:15:08<2:16:04,  8.57it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/130000.tar\n",
            "[TRAIN] Iter: 130000 Loss: 0.009064873680472374  PSNR: 23.879379272460938\n",
            "[TRAIN] Iter: 130100 Loss: 0.015119924210011959  PSNR: 22.07986068725586\n",
            "[TRAIN] Iter: 130200 Loss: 0.011890868656337261  PSNR: 22.68834114074707\n",
            "[TRAIN] Iter: 130300 Loss: 0.011829276569187641  PSNR: 23.31977653503418\n",
            "[TRAIN] Iter: 130400 Loss: 0.008043214678764343  PSNR: 24.59454917907715\n",
            "[TRAIN] Iter: 130500 Loss: 0.010899355635046959  PSNR: 23.048154830932617\n",
            "[TRAIN] Iter: 130600 Loss: 0.011720108799636364  PSNR: 23.403209686279297\n",
            "[TRAIN] Iter: 130700 Loss: 0.00702862162142992  PSNR: 25.823461532592773\n",
            "[TRAIN] Iter: 130800 Loss: 0.011889994144439697  PSNR: 22.258569717407227\n",
            "[TRAIN] Iter: 130900 Loss: 0.00970583688467741  PSNR: 24.025442123413086\n",
            "[TRAIN] Iter: 131000 Loss: 0.00914906244724989  PSNR: 24.825563430786133\n",
            "[TRAIN] Iter: 131100 Loss: 0.008096016012132168  PSNR: 24.589099884033203\n",
            "[TRAIN] Iter: 131200 Loss: 0.013927451334893703  PSNR: 22.776920318603516\n",
            "[TRAIN] Iter: 131300 Loss: 0.015650156885385513  PSNR: 21.785493850708008\n",
            "[TRAIN] Iter: 131400 Loss: 0.010713577270507812  PSNR: 23.924583435058594\n",
            "[TRAIN] Iter: 131500 Loss: 0.008802242577075958  PSNR: 24.47274398803711\n",
            "[TRAIN] Iter: 131600 Loss: 0.013963867910206318  PSNR: 22.310829162597656\n",
            "[TRAIN] Iter: 131700 Loss: 0.011503941379487514  PSNR: 22.733078002929688\n",
            "[TRAIN] Iter: 131800 Loss: 0.012644872069358826  PSNR: 22.52342414855957\n",
            "[TRAIN] Iter: 131900 Loss: 0.00903126411139965  PSNR: 24.140119552612305\n",
            "[TRAIN] Iter: 132000 Loss: 0.007840132340788841  PSNR: 24.94696617126465\n",
            "[TRAIN] Iter: 132100 Loss: 0.01165279746055603  PSNR: 22.82610321044922\n",
            "[TRAIN] Iter: 132200 Loss: 0.00960626546293497  PSNR: 23.78572654724121\n",
            "[TRAIN] Iter: 132300 Loss: 0.011442555114626884  PSNR: 23.474857330322266\n",
            "[TRAIN] Iter: 132400 Loss: 0.014414885081350803  PSNR: 21.952838897705078\n",
            "[TRAIN] Iter: 132500 Loss: 0.0097795519977808  PSNR: 23.76179313659668\n",
            "[TRAIN] Iter: 132600 Loss: 0.013671418651938438  PSNR: 22.520450592041016\n",
            "[TRAIN] Iter: 132700 Loss: 0.012733633629977703  PSNR: 22.184831619262695\n",
            "[TRAIN] Iter: 132800 Loss: 0.008898375555872917  PSNR: 24.337860107421875\n",
            "[TRAIN] Iter: 132900 Loss: 0.009285526350140572  PSNR: 24.04891586303711\n",
            "[TRAIN] Iter: 133000 Loss: 0.017847944051027298  PSNR: 20.731931686401367\n",
            "[TRAIN] Iter: 133100 Loss: 0.00846780277788639  PSNR: 24.639448165893555\n",
            "[TRAIN] Iter: 133200 Loss: 0.011294830590486526  PSNR: 23.165992736816406\n",
            "[TRAIN] Iter: 133300 Loss: 0.01188945583999157  PSNR: 23.217784881591797\n",
            "[TRAIN] Iter: 133400 Loss: 0.008629304356873035  PSNR: 24.382518768310547\n",
            "[TRAIN] Iter: 133500 Loss: 0.010029925964772701  PSNR: 23.755233764648438\n",
            "[TRAIN] Iter: 133600 Loss: 0.009600809775292873  PSNR: 25.028518676757812\n",
            "[TRAIN] Iter: 133700 Loss: 0.006286140531301498  PSNR: 25.999290466308594\n",
            "[TRAIN] Iter: 133800 Loss: 0.00830979086458683  PSNR: 24.30510902404785\n",
            "[TRAIN] Iter: 133900 Loss: 0.007904705591499805  PSNR: 24.753610610961914\n",
            "[TRAIN] Iter: 134000 Loss: 0.011076275259256363  PSNR: 23.27870750427246\n",
            "[TRAIN] Iter: 134100 Loss: 0.008935787715017796  PSNR: 24.922964096069336\n",
            "[TRAIN] Iter: 134200 Loss: 0.010884355753660202  PSNR: 23.541561126708984\n",
            "[TRAIN] Iter: 134300 Loss: 0.010218193754553795  PSNR: 24.433866500854492\n",
            "[TRAIN] Iter: 134400 Loss: 0.010855595581233501  PSNR: 23.2183837890625\n",
            "[TRAIN] Iter: 134500 Loss: 0.00802244059741497  PSNR: 24.64268684387207\n",
            "[TRAIN] Iter: 134600 Loss: 0.007793117314577103  PSNR: 24.67254638671875\n",
            "[TRAIN] Iter: 134700 Loss: 0.010369297116994858  PSNR: 23.329570770263672\n",
            "[TRAIN] Iter: 134800 Loss: 0.006619554944336414  PSNR: 25.931753158569336\n",
            "[TRAIN] Iter: 134900 Loss: 0.006308040115982294  PSNR: 25.53374481201172\n",
            "[TRAIN] Iter: 135000 Loss: 0.012831938453018665  PSNR: 22.291976928710938\n",
            "[TRAIN] Iter: 135100 Loss: 0.014312630519270897  PSNR: 22.354358673095703\n",
            "[TRAIN] Iter: 135200 Loss: 0.011831355281174183  PSNR: 22.747249603271484\n",
            "[TRAIN] Iter: 135300 Loss: 0.010420837439596653  PSNR: 23.556774139404297\n",
            "[TRAIN] Iter: 135400 Loss: 0.010635411366820335  PSNR: 23.087556838989258\n",
            "[TRAIN] Iter: 135500 Loss: 0.01315465196967125  PSNR: 22.430849075317383\n",
            "[TRAIN] Iter: 135600 Loss: 0.01170714758336544  PSNR: 22.929292678833008\n",
            "[TRAIN] Iter: 135700 Loss: 0.010960442014038563  PSNR: 24.076452255249023\n",
            "[TRAIN] Iter: 135800 Loss: 0.009569227695465088  PSNR: 23.76105308532715\n",
            "[TRAIN] Iter: 135900 Loss: 0.00980202667415142  PSNR: 23.793031692504883\n",
            "[TRAIN] Iter: 136000 Loss: 0.00601623672991991  PSNR: 26.12215805053711\n",
            "[TRAIN] Iter: 136100 Loss: 0.00763277430087328  PSNR: 24.62322235107422\n",
            "[TRAIN] Iter: 136200 Loss: 0.009822782129049301  PSNR: 23.82241439819336\n",
            "[TRAIN] Iter: 136300 Loss: 0.011685987003147602  PSNR: 23.181482315063477\n",
            "[TRAIN] Iter: 136400 Loss: 0.007853146642446518  PSNR: 24.413339614868164\n",
            "[TRAIN] Iter: 136500 Loss: 0.01149919256567955  PSNR: 22.88399887084961\n",
            "[TRAIN] Iter: 136600 Loss: 0.007074631750583649  PSNR: 25.13361930847168\n",
            "[TRAIN] Iter: 136700 Loss: 0.011304374784231186  PSNR: 23.39462661743164\n",
            "[TRAIN] Iter: 136800 Loss: 0.009569890797138214  PSNR: 23.475143432617188\n",
            "[TRAIN] Iter: 136900 Loss: 0.007556539960205555  PSNR: 24.79619789123535\n",
            "[TRAIN] Iter: 137000 Loss: 0.008156700991094112  PSNR: 24.897748947143555\n",
            "[TRAIN] Iter: 137100 Loss: 0.008023923262953758  PSNR: 24.203372955322266\n",
            "[TRAIN] Iter: 137200 Loss: 0.006583578884601593  PSNR: 25.57589340209961\n",
            "[TRAIN] Iter: 137300 Loss: 0.009462162852287292  PSNR: 24.07427406311035\n",
            "[TRAIN] Iter: 137400 Loss: 0.005640595685690641  PSNR: 26.568262100219727\n",
            "[TRAIN] Iter: 137500 Loss: 0.013088741339743137  PSNR: 21.960920333862305\n",
            "[TRAIN] Iter: 137600 Loss: 0.008086305111646652  PSNR: 24.60355567932129\n",
            "[TRAIN] Iter: 137700 Loss: 0.009206777438521385  PSNR: 24.250511169433594\n",
            "[TRAIN] Iter: 137800 Loss: 0.01134985126554966  PSNR: 23.460704803466797\n",
            "[TRAIN] Iter: 137900 Loss: 0.009218153543770313  PSNR: 23.957067489624023\n",
            "[TRAIN] Iter: 138000 Loss: 0.012021737173199654  PSNR: 23.05228042602539\n",
            "[TRAIN] Iter: 138100 Loss: 0.007121424190700054  PSNR: 25.357864379882812\n",
            "[TRAIN] Iter: 138200 Loss: 0.007500472944229841  PSNR: 24.60651397705078\n",
            "[TRAIN] Iter: 138300 Loss: 0.012366263195872307  PSNR: 22.502870559692383\n",
            "[TRAIN] Iter: 138400 Loss: 0.01164703443646431  PSNR: 23.38152313232422\n",
            "[TRAIN] Iter: 138500 Loss: 0.008330458775162697  PSNR: 25.473186492919922\n",
            "[TRAIN] Iter: 138600 Loss: 0.011336993426084518  PSNR: 23.10563850402832\n",
            "[TRAIN] Iter: 138700 Loss: 0.010757369920611382  PSNR: 22.970582962036133\n",
            "[TRAIN] Iter: 138800 Loss: 0.013590190559625626  PSNR: 22.928987503051758\n",
            "[TRAIN] Iter: 138900 Loss: 0.00829755887389183  PSNR: 24.566368103027344\n",
            "[TRAIN] Iter: 139000 Loss: 0.011829949915409088  PSNR: 23.52926254272461\n",
            "[TRAIN] Iter: 139100 Loss: 0.009716231375932693  PSNR: 24.41815948486328\n",
            "[TRAIN] Iter: 139200 Loss: 0.008349563926458359  PSNR: 25.30927276611328\n",
            "[TRAIN] Iter: 139300 Loss: 0.007096400484442711  PSNR: 24.751142501831055\n",
            "[TRAIN] Iter: 139400 Loss: 0.01063152588903904  PSNR: 23.42444610595703\n",
            "[TRAIN] Iter: 139500 Loss: 0.015272421762347221  PSNR: 21.84966468811035\n",
            "[TRAIN] Iter: 139600 Loss: 0.01338321901857853  PSNR: 22.25554656982422\n",
            "[TRAIN] Iter: 139700 Loss: 0.01390756294131279  PSNR: 22.441307067871094\n",
            "[TRAIN] Iter: 139800 Loss: 0.009467121213674545  PSNR: 23.421409606933594\n",
            "[TRAIN] Iter: 139900 Loss: 0.007614581845700741  PSNR: 25.506559371948242\n",
            " 70% 139999/200000 [4:34:37<1:56:21,  8.59it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/140000.tar\n",
            "[TRAIN] Iter: 140000 Loss: 0.011617116630077362  PSNR: 22.87848663330078\n",
            "[TRAIN] Iter: 140100 Loss: 0.008753559552133083  PSNR: 25.16543197631836\n",
            "[TRAIN] Iter: 140200 Loss: 0.006424391642212868  PSNR: 26.00580406188965\n",
            "[TRAIN] Iter: 140300 Loss: 0.008994931355118752  PSNR: 25.0283203125\n",
            "[TRAIN] Iter: 140400 Loss: 0.007237761281430721  PSNR: 25.528106689453125\n",
            "[TRAIN] Iter: 140500 Loss: 0.008641356602311134  PSNR: 24.478418350219727\n",
            "[TRAIN] Iter: 140600 Loss: 0.006430255249142647  PSNR: 25.750999450683594\n",
            "[TRAIN] Iter: 140700 Loss: 0.011575952172279358  PSNR: 22.776756286621094\n",
            "[TRAIN] Iter: 140800 Loss: 0.009020931087434292  PSNR: 24.4703311920166\n",
            "[TRAIN] Iter: 140900 Loss: 0.01323006208986044  PSNR: 22.6746768951416\n",
            "[TRAIN] Iter: 141000 Loss: 0.00889649335294962  PSNR: 24.709199905395508\n",
            "[TRAIN] Iter: 141100 Loss: 0.008842875249683857  PSNR: 23.83954620361328\n",
            "[TRAIN] Iter: 141200 Loss: 0.009247069247066975  PSNR: 24.37602424621582\n",
            "[TRAIN] Iter: 141300 Loss: 0.009697654284536839  PSNR: 23.59172821044922\n",
            "[TRAIN] Iter: 141400 Loss: 0.0069303978234529495  PSNR: 25.406536102294922\n",
            "[TRAIN] Iter: 141500 Loss: 0.00879589281976223  PSNR: 24.41124153137207\n",
            "[TRAIN] Iter: 141600 Loss: 0.008801588788628578  PSNR: 23.477888107299805\n",
            "[TRAIN] Iter: 141700 Loss: 0.010372883640229702  PSNR: 23.822221755981445\n",
            "[TRAIN] Iter: 141800 Loss: 0.01154719851911068  PSNR: 23.6588134765625\n",
            "[TRAIN] Iter: 141900 Loss: 0.00899419840425253  PSNR: 24.206722259521484\n",
            "[TRAIN] Iter: 142000 Loss: 0.008451847359538078  PSNR: 24.64583396911621\n",
            "[TRAIN] Iter: 142100 Loss: 0.007290192414075136  PSNR: 25.63393783569336\n",
            "[TRAIN] Iter: 142200 Loss: 0.015619592741131783  PSNR: 21.62969207763672\n",
            "[TRAIN] Iter: 142300 Loss: 0.01156003400683403  PSNR: 23.103702545166016\n",
            "[TRAIN] Iter: 142400 Loss: 0.008186601102352142  PSNR: 24.552032470703125\n",
            "[TRAIN] Iter: 142500 Loss: 0.008705902844667435  PSNR: 24.228986740112305\n",
            "[TRAIN] Iter: 142600 Loss: 0.012336171232163906  PSNR: 23.209318161010742\n",
            "[TRAIN] Iter: 142700 Loss: 0.007816430181264877  PSNR: 25.341541290283203\n",
            "[TRAIN] Iter: 142800 Loss: 0.00999617949128151  PSNR: 23.586977005004883\n",
            "[TRAIN] Iter: 142900 Loss: 0.009508758783340454  PSNR: 23.795106887817383\n",
            "[TRAIN] Iter: 143000 Loss: 0.008540516719222069  PSNR: 24.036611557006836\n",
            "[TRAIN] Iter: 143100 Loss: 0.006996525917202234  PSNR: 25.491487503051758\n",
            "[TRAIN] Iter: 143200 Loss: 0.016160517930984497  PSNR: 21.807052612304688\n",
            "[TRAIN] Iter: 143300 Loss: 0.009433649480342865  PSNR: 24.098915100097656\n",
            "[TRAIN] Iter: 143400 Loss: 0.006794217973947525  PSNR: 24.5274658203125\n",
            "[TRAIN] Iter: 143500 Loss: 0.014473771676421165  PSNR: 21.772323608398438\n",
            "[TRAIN] Iter: 143600 Loss: 0.008267580531537533  PSNR: 24.640443801879883\n",
            "[TRAIN] Iter: 143700 Loss: 0.008131717331707478  PSNR: 24.588905334472656\n",
            "[TRAIN] Iter: 143800 Loss: 0.0071090226992964745  PSNR: 25.23372459411621\n",
            "[TRAIN] Iter: 143900 Loss: 0.013298281468451023  PSNR: 22.42450523376465\n",
            "[TRAIN] Iter: 144000 Loss: 0.009595757350325584  PSNR: 24.018539428710938\n",
            "[TRAIN] Iter: 144100 Loss: 0.007393787615001202  PSNR: 25.496442794799805\n",
            "[TRAIN] Iter: 144200 Loss: 0.007620027754455805  PSNR: 25.37618637084961\n",
            "[TRAIN] Iter: 144300 Loss: 0.007240802515298128  PSNR: 25.535825729370117\n",
            "[TRAIN] Iter: 144400 Loss: 0.01055232621729374  PSNR: 23.895267486572266\n",
            "[TRAIN] Iter: 144500 Loss: 0.011384245939552784  PSNR: 23.5301456451416\n",
            "[TRAIN] Iter: 144600 Loss: 0.009900142438709736  PSNR: 23.468759536743164\n",
            "[TRAIN] Iter: 144700 Loss: 0.010265842080116272  PSNR: 24.2769832611084\n",
            "[TRAIN] Iter: 144800 Loss: 0.010443344712257385  PSNR: 23.646970748901367\n",
            "[TRAIN] Iter: 144900 Loss: 0.011677662841975689  PSNR: 23.037609100341797\n",
            "[TRAIN] Iter: 145000 Loss: 0.013562331907451153  PSNR: 22.58283233642578\n",
            "[TRAIN] Iter: 145100 Loss: 0.013463122770190239  PSNR: 22.155969619750977\n",
            "[TRAIN] Iter: 145200 Loss: 0.008558789268136024  PSNR: 23.75905418395996\n",
            "[TRAIN] Iter: 145300 Loss: 0.0073678158223629  PSNR: 24.718530654907227\n",
            "[TRAIN] Iter: 145400 Loss: 0.011188073083758354  PSNR: 23.350980758666992\n",
            "[TRAIN] Iter: 145500 Loss: 0.008188782259821892  PSNR: 26.711528778076172\n",
            "[TRAIN] Iter: 145600 Loss: 0.00791560485959053  PSNR: 24.78272819519043\n",
            "[TRAIN] Iter: 145700 Loss: 0.009610135108232498  PSNR: 24.49029541015625\n",
            "[TRAIN] Iter: 145800 Loss: 0.00900161825120449  PSNR: 23.92437744140625\n",
            "[TRAIN] Iter: 145900 Loss: 0.009974924847483635  PSNR: 23.63519859313965\n",
            "[TRAIN] Iter: 146000 Loss: 0.013400924392044544  PSNR: 22.666059494018555\n",
            "[TRAIN] Iter: 146100 Loss: 0.01464664377272129  PSNR: 21.477338790893555\n",
            "[TRAIN] Iter: 146200 Loss: 0.008234510198235512  PSNR: 25.01640510559082\n",
            "[TRAIN] Iter: 146300 Loss: 0.010744346305727959  PSNR: 23.66531753540039\n",
            "[TRAIN] Iter: 146400 Loss: 0.010538915172219276  PSNR: 23.205169677734375\n",
            "[TRAIN] Iter: 146500 Loss: 0.009033214300870895  PSNR: 24.305574417114258\n",
            "[TRAIN] Iter: 146600 Loss: 0.006047620438039303  PSNR: 26.107454299926758\n",
            "[TRAIN] Iter: 146700 Loss: 0.013310874812304974  PSNR: 22.30282974243164\n",
            "[TRAIN] Iter: 146800 Loss: 0.009043998084962368  PSNR: 24.691648483276367\n",
            "[TRAIN] Iter: 146900 Loss: 0.011826314963400364  PSNR: 23.193422317504883\n",
            "[TRAIN] Iter: 147000 Loss: 0.008940093219280243  PSNR: 24.747220993041992\n",
            "[TRAIN] Iter: 147100 Loss: 0.009980814531445503  PSNR: 23.2137508392334\n",
            "[TRAIN] Iter: 147200 Loss: 0.010344542562961578  PSNR: 23.747526168823242\n",
            "[TRAIN] Iter: 147300 Loss: 0.010275335982441902  PSNR: 24.129684448242188\n",
            "[TRAIN] Iter: 147400 Loss: 0.005671009421348572  PSNR: 26.39453887939453\n",
            "[TRAIN] Iter: 147500 Loss: 0.00797189399600029  PSNR: 24.179697036743164\n",
            "[TRAIN] Iter: 147600 Loss: 0.009617872536182404  PSNR: 24.137081146240234\n",
            "[TRAIN] Iter: 147700 Loss: 0.008257337845861912  PSNR: 24.733549118041992\n",
            "[TRAIN] Iter: 147800 Loss: 0.008287442848086357  PSNR: 24.386171340942383\n",
            "[TRAIN] Iter: 147900 Loss: 0.00916360691189766  PSNR: 24.77957534790039\n",
            "[TRAIN] Iter: 148000 Loss: 0.0069531225599348545  PSNR: 25.97907829284668\n",
            "[TRAIN] Iter: 148100 Loss: 0.008003014139831066  PSNR: 24.75010871887207\n",
            "[TRAIN] Iter: 148200 Loss: 0.005831554066389799  PSNR: 26.126684188842773\n",
            "[TRAIN] Iter: 148300 Loss: 0.011268200352787971  PSNR: 23.076906204223633\n",
            "[TRAIN] Iter: 148400 Loss: 0.007095928769558668  PSNR: 25.099037170410156\n",
            "[TRAIN] Iter: 148500 Loss: 0.009609928354620934  PSNR: 24.955759048461914\n",
            "[TRAIN] Iter: 148600 Loss: 0.009860025718808174  PSNR: 23.674692153930664\n",
            "[TRAIN] Iter: 148700 Loss: 0.011078225448727608  PSNR: 23.51609230041504\n",
            "[TRAIN] Iter: 148800 Loss: 0.009048892185091972  PSNR: 24.139545440673828\n",
            "[TRAIN] Iter: 148900 Loss: 0.015273163095116615  PSNR: 21.864946365356445\n",
            "[TRAIN] Iter: 149000 Loss: 0.008147125132381916  PSNR: 24.81974983215332\n",
            "[TRAIN] Iter: 149100 Loss: 0.007761767134070396  PSNR: 25.34845733642578\n",
            "[TRAIN] Iter: 149200 Loss: 0.008923588320612907  PSNR: 24.16590690612793\n",
            "[TRAIN] Iter: 149300 Loss: 0.010198957286775112  PSNR: 23.668338775634766\n",
            "[TRAIN] Iter: 149400 Loss: 0.008054729551076889  PSNR: 24.835372924804688\n",
            "[TRAIN] Iter: 149500 Loss: 0.013675754889845848  PSNR: 22.13491439819336\n",
            "[TRAIN] Iter: 149600 Loss: 0.011014297604560852  PSNR: 23.028696060180664\n",
            "[TRAIN] Iter: 149700 Loss: 0.00823226198554039  PSNR: 24.51116943359375\n",
            "[TRAIN] Iter: 149800 Loss: 0.010469640605151653  PSNR: 23.601070404052734\n",
            "[TRAIN] Iter: 149900 Loss: 0.007291589863598347  PSNR: 25.144073486328125\n",
            " 75% 149999/200000 [4:54:13<1:38:05,  8.50it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/150000.tar\n",
            "[TRAIN] Iter: 150000 Loss: 0.010564399883151054  PSNR: 23.371240615844727\n",
            "[TRAIN] Iter: 150100 Loss: 0.01191776804625988  PSNR: 22.617706298828125\n",
            "[TRAIN] Iter: 150200 Loss: 0.006383652798831463  PSNR: 26.065540313720703\n",
            "[TRAIN] Iter: 150300 Loss: 0.00851019099354744  PSNR: 25.145551681518555\n",
            "[TRAIN] Iter: 150400 Loss: 0.013151783496141434  PSNR: 22.611337661743164\n",
            "[TRAIN] Iter: 150500 Loss: 0.009670108556747437  PSNR: 24.820207595825195\n",
            "[TRAIN] Iter: 150600 Loss: 0.010547311045229435  PSNR: 23.058250427246094\n",
            "[TRAIN] Iter: 150700 Loss: 0.009075663983821869  PSNR: 24.404529571533203\n",
            "[TRAIN] Iter: 150800 Loss: 0.007319447118788958  PSNR: 25.142845153808594\n",
            "[TRAIN] Iter: 150900 Loss: 0.007565900683403015  PSNR: 25.513017654418945\n",
            "[TRAIN] Iter: 151000 Loss: 0.009625080972909927  PSNR: 24.7325382232666\n",
            "[TRAIN] Iter: 151100 Loss: 0.009732849895954132  PSNR: 23.711942672729492\n",
            "[TRAIN] Iter: 151200 Loss: 0.008374295197427273  PSNR: 24.347347259521484\n",
            "[TRAIN] Iter: 151300 Loss: 0.013860254548490047  PSNR: 22.360824584960938\n",
            "[TRAIN] Iter: 151400 Loss: 0.0072275009006261826  PSNR: 25.369709014892578\n",
            "[TRAIN] Iter: 151500 Loss: 0.00700445706024766  PSNR: 24.96630859375\n",
            "[TRAIN] Iter: 151600 Loss: 0.009946564212441444  PSNR: 24.068294525146484\n",
            "[TRAIN] Iter: 151700 Loss: 0.00954422540962696  PSNR: 24.051647186279297\n",
            "[TRAIN] Iter: 151800 Loss: 0.009345507249236107  PSNR: 24.163206100463867\n",
            "[TRAIN] Iter: 151900 Loss: 0.011527740396559238  PSNR: 23.877893447875977\n",
            "[TRAIN] Iter: 152000 Loss: 0.00827191211283207  PSNR: 23.9211368560791\n",
            "[TRAIN] Iter: 152100 Loss: 0.009612477384507656  PSNR: 24.07022476196289\n",
            "[TRAIN] Iter: 152200 Loss: 0.011548391543328762  PSNR: 23.524412155151367\n",
            "[TRAIN] Iter: 152300 Loss: 0.011637993156909943  PSNR: 22.85433006286621\n",
            "[TRAIN] Iter: 152400 Loss: 0.009082094766199589  PSNR: 23.77483367919922\n",
            "[TRAIN] Iter: 152500 Loss: 0.011947469785809517  PSNR: 23.06085968017578\n",
            "[TRAIN] Iter: 152600 Loss: 0.011639697477221489  PSNR: 23.29381561279297\n",
            "[TRAIN] Iter: 152700 Loss: 0.007041447795927525  PSNR: 25.793182373046875\n",
            "[TRAIN] Iter: 152800 Loss: 0.0076294345781207085  PSNR: 24.505413055419922\n",
            "[TRAIN] Iter: 152900 Loss: 0.009241118095815182  PSNR: 24.087221145629883\n",
            "[TRAIN] Iter: 153000 Loss: 0.009598670527338982  PSNR: 23.487607955932617\n",
            "[TRAIN] Iter: 153100 Loss: 0.012794080190360546  PSNR: 22.35479736328125\n",
            "[TRAIN] Iter: 153200 Loss: 0.010109065100550652  PSNR: 24.0382080078125\n",
            "[TRAIN] Iter: 153300 Loss: 0.011380546726286411  PSNR: 24.094667434692383\n",
            "[TRAIN] Iter: 153400 Loss: 0.011816265061497688  PSNR: 23.278467178344727\n",
            "[TRAIN] Iter: 153500 Loss: 0.012022927403450012  PSNR: 22.837974548339844\n",
            "[TRAIN] Iter: 153600 Loss: 0.009765321388840675  PSNR: 23.97148323059082\n",
            "[TRAIN] Iter: 153700 Loss: 0.00908526312559843  PSNR: 24.436080932617188\n",
            "[TRAIN] Iter: 153800 Loss: 0.008779722265899181  PSNR: 24.16942024230957\n",
            "[TRAIN] Iter: 153900 Loss: 0.00955444760620594  PSNR: 23.67455291748047\n",
            "[TRAIN] Iter: 154000 Loss: 0.00791441835463047  PSNR: 24.60814094543457\n",
            "[TRAIN] Iter: 154100 Loss: 0.01040584221482277  PSNR: 24.8437442779541\n",
            "[TRAIN] Iter: 154200 Loss: 0.008364558219909668  PSNR: 24.286087036132812\n",
            "[TRAIN] Iter: 154300 Loss: 0.008812722750008106  PSNR: 24.487478256225586\n",
            "[TRAIN] Iter: 154400 Loss: 0.009268712252378464  PSNR: 24.488405227661133\n",
            "[TRAIN] Iter: 154500 Loss: 0.009023213759064674  PSNR: 25.483491897583008\n",
            "[TRAIN] Iter: 154600 Loss: 0.007121379487216473  PSNR: 25.192541122436523\n",
            "[TRAIN] Iter: 154700 Loss: 0.007317506708204746  PSNR: 25.345176696777344\n",
            "[TRAIN] Iter: 154800 Loss: 0.011487528681755066  PSNR: 22.759363174438477\n",
            "[TRAIN] Iter: 154900 Loss: 0.008762288838624954  PSNR: 25.194076538085938\n",
            "[TRAIN] Iter: 155000 Loss: 0.005069850478321314  PSNR: 26.453371047973633\n",
            "[TRAIN] Iter: 155100 Loss: 0.005803267937153578  PSNR: 26.933256149291992\n",
            "[TRAIN] Iter: 155200 Loss: 0.010699351318180561  PSNR: 23.477195739746094\n",
            "[TRAIN] Iter: 155300 Loss: 0.013548227027058601  PSNR: 22.25670623779297\n",
            "[TRAIN] Iter: 155400 Loss: 0.005942325573414564  PSNR: 25.792476654052734\n",
            "[TRAIN] Iter: 155500 Loss: 0.00822378695011139  PSNR: 24.793607711791992\n",
            "[TRAIN] Iter: 155600 Loss: 0.009306617081165314  PSNR: 24.806276321411133\n",
            "[TRAIN] Iter: 155700 Loss: 0.012351143173873425  PSNR: 23.75257682800293\n",
            "[TRAIN] Iter: 155800 Loss: 0.007171597331762314  PSNR: 25.016338348388672\n",
            "[TRAIN] Iter: 155900 Loss: 0.009401082992553711  PSNR: 24.428421020507812\n",
            "[TRAIN] Iter: 156000 Loss: 0.009560991078615189  PSNR: 24.372241973876953\n",
            "[TRAIN] Iter: 156100 Loss: 0.010815063491463661  PSNR: 23.26103973388672\n",
            "[TRAIN] Iter: 156200 Loss: 0.008994345553219318  PSNR: 24.544179916381836\n",
            "[TRAIN] Iter: 156300 Loss: 0.010434131138026714  PSNR: 24.192569732666016\n",
            "[TRAIN] Iter: 156400 Loss: 0.008365700952708721  PSNR: 24.538604736328125\n",
            "[TRAIN] Iter: 156500 Loss: 0.009317569434642792  PSNR: 24.04153060913086\n",
            "[TRAIN] Iter: 156600 Loss: 0.01060103252530098  PSNR: 24.060911178588867\n",
            "[TRAIN] Iter: 156700 Loss: 0.008930808864533901  PSNR: 24.24334144592285\n",
            "[TRAIN] Iter: 156800 Loss: 0.009807299822568893  PSNR: 24.32489013671875\n",
            "[TRAIN] Iter: 156900 Loss: 0.009682072326540947  PSNR: 23.668323516845703\n",
            "[TRAIN] Iter: 157000 Loss: 0.009908834472298622  PSNR: 22.794912338256836\n",
            "[TRAIN] Iter: 157100 Loss: 0.007400366477668285  PSNR: 24.35958480834961\n",
            "[TRAIN] Iter: 157200 Loss: 0.00832657516002655  PSNR: 25.240489959716797\n",
            "[TRAIN] Iter: 157300 Loss: 0.00985056720674038  PSNR: 23.758323669433594\n",
            "[TRAIN] Iter: 157400 Loss: 0.007740486413240433  PSNR: 25.059465408325195\n",
            "[TRAIN] Iter: 157500 Loss: 0.008281269110739231  PSNR: 24.019254684448242\n",
            "[TRAIN] Iter: 157600 Loss: 0.005848504137247801  PSNR: 25.761890411376953\n",
            "[TRAIN] Iter: 157700 Loss: 0.005378532223403454  PSNR: 26.81406021118164\n",
            "[TRAIN] Iter: 157800 Loss: 0.01207142323255539  PSNR: 23.408544540405273\n",
            "[TRAIN] Iter: 157900 Loss: 0.010411335155367851  PSNR: 23.657573699951172\n",
            "[TRAIN] Iter: 158000 Loss: 0.006833645515143871  PSNR: 25.35826873779297\n",
            "[TRAIN] Iter: 158100 Loss: 0.010494396090507507  PSNR: 24.180540084838867\n",
            "[TRAIN] Iter: 158200 Loss: 0.005408637225627899  PSNR: 27.039718627929688\n",
            "[TRAIN] Iter: 158300 Loss: 0.011540330946445465  PSNR: 23.34000587463379\n",
            "[TRAIN] Iter: 158400 Loss: 0.009143611416220665  PSNR: 24.316381454467773\n",
            "[TRAIN] Iter: 158500 Loss: 0.007487979251891375  PSNR: 25.654211044311523\n",
            "[TRAIN] Iter: 158600 Loss: 0.010310948826372623  PSNR: 23.984365463256836\n",
            "[TRAIN] Iter: 158700 Loss: 0.007190551608800888  PSNR: 24.812129974365234\n",
            "[TRAIN] Iter: 158800 Loss: 0.008172811940312386  PSNR: 24.510740280151367\n",
            "[TRAIN] Iter: 158900 Loss: 0.010565236210823059  PSNR: 23.70425796508789\n",
            "[TRAIN] Iter: 159000 Loss: 0.006809978745877743  PSNR: 25.4014835357666\n",
            "[TRAIN] Iter: 159100 Loss: 0.007326246239244938  PSNR: 25.17326545715332\n",
            "[TRAIN] Iter: 159200 Loss: 0.009276652708649635  PSNR: 24.369253158569336\n",
            "[TRAIN] Iter: 159300 Loss: 0.007167539559304714  PSNR: 25.803993225097656\n",
            "[TRAIN] Iter: 159400 Loss: 0.0071552712470293045  PSNR: 25.79718780517578\n",
            "[TRAIN] Iter: 159500 Loss: 0.009415622800588608  PSNR: 23.72237777709961\n",
            "[TRAIN] Iter: 159600 Loss: 0.008879467844963074  PSNR: 24.93079376220703\n",
            "[TRAIN] Iter: 159700 Loss: 0.01269281841814518  PSNR: 23.011640548706055\n",
            "[TRAIN] Iter: 159800 Loss: 0.010549773462116718  PSNR: 23.810056686401367\n",
            "[TRAIN] Iter: 159900 Loss: 0.007100990973412991  PSNR: 25.208202362060547\n",
            " 80% 159999/200000 [5:13:54<1:17:54,  8.56it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/160000.tar\n",
            "[TRAIN] Iter: 160000 Loss: 0.007459849119186401  PSNR: 25.329425811767578\n",
            "[TRAIN] Iter: 160100 Loss: 0.009858803823590279  PSNR: 24.316150665283203\n",
            "[TRAIN] Iter: 160200 Loss: 0.00848684087395668  PSNR: 24.806232452392578\n",
            "[TRAIN] Iter: 160300 Loss: 0.013958649709820747  PSNR: 22.3878231048584\n",
            "[TRAIN] Iter: 160400 Loss: 0.012128116562962532  PSNR: 22.985795974731445\n",
            "[TRAIN] Iter: 160500 Loss: 0.01294722966849804  PSNR: 22.984477996826172\n",
            "[TRAIN] Iter: 160600 Loss: 0.009766094386577606  PSNR: 24.173795700073242\n",
            "[TRAIN] Iter: 160700 Loss: 0.011093118228018284  PSNR: 23.177692413330078\n",
            "[TRAIN] Iter: 160800 Loss: 0.008039961569011211  PSNR: 25.72233772277832\n",
            "[TRAIN] Iter: 160900 Loss: 0.00918464083224535  PSNR: 24.294422149658203\n",
            "[TRAIN] Iter: 161000 Loss: 0.015139646828174591  PSNR: 21.619007110595703\n",
            "[TRAIN] Iter: 161100 Loss: 0.011117509566247463  PSNR: 23.836278915405273\n",
            "[TRAIN] Iter: 161200 Loss: 0.008985418826341629  PSNR: 23.407087326049805\n",
            "[TRAIN] Iter: 161300 Loss: 0.013546324335038662  PSNR: 22.43988037109375\n",
            "[TRAIN] Iter: 161400 Loss: 0.008004119619727135  PSNR: 24.5737247467041\n",
            "[TRAIN] Iter: 161500 Loss: 0.007427921984344721  PSNR: 25.82426643371582\n",
            "[TRAIN] Iter: 161600 Loss: 0.008918984793126583  PSNR: 24.66063117980957\n",
            "[TRAIN] Iter: 161700 Loss: 0.012672322802245617  PSNR: 23.131591796875\n",
            "[TRAIN] Iter: 161800 Loss: 0.008542699739336967  PSNR: 24.932571411132812\n",
            "[TRAIN] Iter: 161900 Loss: 0.011999091133475304  PSNR: 22.569326400756836\n",
            "[TRAIN] Iter: 162000 Loss: 0.006609717849642038  PSNR: 25.608369827270508\n",
            "[TRAIN] Iter: 162100 Loss: 0.009912369772791862  PSNR: 24.420679092407227\n",
            "[TRAIN] Iter: 162200 Loss: 0.01295426394790411  PSNR: 22.469722747802734\n",
            "[TRAIN] Iter: 162300 Loss: 0.006926568225026131  PSNR: 26.101810455322266\n",
            "[TRAIN] Iter: 162400 Loss: 0.010863939300179482  PSNR: 23.202627182006836\n",
            "[TRAIN] Iter: 162500 Loss: 0.00669944379478693  PSNR: 24.846866607666016\n",
            "[TRAIN] Iter: 162600 Loss: 0.008852187544107437  PSNR: 24.374542236328125\n",
            "[TRAIN] Iter: 162700 Loss: 0.014663688838481903  PSNR: 22.287607192993164\n",
            "[TRAIN] Iter: 162800 Loss: 0.007925138808786869  PSNR: 24.719039916992188\n",
            "[TRAIN] Iter: 162900 Loss: 0.00869629718363285  PSNR: 24.794021606445312\n",
            "[TRAIN] Iter: 163000 Loss: 0.008834333159029484  PSNR: 24.853992462158203\n",
            "[TRAIN] Iter: 163100 Loss: 0.007245650514960289  PSNR: 25.097097396850586\n",
            "[TRAIN] Iter: 163200 Loss: 0.011041236110031605  PSNR: 23.922372817993164\n",
            "[TRAIN] Iter: 163300 Loss: 0.011487022042274475  PSNR: 23.206756591796875\n",
            "[TRAIN] Iter: 163400 Loss: 0.012135686352849007  PSNR: 23.003965377807617\n",
            "[TRAIN] Iter: 163500 Loss: 0.007266605738550425  PSNR: 25.277122497558594\n",
            "[TRAIN] Iter: 163600 Loss: 0.007480146363377571  PSNR: 25.058429718017578\n",
            "[TRAIN] Iter: 163700 Loss: 0.008066466078162193  PSNR: 24.713825225830078\n",
            "[TRAIN] Iter: 163800 Loss: 0.010287351906299591  PSNR: 23.078697204589844\n",
            "[TRAIN] Iter: 163900 Loss: 0.00983265321701765  PSNR: 24.165559768676758\n",
            "[TRAIN] Iter: 164000 Loss: 0.006332220509648323  PSNR: 25.614110946655273\n",
            "[TRAIN] Iter: 164100 Loss: 0.008087096735835075  PSNR: 25.17671012878418\n",
            "[TRAIN] Iter: 164200 Loss: 0.007568438537418842  PSNR: 25.837589263916016\n",
            "[TRAIN] Iter: 164300 Loss: 0.008848159573972225  PSNR: 24.9786434173584\n",
            "[TRAIN] Iter: 164400 Loss: 0.006920021027326584  PSNR: 26.18467140197754\n",
            "[TRAIN] Iter: 164500 Loss: 0.009641756303608418  PSNR: 24.700664520263672\n",
            "[TRAIN] Iter: 164600 Loss: 0.006885077804327011  PSNR: 25.3145809173584\n",
            "[TRAIN] Iter: 164700 Loss: 0.010816558264195919  PSNR: 23.985733032226562\n",
            "[TRAIN] Iter: 164800 Loss: 0.008982100524008274  PSNR: 24.748594284057617\n",
            "[TRAIN] Iter: 164900 Loss: 0.010723685845732689  PSNR: 23.593761444091797\n",
            "[TRAIN] Iter: 165000 Loss: 0.010123956017196178  PSNR: 23.64481544494629\n",
            "[TRAIN] Iter: 165100 Loss: 0.01115792989730835  PSNR: 24.070404052734375\n",
            "[TRAIN] Iter: 165200 Loss: 0.006440100260078907  PSNR: 26.30653190612793\n",
            "[TRAIN] Iter: 165300 Loss: 0.011736477725207806  PSNR: 22.48387336730957\n",
            "[TRAIN] Iter: 165400 Loss: 0.00956788845360279  PSNR: 24.31333351135254\n",
            "[TRAIN] Iter: 165500 Loss: 0.00771171972155571  PSNR: 25.529403686523438\n",
            "[TRAIN] Iter: 165600 Loss: 0.01180972158908844  PSNR: 23.16254234313965\n",
            "[TRAIN] Iter: 165700 Loss: 0.008462975732982159  PSNR: 24.549596786499023\n",
            "[TRAIN] Iter: 165800 Loss: 0.009230805560946465  PSNR: 24.31492042541504\n",
            "[TRAIN] Iter: 165900 Loss: 0.005384406074881554  PSNR: 25.95399284362793\n",
            "[TRAIN] Iter: 166000 Loss: 0.008848383091390133  PSNR: 25.180910110473633\n",
            "[TRAIN] Iter: 166100 Loss: 0.011637629941105843  PSNR: 22.936737060546875\n",
            "[TRAIN] Iter: 166200 Loss: 0.006797196343541145  PSNR: 25.543304443359375\n",
            "[TRAIN] Iter: 166300 Loss: 0.008360306732356548  PSNR: 24.5504207611084\n",
            "[TRAIN] Iter: 166400 Loss: 0.01091451570391655  PSNR: 23.831008911132812\n",
            "[TRAIN] Iter: 166500 Loss: 0.011142788454890251  PSNR: 23.330976486206055\n",
            "[TRAIN] Iter: 166600 Loss: 0.01124243251979351  PSNR: 23.225187301635742\n",
            "[TRAIN] Iter: 166700 Loss: 0.011902183294296265  PSNR: 23.62533187866211\n",
            "[TRAIN] Iter: 166800 Loss: 0.00955223198980093  PSNR: 24.169462203979492\n",
            "[TRAIN] Iter: 166900 Loss: 0.008882738649845123  PSNR: 25.286706924438477\n",
            "[TRAIN] Iter: 167000 Loss: 0.012357441708445549  PSNR: 22.83558464050293\n",
            "[TRAIN] Iter: 167100 Loss: 0.012291435152292252  PSNR: 23.77240753173828\n",
            "[TRAIN] Iter: 167200 Loss: 0.010954752564430237  PSNR: 22.725126266479492\n",
            "[TRAIN] Iter: 167300 Loss: 0.007970010861754417  PSNR: 24.65802001953125\n",
            "[TRAIN] Iter: 167400 Loss: 0.010446995496749878  PSNR: 23.747163772583008\n",
            "[TRAIN] Iter: 167500 Loss: 0.007726086303591728  PSNR: 24.914405822753906\n",
            "[TRAIN] Iter: 167600 Loss: 0.01355169340968132  PSNR: 22.358890533447266\n",
            "[TRAIN] Iter: 167700 Loss: 0.010702744126319885  PSNR: 22.85283088684082\n",
            "[TRAIN] Iter: 167800 Loss: 0.00945147406309843  PSNR: 23.8549861907959\n",
            "[TRAIN] Iter: 167900 Loss: 0.012568308040499687  PSNR: 23.8236141204834\n",
            "[TRAIN] Iter: 168000 Loss: 0.01192622259259224  PSNR: 22.649276733398438\n",
            "[TRAIN] Iter: 168100 Loss: 0.009183373302221298  PSNR: 24.28224754333496\n",
            "[TRAIN] Iter: 168200 Loss: 0.009809009730815887  PSNR: 24.577438354492188\n",
            "[TRAIN] Iter: 168300 Loss: 0.010020223446190357  PSNR: 23.71027183532715\n",
            "[TRAIN] Iter: 168400 Loss: 0.011996837332844734  PSNR: 23.16898536682129\n",
            "[TRAIN] Iter: 168500 Loss: 0.01187104545533657  PSNR: 23.63795280456543\n",
            "[TRAIN] Iter: 168600 Loss: 0.009004728868603706  PSNR: 24.528282165527344\n",
            "[TRAIN] Iter: 168700 Loss: 0.01177939586341381  PSNR: 23.246139526367188\n",
            "[TRAIN] Iter: 168800 Loss: 0.00910166185349226  PSNR: 24.816598892211914\n",
            "[TRAIN] Iter: 168900 Loss: 0.007218866143375635  PSNR: 25.341320037841797\n",
            "[TRAIN] Iter: 169000 Loss: 0.006051003001630306  PSNR: 25.897382736206055\n",
            "[TRAIN] Iter: 169100 Loss: 0.008010703139007092  PSNR: 25.40066146850586\n",
            "[TRAIN] Iter: 169200 Loss: 0.00978122465312481  PSNR: 24.73294448852539\n",
            "[TRAIN] Iter: 169300 Loss: 0.008741457015275955  PSNR: 24.750896453857422\n",
            "[TRAIN] Iter: 169400 Loss: 0.007309962064027786  PSNR: 25.108659744262695\n",
            "[TRAIN] Iter: 169500 Loss: 0.012565631419420242  PSNR: 23.14142417907715\n",
            "[TRAIN] Iter: 169600 Loss: 0.009209265932440758  PSNR: 24.36419677734375\n",
            "[TRAIN] Iter: 169700 Loss: 0.010600130073726177  PSNR: 23.757585525512695\n",
            "[TRAIN] Iter: 169800 Loss: 0.007576713338494301  PSNR: 26.258914947509766\n",
            "[TRAIN] Iter: 169900 Loss: 0.005643084645271301  PSNR: 26.197147369384766\n",
            " 85% 169999/200000 [5:33:21<58:09,  8.60it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/170000.tar\n",
            "[TRAIN] Iter: 170000 Loss: 0.010255444794893265  PSNR: 23.556089401245117\n",
            "[TRAIN] Iter: 170100 Loss: 0.008673327043652534  PSNR: 25.299890518188477\n",
            "[TRAIN] Iter: 170200 Loss: 0.011609680950641632  PSNR: 23.175662994384766\n",
            "[TRAIN] Iter: 170300 Loss: 0.007620998192578554  PSNR: 24.65645408630371\n",
            "[TRAIN] Iter: 170400 Loss: 0.008340535685420036  PSNR: 24.40225601196289\n",
            "[TRAIN] Iter: 170500 Loss: 0.014966199174523354  PSNR: 22.150426864624023\n",
            "[TRAIN] Iter: 170600 Loss: 0.008484356105327606  PSNR: 24.575359344482422\n",
            "[TRAIN] Iter: 170700 Loss: 0.008740334771573544  PSNR: 24.507020950317383\n",
            "[TRAIN] Iter: 170800 Loss: 0.011281109414994717  PSNR: 23.505170822143555\n",
            "[TRAIN] Iter: 170900 Loss: 0.007023966405540705  PSNR: 25.829971313476562\n",
            "[TRAIN] Iter: 171000 Loss: 0.009644940495491028  PSNR: 23.848108291625977\n",
            "[TRAIN] Iter: 171100 Loss: 0.012064769864082336  PSNR: 23.164932250976562\n",
            "[TRAIN] Iter: 171200 Loss: 0.007220277562737465  PSNR: 25.653963088989258\n",
            "[TRAIN] Iter: 171300 Loss: 0.008537272922694683  PSNR: 24.68470573425293\n",
            "[TRAIN] Iter: 171400 Loss: 0.01009480468928814  PSNR: 23.485000610351562\n",
            "[TRAIN] Iter: 171500 Loss: 0.010354462079703808  PSNR: 24.384532928466797\n",
            "[TRAIN] Iter: 171600 Loss: 0.007997296750545502  PSNR: 24.712270736694336\n",
            "[TRAIN] Iter: 171700 Loss: 0.008596468716859818  PSNR: 25.33518409729004\n",
            "[TRAIN] Iter: 171800 Loss: 0.009195305407047272  PSNR: 23.920392990112305\n",
            "[TRAIN] Iter: 171900 Loss: 0.008468586951494217  PSNR: 25.002094268798828\n",
            "[TRAIN] Iter: 172000 Loss: 0.008531060069799423  PSNR: 24.56892204284668\n",
            "[TRAIN] Iter: 172100 Loss: 0.010503011755645275  PSNR: 24.238672256469727\n",
            "[TRAIN] Iter: 172200 Loss: 0.006873773410916328  PSNR: 25.93897819519043\n",
            "[TRAIN] Iter: 172300 Loss: 0.010668075643479824  PSNR: 23.164318084716797\n",
            "[TRAIN] Iter: 172400 Loss: 0.011438732035458088  PSNR: 23.687515258789062\n",
            "[TRAIN] Iter: 172500 Loss: 0.009968545287847519  PSNR: 23.70758819580078\n",
            "[TRAIN] Iter: 172600 Loss: 0.004497964400798082  PSNR: 27.38547134399414\n",
            "[TRAIN] Iter: 172700 Loss: 0.011675866320729256  PSNR: 23.226306915283203\n",
            "[TRAIN] Iter: 172800 Loss: 0.009454194456338882  PSNR: 25.154558181762695\n",
            "[TRAIN] Iter: 172900 Loss: 0.011213314719498158  PSNR: 23.862319946289062\n",
            "[TRAIN] Iter: 173000 Loss: 0.00507758092135191  PSNR: 26.165924072265625\n",
            "[TRAIN] Iter: 173100 Loss: 0.007200291845947504  PSNR: 25.220924377441406\n",
            "[TRAIN] Iter: 173200 Loss: 0.005413662642240524  PSNR: 26.618684768676758\n",
            "[TRAIN] Iter: 173300 Loss: 0.010600395500659943  PSNR: 23.151081085205078\n",
            "[TRAIN] Iter: 173400 Loss: 0.010000018402934074  PSNR: 23.25649642944336\n",
            "[TRAIN] Iter: 173500 Loss: 0.01032322272658348  PSNR: 23.683578491210938\n",
            "[TRAIN] Iter: 173600 Loss: 0.006976790260523558  PSNR: 25.500587463378906\n",
            "[TRAIN] Iter: 173700 Loss: 0.007383259013295174  PSNR: 24.79701805114746\n",
            "[TRAIN] Iter: 173800 Loss: 0.009469926357269287  PSNR: 24.385440826416016\n",
            "[TRAIN] Iter: 173900 Loss: 0.009769407100975513  PSNR: 23.694379806518555\n",
            "[TRAIN] Iter: 174000 Loss: 0.006225253455340862  PSNR: 26.103517532348633\n",
            "[TRAIN] Iter: 174100 Loss: 0.010120302438735962  PSNR: 24.36042594909668\n",
            "[TRAIN] Iter: 174200 Loss: 0.0073379878886044025  PSNR: 26.359445571899414\n",
            "[TRAIN] Iter: 174300 Loss: 0.008019443601369858  PSNR: 24.770841598510742\n",
            "[TRAIN] Iter: 174400 Loss: 0.01302333828061819  PSNR: 22.841615676879883\n",
            "[TRAIN] Iter: 174500 Loss: 0.007987081073224545  PSNR: 25.135478973388672\n",
            "[TRAIN] Iter: 174600 Loss: 0.008571822196245193  PSNR: 24.613584518432617\n",
            "[TRAIN] Iter: 174700 Loss: 0.008571388199925423  PSNR: 24.076465606689453\n",
            "[TRAIN] Iter: 174800 Loss: 0.00681365467607975  PSNR: 25.587482452392578\n",
            "[TRAIN] Iter: 174900 Loss: 0.01371418870985508  PSNR: 22.609594345092773\n",
            "[TRAIN] Iter: 175000 Loss: 0.009017223492264748  PSNR: 24.02088737487793\n",
            "[TRAIN] Iter: 175100 Loss: 0.009752931073307991  PSNR: 23.66010284423828\n",
            "[TRAIN] Iter: 175200 Loss: 0.011893413960933685  PSNR: 23.378868103027344\n",
            "[TRAIN] Iter: 175300 Loss: 0.008993919938802719  PSNR: 25.074087142944336\n",
            "[TRAIN] Iter: 175400 Loss: 0.00891166552901268  PSNR: 24.259105682373047\n",
            "[TRAIN] Iter: 175500 Loss: 0.00864566769450903  PSNR: 24.17113494873047\n",
            "[TRAIN] Iter: 175600 Loss: 0.008138411678373814  PSNR: 24.993005752563477\n",
            "[TRAIN] Iter: 175700 Loss: 0.00781948957592249  PSNR: 24.875511169433594\n",
            "[TRAIN] Iter: 175800 Loss: 0.010558881796896458  PSNR: 24.1343936920166\n",
            "[TRAIN] Iter: 175900 Loss: 0.00762157654389739  PSNR: 24.923118591308594\n",
            "[TRAIN] Iter: 176000 Loss: 0.009840570390224457  PSNR: 24.56754493713379\n",
            "[TRAIN] Iter: 176100 Loss: 0.009091880172491074  PSNR: 24.881263732910156\n",
            "[TRAIN] Iter: 176200 Loss: 0.00819417368620634  PSNR: 25.265365600585938\n",
            "[TRAIN] Iter: 176300 Loss: 0.008427361026406288  PSNR: 24.339691162109375\n",
            "[TRAIN] Iter: 176400 Loss: 0.010371881537139416  PSNR: 23.832592010498047\n",
            "[TRAIN] Iter: 176500 Loss: 0.006391415372490883  PSNR: 26.227460861206055\n",
            "[TRAIN] Iter: 176600 Loss: 0.010280463844537735  PSNR: 23.97650718688965\n",
            "[TRAIN] Iter: 176700 Loss: 0.008512472733855247  PSNR: 24.86722755432129\n",
            "[TRAIN] Iter: 176800 Loss: 0.007680976763367653  PSNR: 25.677833557128906\n",
            "[TRAIN] Iter: 176900 Loss: 0.006262162234634161  PSNR: 26.039743423461914\n",
            "[TRAIN] Iter: 177000 Loss: 0.00944901816546917  PSNR: 23.631635665893555\n",
            "[TRAIN] Iter: 177100 Loss: 0.010436201468110085  PSNR: 23.985822677612305\n",
            "[TRAIN] Iter: 177200 Loss: 0.01142448652535677  PSNR: 23.51396942138672\n",
            "[TRAIN] Iter: 177300 Loss: 0.006152578163892031  PSNR: 26.790861129760742\n",
            "[TRAIN] Iter: 177400 Loss: 0.009746864438056946  PSNR: 23.943187713623047\n",
            "[TRAIN] Iter: 177500 Loss: 0.007806585170328617  PSNR: 25.615583419799805\n",
            "[TRAIN] Iter: 177600 Loss: 0.006156563758850098  PSNR: 26.771888732910156\n",
            "[TRAIN] Iter: 177700 Loss: 0.011386632919311523  PSNR: 23.39129066467285\n",
            "[TRAIN] Iter: 177800 Loss: 0.009803524240851402  PSNR: 23.64116668701172\n",
            "[TRAIN] Iter: 177900 Loss: 0.01053063478320837  PSNR: 23.592531204223633\n",
            "[TRAIN] Iter: 178000 Loss: 0.00960815604776144  PSNR: 24.312429428100586\n",
            "[TRAIN] Iter: 178100 Loss: 0.007817698642611504  PSNR: 25.019107818603516\n",
            "[TRAIN] Iter: 178200 Loss: 0.01305987499654293  PSNR: 22.911907196044922\n",
            "[TRAIN] Iter: 178300 Loss: 0.012523637153208256  PSNR: 22.56792640686035\n",
            "[TRAIN] Iter: 178400 Loss: 0.009474677965044975  PSNR: 24.13068199157715\n",
            "[TRAIN] Iter: 178500 Loss: 0.008956719189882278  PSNR: 24.221637725830078\n",
            "[TRAIN] Iter: 178600 Loss: 0.008724120445549488  PSNR: 25.172800064086914\n",
            "[TRAIN] Iter: 178700 Loss: 0.00773578230291605  PSNR: 24.688518524169922\n",
            "[TRAIN] Iter: 178800 Loss: 0.009206985123455524  PSNR: 24.085166931152344\n",
            "[TRAIN] Iter: 178900 Loss: 0.008688073605298996  PSNR: 24.651735305786133\n",
            "[TRAIN] Iter: 179000 Loss: 0.010644935071468353  PSNR: 23.528776168823242\n",
            "[TRAIN] Iter: 179100 Loss: 0.010708230547606945  PSNR: 24.283424377441406\n",
            "[TRAIN] Iter: 179200 Loss: 0.012340164743363857  PSNR: 23.603300094604492\n",
            "[TRAIN] Iter: 179300 Loss: 0.006495459005236626  PSNR: 25.39719009399414\n",
            "[TRAIN] Iter: 179400 Loss: 0.009325245395302773  PSNR: 24.21384620666504\n",
            "[TRAIN] Iter: 179500 Loss: 0.008784491568803787  PSNR: 24.563968658447266\n",
            "[TRAIN] Iter: 179600 Loss: 0.009506681933999062  PSNR: 24.344972610473633\n",
            "[TRAIN] Iter: 179700 Loss: 0.009177825413644314  PSNR: 24.310802459716797\n",
            "[TRAIN] Iter: 179800 Loss: 0.009292731061577797  PSNR: 24.215272903442383\n",
            "[TRAIN] Iter: 179900 Loss: 0.007326092105358839  PSNR: 26.073118209838867\n",
            " 90% 179999/200000 [5:52:38<38:31,  8.65it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/180000.tar\n",
            "[TRAIN] Iter: 180000 Loss: 0.01306643895804882  PSNR: 22.594364166259766\n",
            "[TRAIN] Iter: 180100 Loss: 0.00851648673415184  PSNR: 24.85895347595215\n",
            "[TRAIN] Iter: 180200 Loss: 0.010127396322786808  PSNR: 24.356613159179688\n",
            "[TRAIN] Iter: 180300 Loss: 0.01174466498196125  PSNR: 23.085309982299805\n",
            "[TRAIN] Iter: 180400 Loss: 0.0109182707965374  PSNR: 23.237838745117188\n",
            "[TRAIN] Iter: 180500 Loss: 0.011468404904007912  PSNR: 24.01881217956543\n",
            "[TRAIN] Iter: 180600 Loss: 0.009855843149125576  PSNR: 23.774599075317383\n",
            "[TRAIN] Iter: 180700 Loss: 0.007995158433914185  PSNR: 24.3675479888916\n",
            "[TRAIN] Iter: 180800 Loss: 0.009245375171303749  PSNR: 24.17220115661621\n",
            "[TRAIN] Iter: 180900 Loss: 0.00742461858317256  PSNR: 25.60235023498535\n",
            "[TRAIN] Iter: 181000 Loss: 0.00846740510314703  PSNR: 24.391815185546875\n",
            "[TRAIN] Iter: 181100 Loss: 0.009011372923851013  PSNR: 24.238801956176758\n",
            "[TRAIN] Iter: 181200 Loss: 0.00683700293302536  PSNR: 25.72049903869629\n",
            "[TRAIN] Iter: 181300 Loss: 0.008640885353088379  PSNR: 25.50271987915039\n",
            "[TRAIN] Iter: 181400 Loss: 0.012142783030867577  PSNR: 23.306472778320312\n",
            "[TRAIN] Iter: 181500 Loss: 0.014066190458834171  PSNR: 22.236045837402344\n",
            "[TRAIN] Iter: 181600 Loss: 0.009660037234425545  PSNR: 23.969562530517578\n",
            "[TRAIN] Iter: 181700 Loss: 0.014795569702982903  PSNR: 22.30817222595215\n",
            "[TRAIN] Iter: 181800 Loss: 0.00594035629183054  PSNR: 26.229768753051758\n",
            "[TRAIN] Iter: 181900 Loss: 0.010151108726859093  PSNR: 23.61366844177246\n",
            "[TRAIN] Iter: 182000 Loss: 0.008268464356660843  PSNR: 25.25830078125\n",
            "[TRAIN] Iter: 182100 Loss: 0.011875161901116371  PSNR: 22.684968948364258\n",
            "[TRAIN] Iter: 182200 Loss: 0.0075811780989170074  PSNR: 25.726211547851562\n",
            "[TRAIN] Iter: 182300 Loss: 0.005806853994727135  PSNR: 26.4669132232666\n",
            "[TRAIN] Iter: 182400 Loss: 0.007869455963373184  PSNR: 25.571874618530273\n",
            "[TRAIN] Iter: 182500 Loss: 0.009494177997112274  PSNR: 24.36803436279297\n",
            "[TRAIN] Iter: 182600 Loss: 0.008922944776713848  PSNR: 24.10875701904297\n",
            "[TRAIN] Iter: 182700 Loss: 0.008865665644407272  PSNR: 23.788610458374023\n",
            "[TRAIN] Iter: 182800 Loss: 0.008213745430111885  PSNR: 24.972694396972656\n",
            "[TRAIN] Iter: 182900 Loss: 0.007453384343534708  PSNR: 25.36111068725586\n",
            "[TRAIN] Iter: 183000 Loss: 0.008337492123246193  PSNR: 24.65520668029785\n",
            "[TRAIN] Iter: 183100 Loss: 0.006930484436452389  PSNR: 26.011499404907227\n",
            "[TRAIN] Iter: 183200 Loss: 0.006015670485794544  PSNR: 25.582103729248047\n",
            "[TRAIN] Iter: 183300 Loss: 0.007791206240653992  PSNR: 25.70001792907715\n",
            "[TRAIN] Iter: 183400 Loss: 0.008261180482804775  PSNR: 25.436321258544922\n",
            "[TRAIN] Iter: 183500 Loss: 0.010814319364726543  PSNR: 23.71918487548828\n",
            "[TRAIN] Iter: 183600 Loss: 0.007986201904714108  PSNR: 24.606945037841797\n",
            "[TRAIN] Iter: 183700 Loss: 0.012390892952680588  PSNR: 23.626670837402344\n",
            "[TRAIN] Iter: 183800 Loss: 0.008262187242507935  PSNR: 24.553115844726562\n",
            "[TRAIN] Iter: 183900 Loss: 0.005471762735396624  PSNR: 26.89171028137207\n",
            "[TRAIN] Iter: 184000 Loss: 0.008543528616428375  PSNR: 25.67833709716797\n",
            "[TRAIN] Iter: 184100 Loss: 0.0083934860303998  PSNR: 23.970909118652344\n",
            "[TRAIN] Iter: 184200 Loss: 0.006938284263014793  PSNR: 26.178150177001953\n",
            "[TRAIN] Iter: 184300 Loss: 0.007779859937727451  PSNR: 25.367328643798828\n",
            "[TRAIN] Iter: 184400 Loss: 0.00937026459723711  PSNR: 24.512929916381836\n",
            "[TRAIN] Iter: 184500 Loss: 0.007027283310890198  PSNR: 25.28172492980957\n",
            "[TRAIN] Iter: 184600 Loss: 0.01016614306718111  PSNR: 23.83387565612793\n",
            "[TRAIN] Iter: 184700 Loss: 0.007242040708661079  PSNR: 25.343032836914062\n",
            "[TRAIN] Iter: 184800 Loss: 0.008028760552406311  PSNR: 24.93425178527832\n",
            "[TRAIN] Iter: 184900 Loss: 0.006025150418281555  PSNR: 26.35686492919922\n",
            "[TRAIN] Iter: 185000 Loss: 0.009594950824975967  PSNR: 24.134485244750977\n",
            "[TRAIN] Iter: 185100 Loss: 0.008552039042115211  PSNR: 25.62699317932129\n",
            "[TRAIN] Iter: 185200 Loss: 0.007135397754609585  PSNR: 24.659807205200195\n",
            "[TRAIN] Iter: 185300 Loss: 0.006189723499119282  PSNR: 26.452552795410156\n",
            "[TRAIN] Iter: 185400 Loss: 0.008410978130996227  PSNR: 24.34419822692871\n",
            "[TRAIN] Iter: 185500 Loss: 0.01192924752831459  PSNR: 22.56221580505371\n",
            "[TRAIN] Iter: 185600 Loss: 0.005573223810642958  PSNR: 26.950298309326172\n",
            "[TRAIN] Iter: 185700 Loss: 0.010461145080626011  PSNR: 23.663070678710938\n",
            "[TRAIN] Iter: 185800 Loss: 0.007796761579811573  PSNR: 25.935503005981445\n",
            "[TRAIN] Iter: 185900 Loss: 0.010236495174467564  PSNR: 24.231887817382812\n",
            "[TRAIN] Iter: 186000 Loss: 0.006128603592514992  PSNR: 26.42499542236328\n",
            "[TRAIN] Iter: 186100 Loss: 0.006207702681422234  PSNR: 26.20263671875\n",
            "[TRAIN] Iter: 186200 Loss: 0.00771359633654356  PSNR: 25.422754287719727\n",
            "[TRAIN] Iter: 186300 Loss: 0.009083323180675507  PSNR: 24.569534301757812\n",
            "[TRAIN] Iter: 186400 Loss: 0.0051782261580228806  PSNR: 27.138185501098633\n",
            "[TRAIN] Iter: 186500 Loss: 0.008810481987893581  PSNR: 24.578712463378906\n",
            "[TRAIN] Iter: 186600 Loss: 0.008382730185985565  PSNR: 24.885786056518555\n",
            "[TRAIN] Iter: 186700 Loss: 0.01248306967318058  PSNR: 22.618879318237305\n",
            "[TRAIN] Iter: 186800 Loss: 0.005938359536230564  PSNR: 25.83904266357422\n",
            "[TRAIN] Iter: 186900 Loss: 0.009131252765655518  PSNR: 24.529098510742188\n",
            "[TRAIN] Iter: 187000 Loss: 0.007134164217859507  PSNR: 25.455076217651367\n",
            "[TRAIN] Iter: 187100 Loss: 0.01022278144955635  PSNR: 24.207204818725586\n",
            "[TRAIN] Iter: 187200 Loss: 0.008859380148351192  PSNR: 24.89278793334961\n",
            "[TRAIN] Iter: 187300 Loss: 0.00949222780764103  PSNR: 23.871078491210938\n",
            "[TRAIN] Iter: 187400 Loss: 0.010009419173002243  PSNR: 24.45050811767578\n",
            "[TRAIN] Iter: 187500 Loss: 0.00633500749245286  PSNR: 26.068099975585938\n",
            "[TRAIN] Iter: 187600 Loss: 0.010033853352069855  PSNR: 23.1828670501709\n",
            "[TRAIN] Iter: 187700 Loss: 0.011707721278071404  PSNR: 23.601411819458008\n",
            "[TRAIN] Iter: 187800 Loss: 0.01051806379109621  PSNR: 24.004770278930664\n",
            "[TRAIN] Iter: 187900 Loss: 0.006365332752466202  PSNR: 27.41631317138672\n",
            "[TRAIN] Iter: 188000 Loss: 0.005451885052025318  PSNR: 27.338483810424805\n",
            "[TRAIN] Iter: 188100 Loss: 0.007252201903611422  PSNR: 25.52582359313965\n",
            "[TRAIN] Iter: 188200 Loss: 0.008918555453419685  PSNR: 24.378578186035156\n",
            "[TRAIN] Iter: 188300 Loss: 0.010232292115688324  PSNR: 23.96677017211914\n",
            "[TRAIN] Iter: 188400 Loss: 0.006855628453195095  PSNR: 25.77820587158203\n",
            "[TRAIN] Iter: 188500 Loss: 0.006159433163702488  PSNR: 26.44280433654785\n",
            "[TRAIN] Iter: 188600 Loss: 0.00923885591328144  PSNR: 24.031034469604492\n",
            "[TRAIN] Iter: 188700 Loss: 0.007720991037786007  PSNR: 24.74015998840332\n",
            "[TRAIN] Iter: 188800 Loss: 0.007177138701081276  PSNR: 25.254409790039062\n",
            "[TRAIN] Iter: 188900 Loss: 0.008572207763791084  PSNR: 24.604267120361328\n",
            "[TRAIN] Iter: 189000 Loss: 0.010502999648451805  PSNR: 23.114816665649414\n",
            "[TRAIN] Iter: 189100 Loss: 0.009259713813662529  PSNR: 24.11371421813965\n",
            "[TRAIN] Iter: 189200 Loss: 0.012640220113098621  PSNR: 22.895164489746094\n",
            "[TRAIN] Iter: 189300 Loss: 0.010413271375000477  PSNR: 23.90213966369629\n",
            "[TRAIN] Iter: 189400 Loss: 0.01019496750086546  PSNR: 23.68226432800293\n",
            "[TRAIN] Iter: 189500 Loss: 0.007180444896221161  PSNR: 25.292190551757812\n",
            "[TRAIN] Iter: 189600 Loss: 0.005578459240496159  PSNR: 26.80998992919922\n",
            "[TRAIN] Iter: 189700 Loss: 0.006357061676681042  PSNR: 25.860170364379883\n",
            "[TRAIN] Iter: 189800 Loss: 0.006918394006788731  PSNR: 26.296428680419922\n",
            "[TRAIN] Iter: 189900 Loss: 0.0073834918439388275  PSNR: 24.813108444213867\n",
            " 95% 189999/200000 [6:12:05<19:26,  8.57it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/190000.tar\n",
            "[TRAIN] Iter: 190000 Loss: 0.007978497073054314  PSNR: 24.719287872314453\n",
            "[TRAIN] Iter: 190100 Loss: 0.008886256255209446  PSNR: 25.32955551147461\n",
            "[TRAIN] Iter: 190200 Loss: 0.006680251099169254  PSNR: 25.99627685546875\n",
            "[TRAIN] Iter: 190300 Loss: 0.007033799309283495  PSNR: 24.9996395111084\n",
            "[TRAIN] Iter: 190400 Loss: 0.00954956654459238  PSNR: 24.807252883911133\n",
            "[TRAIN] Iter: 190500 Loss: 0.012052860110998154  PSNR: 22.7932186126709\n",
            "[TRAIN] Iter: 190600 Loss: 0.010700266808271408  PSNR: 23.73426628112793\n",
            "[TRAIN] Iter: 190700 Loss: 0.008686728775501251  PSNR: 25.619413375854492\n",
            "[TRAIN] Iter: 190800 Loss: 0.006527378223836422  PSNR: 25.61427116394043\n",
            "[TRAIN] Iter: 190900 Loss: 0.007545784115791321  PSNR: 24.87624740600586\n",
            "[TRAIN] Iter: 191000 Loss: 0.006088046357035637  PSNR: 26.593090057373047\n",
            "[TRAIN] Iter: 191100 Loss: 0.008638143539428711  PSNR: 24.8906307220459\n",
            "[TRAIN] Iter: 191200 Loss: 0.007011808454990387  PSNR: 25.53374481201172\n",
            "[TRAIN] Iter: 191300 Loss: 0.006353070959448814  PSNR: 25.915769577026367\n",
            "[TRAIN] Iter: 191400 Loss: 0.00963823776692152  PSNR: 24.62868881225586\n",
            "[TRAIN] Iter: 191500 Loss: 0.008654252626001835  PSNR: 25.101764678955078\n",
            "[TRAIN] Iter: 191600 Loss: 0.007774548605084419  PSNR: 25.891298294067383\n",
            "[TRAIN] Iter: 191700 Loss: 0.008586687967181206  PSNR: 24.872426986694336\n",
            "[TRAIN] Iter: 191800 Loss: 0.009937172755599022  PSNR: 23.439048767089844\n",
            "[TRAIN] Iter: 191900 Loss: 0.01053977757692337  PSNR: 23.601102828979492\n",
            "[TRAIN] Iter: 192000 Loss: 0.007317747455090284  PSNR: 25.697416305541992\n",
            "[TRAIN] Iter: 192100 Loss: 0.011369835585355759  PSNR: 23.17173957824707\n",
            "[TRAIN] Iter: 192200 Loss: 0.014755068346858025  PSNR: 22.1988468170166\n",
            "[TRAIN] Iter: 192300 Loss: 0.007584521546959877  PSNR: 24.753448486328125\n",
            "[TRAIN] Iter: 192400 Loss: 0.00807398185133934  PSNR: 25.28548240661621\n",
            "[TRAIN] Iter: 192500 Loss: 0.010662244632840157  PSNR: 24.182273864746094\n",
            "[TRAIN] Iter: 192600 Loss: 0.006454896181821823  PSNR: 25.82061767578125\n",
            "[TRAIN] Iter: 192700 Loss: 0.006433720700442791  PSNR: 25.91007423400879\n",
            "[TRAIN] Iter: 192800 Loss: 0.012002572417259216  PSNR: 23.29266357421875\n",
            "[TRAIN] Iter: 192900 Loss: 0.01076565869152546  PSNR: 23.558116912841797\n",
            "[TRAIN] Iter: 193000 Loss: 0.00947011262178421  PSNR: 24.49104118347168\n",
            "[TRAIN] Iter: 193100 Loss: 0.007737705484032631  PSNR: 25.637393951416016\n",
            "[TRAIN] Iter: 193200 Loss: 0.011110227555036545  PSNR: 23.56497573852539\n",
            "[TRAIN] Iter: 193300 Loss: 0.007633711211383343  PSNR: 25.34808349609375\n",
            "[TRAIN] Iter: 193400 Loss: 0.007646022364497185  PSNR: 25.334068298339844\n",
            "[TRAIN] Iter: 193500 Loss: 0.01002255268394947  PSNR: 23.746122360229492\n",
            "[TRAIN] Iter: 193600 Loss: 0.010260259732604027  PSNR: 24.819210052490234\n",
            "[TRAIN] Iter: 193700 Loss: 0.010113632306456566  PSNR: 23.86150360107422\n",
            "[TRAIN] Iter: 193800 Loss: 0.005840432830154896  PSNR: 26.431835174560547\n",
            "[TRAIN] Iter: 193900 Loss: 0.0066363029181957245  PSNR: 25.3187313079834\n",
            "[TRAIN] Iter: 194000 Loss: 0.005862793885171413  PSNR: 26.589509963989258\n",
            "[TRAIN] Iter: 194100 Loss: 0.010016610845923424  PSNR: 23.836883544921875\n",
            "[TRAIN] Iter: 194200 Loss: 0.007729783654212952  PSNR: 25.51272201538086\n",
            "[TRAIN] Iter: 194300 Loss: 0.008500149473547935  PSNR: 24.46533203125\n",
            "[TRAIN] Iter: 194400 Loss: 0.006689294707030058  PSNR: 26.43379783630371\n",
            "[TRAIN] Iter: 194500 Loss: 0.0051757642067968845  PSNR: 27.154460906982422\n",
            "[TRAIN] Iter: 194600 Loss: 0.00737968971952796  PSNR: 25.12382698059082\n",
            "[TRAIN] Iter: 194700 Loss: 0.01118793711066246  PSNR: 24.146255493164062\n",
            "[TRAIN] Iter: 194800 Loss: 0.006676255259662867  PSNR: 25.80433464050293\n",
            "[TRAIN] Iter: 194900 Loss: 0.006999853067100048  PSNR: 25.510055541992188\n",
            "[TRAIN] Iter: 195000 Loss: 0.00680733984336257  PSNR: 25.209245681762695\n",
            "[TRAIN] Iter: 195100 Loss: 0.005764826200902462  PSNR: 26.504281997680664\n",
            "[TRAIN] Iter: 195200 Loss: 0.008930412121117115  PSNR: 24.577009201049805\n",
            "[TRAIN] Iter: 195300 Loss: 0.007111538201570511  PSNR: 25.127687454223633\n",
            "[TRAIN] Iter: 195400 Loss: 0.010199065320193768  PSNR: 24.695716857910156\n",
            "[TRAIN] Iter: 195500 Loss: 0.009776579216122627  PSNR: 23.853437423706055\n",
            "[TRAIN] Iter: 195600 Loss: 0.012509625405073166  PSNR: 23.247671127319336\n",
            "[TRAIN] Iter: 195700 Loss: 0.012482455000281334  PSNR: 23.166812896728516\n",
            "[TRAIN] Iter: 195800 Loss: 0.00854487158358097  PSNR: 25.148935317993164\n",
            "[TRAIN] Iter: 195900 Loss: 0.009007938206195831  PSNR: 23.969799041748047\n",
            "[TRAIN] Iter: 196000 Loss: 0.008991166017949581  PSNR: 24.825191497802734\n",
            "[TRAIN] Iter: 196100 Loss: 0.009842992760241032  PSNR: 23.78044891357422\n",
            "[TRAIN] Iter: 196200 Loss: 0.009434523992240429  PSNR: 24.223583221435547\n",
            "[TRAIN] Iter: 196300 Loss: 0.00985003262758255  PSNR: 24.561185836791992\n",
            "[TRAIN] Iter: 196400 Loss: 0.007923335768282413  PSNR: 25.6263370513916\n",
            "[TRAIN] Iter: 196500 Loss: 0.005871419794857502  PSNR: 25.940088272094727\n",
            "[TRAIN] Iter: 196600 Loss: 0.006748849991708994  PSNR: 26.008438110351562\n",
            "[TRAIN] Iter: 196700 Loss: 0.008747226558625698  PSNR: 24.63507652282715\n",
            "[TRAIN] Iter: 196800 Loss: 0.008261623792350292  PSNR: 24.808507919311523\n",
            "[TRAIN] Iter: 196900 Loss: 0.01312054693698883  PSNR: 22.827924728393555\n",
            "[TRAIN] Iter: 197000 Loss: 0.0066857123747467995  PSNR: 27.076993942260742\n",
            "[TRAIN] Iter: 197100 Loss: 0.013242362067103386  PSNR: 23.179744720458984\n",
            "[TRAIN] Iter: 197200 Loss: 0.010695556178689003  PSNR: 24.041061401367188\n",
            "[TRAIN] Iter: 197300 Loss: 0.006645963527262211  PSNR: 26.221399307250977\n",
            "[TRAIN] Iter: 197400 Loss: 0.007611280772835016  PSNR: 25.7425479888916\n",
            "[TRAIN] Iter: 197500 Loss: 0.006836422719061375  PSNR: 25.60991096496582\n",
            "[TRAIN] Iter: 197600 Loss: 0.006417530123144388  PSNR: 26.51427459716797\n",
            "[TRAIN] Iter: 197700 Loss: 0.008170567452907562  PSNR: 24.772136688232422\n",
            "[TRAIN] Iter: 197800 Loss: 0.010507811792194843  PSNR: 23.6569766998291\n",
            "[TRAIN] Iter: 197900 Loss: 0.006200927309691906  PSNR: 25.77163314819336\n",
            "[TRAIN] Iter: 198000 Loss: 0.00768035463988781  PSNR: 25.19362449645996\n",
            "[TRAIN] Iter: 198100 Loss: 0.007200208492577076  PSNR: 25.82310676574707\n",
            "[TRAIN] Iter: 198200 Loss: 0.010509252548217773  PSNR: 23.89420509338379\n",
            "[TRAIN] Iter: 198300 Loss: 0.007690358906984329  PSNR: 25.404043197631836\n",
            "[TRAIN] Iter: 198400 Loss: 0.00887906551361084  PSNR: 24.17902183532715\n",
            "[TRAIN] Iter: 198500 Loss: 0.009840365499258041  PSNR: 24.236982345581055\n",
            "[TRAIN] Iter: 198600 Loss: 0.007425394374877214  PSNR: 25.49713897705078\n",
            "[TRAIN] Iter: 198700 Loss: 0.00860493816435337  PSNR: 25.525619506835938\n",
            "[TRAIN] Iter: 198800 Loss: 0.007982511073350906  PSNR: 25.28193473815918\n",
            "[TRAIN] Iter: 198900 Loss: 0.008468803949654102  PSNR: 25.357046127319336\n",
            "[TRAIN] Iter: 199000 Loss: 0.008737047202885151  PSNR: 24.71174430847168\n",
            "[TRAIN] Iter: 199100 Loss: 0.00647836783900857  PSNR: 26.73076820373535\n",
            "[TRAIN] Iter: 199200 Loss: 0.008523322641849518  PSNR: 25.05927276611328\n",
            "[TRAIN] Iter: 199300 Loss: 0.01146725658327341  PSNR: 23.216915130615234\n",
            "[TRAIN] Iter: 199400 Loss: 0.011218057945370674  PSNR: 23.795717239379883\n",
            "[TRAIN] Iter: 199500 Loss: 0.009538860991597176  PSNR: 23.88432502746582\n",
            "[TRAIN] Iter: 199600 Loss: 0.008931517601013184  PSNR: 24.78110694885254\n",
            "[TRAIN] Iter: 199700 Loss: 0.006470703519880772  PSNR: 25.208433151245117\n",
            "[TRAIN] Iter: 199800 Loss: 0.005844990722835064  PSNR: 26.699771881103516\n",
            "[TRAIN] Iter: 199900 Loss: 0.011494405567646027  PSNR: 24.236783981323242\n",
            "100% 199999/200000 [6:31:39<00:00,  8.54it/s]Saved checkpoints at ./logs/blender_paper_bottle_flip_no_encoding/200000.tar\n",
            "test poses shape torch.Size([200, 4, 4])\n",
            "\n",
            "  0% 0/200 [00:00<?, ?it/s]\u001b[A0 0.0005769729614257812\n",
            "torch.Size([800, 800, 3]) torch.Size([800, 800])\n",
            "\n",
            "  0% 1/200 [00:21<1:11:27, 21.54s/it]\u001b[A1 21.54414987564087\n",
            "\n",
            "  1% 2/200 [00:43<1:11:01, 21.52s/it]\u001b[A2 21.504024267196655\n",
            "\n",
            "  2% 3/200 [01:04<1:10:38, 21.51s/it]\u001b[A3 21.504138231277466\n",
            "\n",
            "  2% 4/200 [01:26<1:10:15, 21.51s/it]\u001b[A4 21.495973825454712\n",
            "\n",
            "  2% 5/200 [01:47<1:09:53, 21.51s/it]\u001b[A5 21.50715208053589\n",
            "\n",
            "  3% 6/200 [02:09<1:09:32, 21.51s/it]\u001b[A6 21.507091283798218\n",
            "\n",
            "  4% 7/200 [02:30<1:09:10, 21.51s/it]\u001b[A7 21.50919246673584\n",
            "\n",
            "  4% 8/200 [02:52<1:08:49, 21.51s/it]\u001b[A8 21.51094961166382\n",
            "\n",
            "  4% 9/200 [03:13<1:08:28, 21.51s/it]\u001b[A9 21.515311002731323\n",
            "\n",
            "  5% 10/200 [03:35<1:08:07, 21.51s/it]\u001b[A10 21.514024257659912\n",
            "\n",
            "  6% 11/200 [03:56<1:07:46, 21.51s/it]\u001b[A11 21.521329164505005\n",
            "\n",
            "  6% 12/200 [04:18<1:07:24, 21.52s/it]\u001b[A12 21.518181800842285\n",
            "\n",
            "  6% 13/200 [04:39<1:07:03, 21.52s/it]\u001b[A13 21.515581369400024\n",
            "\n",
            "  7% 14/200 [05:01<1:06:41, 21.52s/it]\u001b[A14 21.51647686958313\n",
            "\n",
            "  8% 15/200 [05:22<1:06:19, 21.51s/it]\u001b[A15 21.503555059432983\n",
            "\n",
            "  8% 16/200 [05:44<1:05:58, 21.51s/it]\u001b[A16 21.517230987548828\n",
            "\n",
            "  8% 17/200 [06:05<1:05:37, 21.52s/it]\u001b[A17 21.524150133132935\n",
            "\n",
            "  9% 18/200 [06:27<1:05:16, 21.52s/it]\u001b[A18 21.51989507675171\n",
            "\n",
            " 10% 19/200 [06:48<1:04:54, 21.51s/it]\u001b[A19 21.508288860321045\n",
            "\n",
            " 10% 20/200 [07:10<1:04:32, 21.51s/it]\u001b[A20 21.513277530670166\n",
            "\n",
            " 10% 21/200 [07:31<1:04:10, 21.51s/it]\u001b[A21 21.510212421417236\n",
            "\n",
            " 11% 22/200 [07:53<1:03:49, 21.51s/it]\u001b[A22 21.510987520217896\n",
            "\n",
            " 12% 23/200 [08:14<1:03:27, 21.51s/it]\u001b[A23 21.50644087791443\n",
            "\n",
            " 12% 24/200 [08:36<1:03:06, 21.51s/it]\u001b[A24 21.519421100616455\n",
            "\n",
            " 12% 25/200 [08:57<1:02:45, 21.52s/it]\u001b[A25 21.51988172531128\n",
            "\n",
            " 13% 26/200 [09:19<1:02:23, 21.51s/it]\u001b[A26 21.50866413116455\n",
            "\n",
            " 14% 27/200 [09:40<1:02:01, 21.51s/it]\u001b[A27 21.50312304496765\n",
            "\n",
            " 14% 28/200 [10:02<1:01:40, 21.51s/it]\u001b[A28 21.514888286590576\n",
            "\n",
            " 14% 29/200 [10:23<1:01:18, 21.51s/it]\u001b[A29 21.508490562438965\n",
            "\n",
            " 15% 30/200 [10:45<1:00:56, 21.51s/it]\u001b[A30 21.510141372680664\n",
            "\n",
            " 16% 31/200 [11:06<1:00:35, 21.51s/it]\u001b[A31 21.514791250228882\n",
            "\n",
            " 16% 32/200 [11:28<1:00:14, 21.52s/it]\u001b[A32 21.524329900741577\n",
            "\n",
            " 16% 33/200 [11:49<59:53, 21.52s/it]  \u001b[A33 21.514700412750244\n",
            "\n",
            " 17% 34/200 [12:11<59:31, 21.52s/it]\u001b[A34 21.51461124420166\n",
            "\n",
            " 18% 35/200 [12:32<59:09, 21.51s/it]\u001b[A35 21.499510765075684\n",
            "\n",
            " 18% 36/200 [12:54<58:48, 21.51s/it]\u001b[A36 21.518303871154785\n",
            "\n",
            " 18% 37/200 [13:15<58:26, 21.51s/it]\u001b[A37 21.516587495803833\n",
            "\n",
            " 19% 38/200 [13:37<58:05, 21.51s/it]\u001b[A38 21.510968446731567\n",
            "\n",
            " 20% 39/200 [13:58<57:42, 21.51s/it]\u001b[A39 21.49855399131775\n",
            "\n",
            " 20% 40/200 [14:20<57:21, 21.51s/it]\u001b[A40 21.515252113342285\n",
            "\n",
            " 20% 41/200 [14:42<56:59, 21.51s/it]\u001b[A41 21.50305962562561\n",
            "\n",
            " 21% 42/200 [15:03<56:38, 21.51s/it]\u001b[A42 21.517980098724365\n",
            "\n",
            " 22% 43/200 [15:25<56:16, 21.51s/it]\u001b[A43 21.50182342529297\n",
            "\n",
            " 22% 44/200 [15:46<55:55, 21.51s/it]\u001b[A44 21.501891374588013\n",
            "\n",
            " 22% 45/200 [16:08<55:33, 21.51s/it]\u001b[A45 21.50504422187805\n",
            "\n",
            " 23% 46/200 [16:29<55:11, 21.51s/it]\u001b[A46 21.505168199539185\n",
            "\n",
            " 24% 47/200 [16:51<54:49, 21.50s/it]\u001b[A47 21.491209506988525\n",
            "\n",
            " 24% 48/200 [17:12<54:28, 21.50s/it]\u001b[A48 21.50393056869507\n",
            "\n",
            " 24% 49/200 [17:34<54:06, 21.50s/it]\u001b[A49 21.497997760772705\n",
            "\n",
            " 25% 50/200 [17:55<53:45, 21.50s/it]\u001b[A50 21.514169216156006\n",
            "\n",
            " 26% 51/200 [18:17<53:23, 21.50s/it]\u001b[A51 21.498822689056396\n",
            "\n",
            " 26% 52/200 [18:38<53:02, 21.51s/it]\u001b[A52 21.513347864151\n",
            "\n",
            " 26% 53/200 [19:00<52:41, 21.51s/it]\u001b[A53 21.507214307785034\n",
            "\n",
            " 27% 54/200 [19:21<52:20, 21.51s/it]\u001b[A54 21.51485252380371\n",
            "\n",
            " 28% 55/200 [19:43<51:58, 21.51s/it]\u001b[A55 21.505322456359863\n",
            "\n",
            " 28% 56/200 [20:04<51:37, 21.51s/it]\u001b[A56 21.510977745056152\n",
            "\n",
            " 28% 57/200 [20:26<51:15, 21.51s/it]\u001b[A57 21.51280426979065\n",
            "\n",
            " 29% 58/200 [20:47<50:54, 21.51s/it]\u001b[A58 21.516590118408203\n",
            "\n",
            " 30% 59/200 [21:09<50:32, 21.51s/it]\u001b[A59 21.505736589431763\n",
            "\n",
            " 30% 60/200 [21:30<50:11, 21.51s/it]\u001b[A60 21.50999164581299\n",
            "\n",
            " 30% 61/200 [21:52<49:49, 21.51s/it]\u001b[A61 21.50811505317688\n",
            "\n",
            " 31% 62/200 [22:13<49:28, 21.51s/it]\u001b[A62 21.515172958374023\n",
            "\n",
            " 32% 63/200 [22:35<49:06, 21.51s/it]\u001b[A63 21.50082802772522\n",
            "\n",
            " 32% 64/200 [22:56<48:45, 21.51s/it]\u001b[A64 21.52039670944214\n",
            "\n",
            " 32% 65/200 [23:18<48:23, 21.51s/it]\u001b[A65 21.50917363166809\n",
            "\n",
            " 33% 66/200 [23:39<48:02, 21.51s/it]\u001b[A66 21.51823329925537\n",
            "\n",
            " 34% 67/200 [24:01<47:41, 21.51s/it]\u001b[A67 21.509794235229492\n",
            "\n",
            " 34% 68/200 [24:22<47:19, 21.51s/it]\u001b[A68 21.516284704208374\n",
            "\n",
            " 34% 69/200 [24:44<46:58, 21.51s/it]\u001b[A69 21.511473655700684\n",
            "\n",
            " 35% 70/200 [25:05<46:36, 21.51s/it]\u001b[A70 21.511785745620728\n",
            "\n",
            " 36% 71/200 [25:27<46:14, 21.51s/it]\u001b[A71 21.505964756011963\n",
            "\n",
            " 36% 72/200 [25:48<45:53, 21.51s/it]\u001b[A72 21.525171279907227\n",
            "\n",
            " 36% 73/200 [26:10<45:32, 21.51s/it]\u001b[A73 21.50956892967224\n",
            "\n",
            " 37% 74/200 [26:31<45:10, 21.52s/it]\u001b[A74 21.520854711532593\n",
            "\n",
            " 38% 75/200 [26:53<44:49, 21.52s/it]\u001b[A75 21.516299724578857\n",
            "\n",
            " 38% 76/200 [27:14<44:27, 21.52s/it]\u001b[A76 21.515469551086426\n",
            "\n",
            " 38% 77/200 [27:36<44:06, 21.52s/it]\u001b[A77 21.51849627494812\n",
            "\n",
            " 39% 78/200 [27:57<43:45, 21.52s/it]\u001b[A78 21.526782751083374\n",
            "\n",
            " 40% 79/200 [28:19<43:23, 21.52s/it]\u001b[A79 21.51891851425171\n",
            "\n",
            " 40% 80/200 [28:40<43:02, 21.52s/it]\u001b[A80 21.520233154296875\n",
            "\n",
            " 40% 81/200 [29:02<42:40, 21.52s/it]\u001b[A81 21.514544248580933\n",
            "\n",
            " 41% 82/200 [29:23<42:19, 21.52s/it]\u001b[A82 21.523061752319336\n",
            "\n",
            " 42% 83/200 [29:45<41:57, 21.52s/it]\u001b[A83 21.511346340179443\n",
            "\n",
            " 42% 84/200 [30:07<41:36, 21.52s/it]\u001b[A84 21.52066922187805\n",
            "\n",
            " 42% 85/200 [30:28<41:14, 21.52s/it]\u001b[A85 21.518662691116333\n",
            "\n",
            " 43% 86/200 [30:50<40:53, 21.52s/it]\u001b[A86 21.52365732192993\n",
            "\n",
            " 44% 87/200 [31:11<40:31, 21.52s/it]\u001b[A87 21.51558208465576\n",
            "\n",
            " 44% 88/200 [31:33<40:10, 21.52s/it]\u001b[A88 21.52578115463257\n",
            "\n",
            " 44% 89/200 [31:54<39:48, 21.52s/it]\u001b[A89 21.521923780441284\n",
            "\n",
            " 45% 90/200 [32:16<39:27, 21.52s/it]\u001b[A90 21.51845097541809\n",
            "\n",
            " 46% 91/200 [32:37<39:05, 21.52s/it]\u001b[A91 21.51123356819153\n",
            "\n",
            " 46% 92/200 [32:59<38:44, 21.52s/it]\u001b[A92 21.522301197052002\n",
            "\n",
            " 46% 93/200 [33:20<38:22, 21.52s/it]\u001b[A93 21.5221426486969\n",
            "\n",
            " 47% 94/200 [33:42<38:01, 21.52s/it]\u001b[A94 21.52684187889099\n",
            "\n",
            " 48% 95/200 [34:03<37:39, 21.52s/it]\u001b[A95 21.52174162864685\n",
            "\n",
            " 48% 96/200 [34:25<37:18, 21.53s/it]\u001b[A96 21.539036750793457\n",
            "\n",
            " 48% 97/200 [34:46<36:56, 21.52s/it]\u001b[A97 21.516185998916626\n",
            "\n",
            " 49% 98/200 [35:08<36:35, 21.52s/it]\u001b[A98 21.522536039352417\n",
            "\n",
            " 50% 99/200 [35:29<36:13, 21.52s/it]\u001b[A99 21.526788234710693\n",
            "\n",
            " 50% 100/200 [35:51<35:52, 21.53s/it]\u001b[A100 21.533966779708862\n",
            "\n",
            " 50% 101/200 [36:12<35:31, 21.53s/it]\u001b[A101 21.528374433517456\n",
            "\n",
            " 51% 102/200 [36:34<35:09, 21.53s/it]\u001b[A102 21.529195547103882\n",
            "\n",
            " 52% 103/200 [36:55<34:48, 21.53s/it]\u001b[A103 21.52780556678772\n",
            "\n",
            " 52% 104/200 [37:17<34:26, 21.53s/it]\u001b[A104 21.53705930709839\n",
            "\n",
            " 52% 105/200 [37:39<34:05, 21.53s/it]\u001b[A105 21.5239520072937\n",
            "\n",
            " 53% 106/200 [38:00<33:43, 21.53s/it]\u001b[A106 21.528674602508545\n",
            "\n",
            " 54% 107/200 [38:22<33:21, 21.53s/it]\u001b[A107 21.521979331970215\n",
            "\n",
            " 54% 108/200 [38:43<33:00, 21.53s/it]\u001b[A108 21.529181957244873\n",
            "\n",
            " 55% 109/200 [39:05<32:38, 21.53s/it]\u001b[A109 21.520504236221313\n",
            "\n",
            " 55% 110/200 [39:26<32:17, 21.53s/it]\u001b[A110 21.533552646636963\n",
            "\n",
            " 56% 111/200 [39:48<31:55, 21.53s/it]\u001b[A111 21.524123430252075\n",
            "\n",
            " 56% 112/200 [40:09<31:34, 21.53s/it]\u001b[A112 21.527535676956177\n",
            "\n",
            " 56% 113/200 [40:31<31:12, 21.52s/it]\u001b[A113 21.5191011428833\n",
            "\n",
            " 57% 114/200 [40:52<30:51, 21.53s/it]\u001b[A114 21.53982424736023\n",
            "\n",
            " 57% 115/200 [41:14<30:29, 21.53s/it]\u001b[A115 21.517722129821777\n",
            "\n",
            " 58% 116/200 [41:35<30:08, 21.52s/it]\u001b[A116 21.523019075393677\n",
            "\n",
            " 58% 117/200 [41:57<29:46, 21.53s/it]\u001b[A117 21.527830362319946\n",
            "\n",
            " 59% 118/200 [42:18<29:25, 21.53s/it]\u001b[A118 21.52594017982483\n",
            "\n",
            " 60% 119/200 [42:40<29:03, 21.53s/it]\u001b[A119 21.523471355438232\n",
            "\n",
            " 60% 120/200 [43:01<28:42, 21.53s/it]\u001b[A120 21.52839684486389\n",
            "\n",
            " 60% 121/200 [43:23<28:20, 21.53s/it]\u001b[A121 21.526467084884644\n",
            "\n",
            " 61% 122/200 [43:44<27:58, 21.53s/it]\u001b[A122 21.523842573165894\n",
            "\n",
            " 62% 123/200 [44:06<27:37, 21.52s/it]\u001b[A123 21.51543688774109\n",
            "\n",
            " 62% 124/200 [44:28<27:15, 21.53s/it]\u001b[A124 21.532028913497925\n",
            "\n",
            " 62% 125/200 [44:49<26:54, 21.53s/it]\u001b[A125 21.52418565750122\n",
            "\n",
            " 63% 126/200 [45:11<26:32, 21.53s/it]\u001b[A126 21.529799461364746\n",
            "\n",
            " 64% 127/200 [45:32<26:11, 21.53s/it]\u001b[A127 21.52326774597168\n",
            "\n",
            " 64% 128/200 [45:54<25:49, 21.53s/it]\u001b[A128 21.528337955474854\n",
            "\n",
            " 64% 129/200 [46:15<25:28, 21.53s/it]\u001b[A129 21.523950338363647\n",
            "\n",
            " 65% 130/200 [46:37<25:06, 21.53s/it]\u001b[A130 21.531520128250122\n",
            "\n",
            " 66% 131/200 [46:58<24:45, 21.52s/it]\u001b[A131 21.51835560798645\n",
            "\n",
            " 66% 132/200 [47:20<24:23, 21.52s/it]\u001b[A132 21.51788902282715\n",
            "\n",
            " 66% 133/200 [47:41<24:02, 21.52s/it]\u001b[A133 21.52540421485901\n",
            "\n",
            " 67% 134/200 [48:03<23:40, 21.52s/it]\u001b[A134 21.51982069015503\n",
            "\n",
            " 68% 135/200 [48:24<23:18, 21.52s/it]\u001b[A135 21.520219564437866\n",
            "\n",
            " 68% 136/200 [48:46<22:57, 21.52s/it]\u001b[A136 21.52775740623474\n",
            "\n",
            " 68% 137/200 [49:07<22:35, 21.52s/it]\u001b[A137 21.52148962020874\n",
            "\n",
            " 69% 138/200 [49:29<22:14, 21.52s/it]\u001b[A138 21.526392936706543\n",
            "\n",
            " 70% 139/200 [49:50<21:52, 21.52s/it]\u001b[A139 21.51695680618286\n",
            "\n",
            " 70% 140/200 [50:12<21:31, 21.52s/it]\u001b[A140 21.520939826965332\n",
            "\n",
            " 70% 141/200 [50:33<21:09, 21.52s/it]\u001b[A141 21.51008415222168\n",
            "\n",
            " 71% 142/200 [50:55<20:48, 21.52s/it]\u001b[A142 21.52986717224121\n",
            "\n",
            " 72% 143/200 [51:16<20:26, 21.52s/it]\u001b[A143 21.50712299346924\n",
            "\n",
            " 72% 144/200 [51:38<20:04, 21.52s/it]\u001b[A144 21.517292976379395\n",
            "\n",
            " 72% 145/200 [51:59<19:43, 21.52s/it]\u001b[A145 21.523781538009644\n",
            "\n",
            " 73% 146/200 [52:21<19:21, 21.52s/it]\u001b[A146 21.514842748641968\n",
            "\n",
            " 74% 147/200 [52:43<19:00, 21.51s/it]\u001b[A147 21.505729913711548\n",
            "\n",
            " 74% 148/200 [53:04<18:38, 21.52s/it]\u001b[A148 21.519033193588257\n",
            "\n",
            " 74% 149/200 [53:26<18:17, 21.52s/it]\u001b[A149 21.514972448349\n",
            "\n",
            " 75% 150/200 [53:47<17:55, 21.52s/it]\u001b[A150 21.514123678207397\n",
            "\n",
            " 76% 151/200 [54:09<17:34, 21.51s/it]\u001b[A151 21.511953592300415\n",
            "\n",
            " 76% 152/200 [54:30<17:12, 21.52s/it]\u001b[A152 21.519456386566162\n",
            "\n",
            " 76% 153/200 [54:52<16:51, 21.51s/it]\u001b[A153 21.51061463356018\n",
            "\n",
            " 77% 154/200 [55:13<16:29, 21.52s/it]\u001b[A154 21.527462244033813\n",
            "\n",
            " 78% 155/200 [55:35<16:08, 21.52s/it]\u001b[A155 21.51028299331665\n",
            "\n",
            " 78% 156/200 [55:56<15:46, 21.52s/it]\u001b[A156 21.519073009490967\n",
            "\n",
            " 78% 157/200 [56:18<15:25, 21.52s/it]\u001b[A157 21.51150918006897\n",
            "\n",
            " 79% 158/200 [56:39<15:03, 21.51s/it]\u001b[A158 21.509270668029785\n",
            "\n",
            " 80% 159/200 [57:01<14:41, 21.51s/it]\u001b[A159 21.505016803741455\n",
            "\n",
            " 80% 160/200 [57:22<14:20, 21.51s/it]\u001b[A160 21.50428819656372\n",
            "\n",
            " 80% 161/200 [57:44<13:58, 21.51s/it]\u001b[A161 21.497848987579346\n",
            "\n",
            " 81% 162/200 [58:05<13:37, 21.51s/it]\u001b[A162 21.51473379135132\n",
            "\n",
            " 82% 163/200 [58:27<13:15, 21.51s/it]\u001b[A163 21.512776374816895\n",
            "\n",
            " 82% 164/200 [58:48<12:54, 21.51s/it]\u001b[A164 21.511764526367188\n",
            "\n",
            " 82% 165/200 [59:10<12:32, 21.51s/it]\u001b[A165 21.510188102722168\n",
            "\n",
            " 83% 166/200 [59:31<12:11, 21.51s/it]\u001b[A166 21.513710975646973\n",
            "\n",
            " 84% 167/200 [59:53<11:49, 21.51s/it]\u001b[A167 21.49726176261902\n",
            "\n",
            " 84% 168/200 [1:00:14<11:28, 21.51s/it]\u001b[A168 21.50783395767212\n",
            "\n",
            " 84% 169/200 [1:00:36<11:06, 21.51s/it]\u001b[A169 21.507118701934814\n",
            "\n",
            " 85% 170/200 [1:00:57<10:45, 21.51s/it]\u001b[A170 21.51034665107727\n",
            "\n",
            " 86% 171/200 [1:01:19<10:23, 21.51s/it]\u001b[A171 21.503345727920532\n",
            "\n",
            " 86% 172/200 [1:01:40<10:02, 21.51s/it]\u001b[A172 21.50846028327942\n",
            "\n",
            " 86% 173/200 [1:02:02<09:40, 21.51s/it]\u001b[A173 21.50808048248291\n",
            "\n",
            " 87% 174/200 [1:02:23<09:19, 21.51s/it]\u001b[A174 21.51193594932556\n",
            "\n",
            " 88% 175/200 [1:02:45<08:57, 21.51s/it]\u001b[A175 21.50048327445984\n",
            "\n",
            " 88% 176/200 [1:03:06<08:36, 21.51s/it]\u001b[A176 21.506279706954956\n",
            "\n",
            " 88% 177/200 [1:03:28<08:14, 21.51s/it]\u001b[A177 21.50826120376587\n",
            "\n",
            " 89% 178/200 [1:03:49<07:53, 21.51s/it]\u001b[A178 21.512428283691406\n",
            "\n",
            " 90% 179/200 [1:04:11<07:31, 21.51s/it]\u001b[A179 21.511813640594482\n",
            "\n",
            " 90% 180/200 [1:04:32<07:10, 21.51s/it]\u001b[A180 21.516900539398193\n",
            "\n",
            " 90% 181/200 [1:04:54<06:48, 21.51s/it]\u001b[A181 21.50542140007019\n",
            "\n",
            " 91% 182/200 [1:05:15<06:27, 21.51s/it]\u001b[A182 21.51584529876709\n",
            "\n",
            " 92% 183/200 [1:05:37<06:05, 21.51s/it]\u001b[A183 21.506527423858643\n",
            "\n",
            " 92% 184/200 [1:05:58<05:44, 21.51s/it]\u001b[A184 21.507904529571533\n",
            "\n",
            " 92% 185/200 [1:06:20<05:22, 21.51s/it]\u001b[A185 21.50879693031311\n",
            "\n",
            " 93% 186/200 [1:06:41<05:01, 21.51s/it]\u001b[A186 21.515501499176025\n",
            "\n",
            " 94% 187/200 [1:07:03<04:39, 21.51s/it]\u001b[A187 21.51064705848694\n",
            "\n",
            " 94% 188/200 [1:07:24<04:18, 21.51s/it]\u001b[A188 21.512489318847656\n",
            "\n",
            " 94% 189/200 [1:07:46<03:56, 21.51s/it]\u001b[A189 21.50375270843506\n",
            "\n",
            " 95% 190/200 [1:08:07<03:35, 21.51s/it]\u001b[A190 21.51244616508484\n",
            "\n",
            " 96% 191/200 [1:08:29<03:13, 21.51s/it]\u001b[A191 21.511439323425293\n",
            "\n",
            " 96% 192/200 [1:08:50<02:52, 21.51s/it]\u001b[A192 21.5168194770813\n",
            "\n",
            " 96% 193/200 [1:09:12<02:30, 21.51s/it]\u001b[A193 21.504449605941772\n",
            "\n",
            " 97% 194/200 [1:09:33<02:09, 21.51s/it]\u001b[A194 21.499645471572876\n",
            "\n",
            " 98% 195/200 [1:09:55<01:47, 21.51s/it]\u001b[A195 21.5104877948761\n",
            "\n",
            " 98% 196/200 [1:10:17<01:26, 21.51s/it]\u001b[A196 21.516725778579712\n",
            "\n",
            " 98% 197/200 [1:10:38<01:04, 21.51s/it]\u001b[A197 21.50838327407837\n",
            "\n",
            " 99% 198/200 [1:11:00<00:43, 21.51s/it]\u001b[A198 21.509210109710693\n",
            "\n",
            "100% 199/200 [1:11:21<00:21, 21.51s/it]\u001b[A199 21.511361360549927\n",
            "\n",
            "100% 200/200 [1:11:43<00:00, 21.52s/it]\n",
            "Saved test set\n",
            "val poses shape torch.Size([25, 4, 4])\n",
            "\n",
            "  0% 0/25 [00:00<?, ?it/s]\u001b[A0 0.00045418739318847656\n",
            "torch.Size([800, 800, 3]) torch.Size([800, 800])\n",
            "\n",
            "  4% 1/25 [00:21<08:35, 21.49s/it]\u001b[A1 21.488723039627075\n",
            "\n",
            "  8% 2/25 [00:43<08:14, 21.50s/it]\u001b[A2 21.515488386154175\n",
            "\n",
            " 12% 3/25 [01:04<07:53, 21.51s/it]\u001b[A3 21.514742136001587\n",
            "\n",
            " 16% 4/25 [01:26<07:31, 21.51s/it]\u001b[A4 21.50663709640503\n",
            "\n",
            " 20% 5/25 [01:47<07:10, 21.51s/it]\u001b[A5 21.503702640533447\n",
            "\n",
            " 24% 6/25 [02:09<06:48, 21.51s/it]\u001b[A6 21.514811038970947\n",
            "\n",
            " 28% 7/25 [02:30<06:27, 21.51s/it]\u001b[A7 21.521308183670044\n",
            "\n",
            " 32% 8/25 [02:52<06:05, 21.51s/it]\u001b[A8 21.5081045627594\n",
            "\n",
            " 36% 9/25 [03:13<05:44, 21.52s/it]\u001b[A9 21.52743649482727\n",
            "\n",
            " 40% 10/25 [03:35<05:22, 21.51s/it]\u001b[A10 21.510056972503662\n",
            "\n",
            " 44% 11/25 [03:56<05:01, 21.51s/it]\u001b[A11 21.51510453224182\n",
            "\n",
            " 48% 12/25 [04:18<04:39, 21.52s/it]\u001b[A12 21.51673150062561\n",
            "\n",
            " 52% 13/25 [04:39<04:18, 21.51s/it]\u001b[A13 21.499730348587036\n",
            "\n",
            " 56% 14/25 [05:01<03:56, 21.51s/it]\u001b[A14 21.51800560951233\n",
            "\n",
            " 60% 15/25 [05:22<03:35, 21.52s/it]\u001b[A15 21.524312257766724\n",
            "\n",
            " 64% 16/25 [05:44<03:13, 21.51s/it]\u001b[A16 21.511425733566284\n",
            "\n",
            " 68% 17/25 [06:05<02:52, 21.52s/it]\u001b[A17 21.527595043182373\n",
            "\n",
            " 72% 18/25 [06:27<02:30, 21.52s/it]\u001b[A18 21.50773048400879\n",
            "\n",
            " 76% 19/25 [06:48<02:09, 21.52s/it]\u001b[A19 21.524203538894653\n",
            "\n",
            " 80% 20/25 [07:10<01:47, 21.52s/it]\u001b[A20 21.529885053634644\n",
            "\n",
            " 84% 21/25 [07:31<01:26, 21.52s/it]\u001b[A21 21.502779245376587\n",
            "\n",
            " 88% 22/25 [07:53<01:04, 21.52s/it]\u001b[A22 21.52608633041382\n",
            "\n",
            " 92% 23/25 [08:14<00:43, 21.52s/it]\u001b[A23 21.512078285217285\n",
            "\n",
            " 96% 24/25 [08:36<00:21, 21.51s/it]\u001b[A24 21.510266065597534\n",
            "\n",
            "100% 25/25 [08:57<00:00, 21.51s/it]\n",
            "(25, 800, 800, 3) (25, 800, 800, 3)\n",
            "0.003006967483088374 25.218713760375977\n",
            "tensor([25.2187]) torch.Size([1])\n",
            "Validation PSNR = tensor([25.2187]) with validation size 25\n",
            "Saved val set\n",
            "[TRAIN] Iter: 200000 Loss: 0.008377471938729286  PSNR: 25.218713760375977\n",
            "100% 200000/200000 [7:52:25<00:00,  7.06it/s]  \n"
          ]
        }
      ],
      "source": [
        "# nerf without positional encoding\n",
        "!cd nerf-pytorch/ && python run_nerf.py --config configs/bottles.txt --i_embed -1 --i_weights 10000 --N_iters 200000 --i_testset 200000 --i_valset 200000 --i_valsize 25 --i_video 300000 --lrate 1e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ivXFxWL7CZj"
      },
      "outputs": [],
      "source": [
        "# Save results to the cloud disk\n",
        "!cp -r ./nerf-pytorch/logs/blender_paper_bottle_flip_no_encoding /content/drive/MyDrive/HW3_res/logs_blender_paper_bottle_flip_no_encoding"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
